The computing world is increasingly finding parallelism as the only
viable approach to maintaining continued performance improvements in
the era of multicore hardware. Despite this, the adoption of parallel
programming practises has been slow and awkward, due to the
prohibitive complexity of parallel programming, and low level of
abstractions available.

Algorithmic Skeletons address this issue by providing reusable
patterns for parallel programming, offering higher level abstractions
and reducing programmer effort~\cite{Cole1989, Cole2004}. Tuning the
performance of these Algorithmic Skeletons is a manual process which
requires exhaustively searching the optimisation space to select
optimal parameters.
% TODO: how big are these optimisation spaces?

The aim of this project is to demonstrate that the tuning of
optimisation parameters could be successfully performed at
runtime. This would enable self-tuning programs which adapt to their
execution environment by selecting optimal parameters
dynamically. Such configurations of parameters could be learned over
time, allowing each successive iteration of a program to benefit from
its predecessors.
% TODO: this last sentence is weak ^^

The case for dynamically autotuning applications is strong. There are
many factors which contribute to the performance of programs which
cannot be determined by program developers. For example,

% TODO: the case of self-tuning systems: TCP/IP.

As a result, performance optimisation requires the programmer to
either overfit the choice of parameters to optimise for a specific
task and environment, or laboriously create heuristics which segment
the optimisation space into regions of identical configurations. Both
approaches have significant drawbacks: optimising for a specific task
and environment creates brittle and non-portable optimisations that do
not generalise to other architectures and inputs; and the task of
creating heuristics which cover every possible combination of factors
is prohibitively time consuming for developers.

This project proposes two hypotheses about the performance of
Algorithmic Skeletons:
\begin{itemize}
\item a dynamic autotuner will improve the performance of Algorithmic
  Skeletons in the general case, by selecting optimisations which
  target specific dynamic features;
\item a dynamic autotuner will provide better average performance than
  a statically tuned equivalent implementation across different
  inputs, by adapting to features which can only be determined
  dynamically.
\end{itemize}

These hypotheses can be referred to respectively as the claims for
autotuning using \emph{specialisation} and \emph{generalisation}. It
can be inferred from these that a dynamic autotuner cannot provide
better performance than a statically tuned equivalent implementation
for a \emph{fixed} input, since the extra instructions that implement
the dynamic autotuning behaviour present a performance overhead. The
reduction of this overhead is one of the greatest challenges facing
the development of dynamic autotuners. The novelty of my solution is
to reduce parameter convergence time by implementing a dynamic
autotuner for Algorithmic Skeletons which can store the results of
successive iterations persistently, across program runs. An evaluation
of a successful implementation will contribute empirical evidence
supporting the two hypotheses.
% TODO: parameter convergence time? This is a totally new topic

The rest of the document is structured as follows:
Section~\ref{sec:motivation} contains the motivation for this
research; Section~\ref{sec:background} briefly outlines related work;
Sections~\ref{sec:methodology} and~\ref{sec:evaluation} describe the
methodology and evaluation plans for this research;
Section~\ref{sec:work-plan} contains the work plan; followed by the
conclusion in Section~\ref{sec:conclusions}.

% It is my hypothesis that the performance an optimisation system which
% will be improved by developing a dynamic autotuner.  which considers
% dynamic features which cannot be determined at compile time. The
% premise is that the optimisation spaces of Algorithmic Skeletons are
% shaped by features which can only be determined at runtime. Effective
% searching of these spaces can only be performed by collecting
% empirical data rather than building predictive models. To justify this
% hypothesis, support for dynamic autotuning will be added to SkelCL, a
% C++ Algorithmic Skeleton Framework which targets heterogeneous
% parallel programming using OpenCL.

% % mapreduce
% ~\cite{Dean2008}

% Intel TBB\footnote{\url{https://www.threadingbuildingblocks.org/}}

% % snippet
% Parallel computing, traditionally the remit of scientific programming,
% is increasingly being seen as the only viable approach for maintaining
% continued performance improvements in a multicore computing world.
% Despite its growing popularity, writing robust parallel software is an
% inherently challenging task, requiring the programmer to think in
% unfamiliar paradigms, and with many associated problems raised by race
% conditions, deadlock, and managing access to shared resources.

% % snippet
% Equivalent parallel programs require many more lines of code than
% their sequential counterparts, and the additional programmer effort
% that is required is dedicated to writing coordination logic -- the
% logic responsible for allocating and coordinating access to shared
% resources. Algorithmic Skeletons address the difficulties of parallel
% programming by providing a higher level abstraction that encapsulates
% this coordination logic, ridding the application programmer of these
% responsibilities and allowing them to focus instead on the
% problem-solving logic.

% % snippet
% Previous research has attempted to address this issue using iterative
% compilation techniques, by tuning the performance of Algorithmic
% Skeletons through searching the optimisation space offline and
% selecting a set of parameter values that provide optimum performance
% for a given program. Such tools perform static autotuning, that is,
% they automatically tune optimisation parameters based on static
% features which can be selected at compile time.

% % snippet
% Research interest in Algorithmic Skeletons is high, and while
% frameworks of Algorithmic Skeletons abound, widespread adoption has so
% far been restricted largely to established use cases that rely heavily
% on high performance computing, for example, Google's MapReduce, and
% Intel's Thread Building Blocks. While the demand for such frameworks
% in the field of High Performance Computing (HPC) is self-evident, this
% should by no means blinker the ambitions of skeletons research. The
% benefits of Algorithmic Skeletons extend beyond that of HPC and cover
% general purpose computing.

% % snippet
% The popularity of programming with parallel patterns is rapidly
% increasing, as they have been demonstrated as a means of providing
% re-usability to the thousands of man-hours that is required to write a
% tuned and stable parallel application. For example, Google's
% MapReduce, which allows their programmers to write data sorting
% programs in 55 lines of code, while taking advantage of the over
% 800,000 lines of code present in a MapReduce implementation. Any
% research forwarding the cause of parallel patterns will prove
% extremely valuable to both application developers and future
% researchers.
