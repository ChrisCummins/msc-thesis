Parallelism is increasingly being seen as the only viable approach to
maintaining continued performance improvements in a multicore
world. Despite this, the adoption of parallel programming practises
has been slow and awkward, due to the prohibitive complexity and low
level of abstractions available to programmers.

Algorithmic Skeletons address this issue by providing reusable
patterns for parallel programming, offering higher-level abstractions
and reducing programmer effort~\cite{Cole1989, Cole2004}. Tuning the
performance of these Algorithmic Skeletons requires programmers to
either manually set optimisation parameters based on intuition, or to
use an offline training technique to search the space of optimisation
parameters and select the optimal configuration.

The aim of this project is to demonstrate that the tuning of
optimisation parameters can be successfully performed at runtime
without needing offline training. This will enable self-tuning
programs which adapt to their execution environment by selecting
optimal parameters dynamically. Dynamic optimisation training will be
achieved using online machine learning, allowing successive program
runs to improve upon its predecessors.

\subsection{Hypotheses}
This project proposes two hypotheses about the performance of
Algorithmic Skeletons:
\begin{itemize}
\item a dynamic autotuner will select optimisations that provide
  improved performance over a baseline Algorithmic Skeleton
  implementation;
\item a dynamic autotuner will provide improved performance over a
  hand-tuned OpenCL implementation across a range of different inputs,
  by adapting to changes in the inputs dynamically.
\end{itemize}

These hypotheses can be referred to respectively as the claims
\emph{specialisation} and \emph{generalisation}. We can infer from
these that a dynamic autotuner cannot provide better performance than
an equivalent OpenCL implementation which has been tuned for a
\emph{fixed} input, since the extra instructions required to implement
the dynamic autotuner present an unavoidable performance overhead. The
reduction of this overhead is one of the greatest challenges facing
the development of dynamic autotuners.

\subsection{Contributions}

The novelty of my solution is to apply online machine learning
techniques to the problem of optimisation parameter selection for
Algorithmic Skeletons. Targeting the high-level abstractions of
Algorithmic Skeletons at runtime exposes many features which cannot be
statically determined, such as:

\begin{itemize}
\itemsep0em
\item properties of the input data, e.g.\ the size, data type, and
  dimensionality;
\item copy-up and copy-down times for transferring data to and from
  device memory;
\item the available devices and number of cores;
\item OpenCL compiler settings and optimisation flags;
\item system state information, e.g.\ system load.
\end{itemize}

% \subsection{Structure}
% The rest of the document is structured as follows:
% Section~\ref{sec:motivation} contains a motivating example;
% Section~\ref{sec:background} briefly outlines related work and
% background; Sections~\ref{sec:methodology} and~\ref{sec:evaluation}
% describe the methodology and evaluation; Section~\ref{sec:work-plan}
% contains the work plan; followed by the conclusion in
% Section~\ref{sec:conclusions}.
