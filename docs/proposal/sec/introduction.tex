Parallelism is increasingly seen as the only viable approach to
maintaining continued performance improvements in a multicore
world. Despite this, the adoption of parallel programming practises
has been slow and awkward, due to the prohibitive complexity and low
level of abstractions available to programmers.

Algorithmic Skeletons address this issue by providing reusable
patterns for parallel programming, offering higher-level abstractions
and reducing programmer effort~\cite{Cole1989, Cole2004}. Tuning the
performance of these Algorithmic Skeletons requires programmers to
either manually set optimisation parameters based on intuition, or to
search the huge space of possible optimisation parameters by
repeatedly evaluating different configurations to select the
configuration which gives the best performance.

The aim of this project is to demonstrate that the tuning of
optimisation parameters can be successfully performed at runtime
without needing offline training. This will enable self-tuning
programs which adapt to their execution environment by selecting
optimal parameters dynamically. Online machine learning will enable
the runtime exploration of the optimisation space while selecting
configurations that maximise performance.

\subsection{Hypotheses}
This project proposes two hypotheses about the performance of
Algorithmic Skeletons:
\begin{itemize}
\item a dynamic autotuner will select optimisations that provide
  improved performance over a baseline Algorithmic Skeleton
  implementation;
\item a dynamic autotuner will provide improved performance over a
  hand-tuned OpenCL implementation across a range of different inputs,
  by adapting to changes in the inputs dynamically.
\end{itemize}

These hypotheses can be referred to respectively as the claims
\emph{specialisation} and \emph{generalisation}. We can infer from
these that a dynamic autotuner cannot provide better performance than
an equivalent OpenCL implementation which has been tuned for a
\emph{fixed} input, since the extra instructions required to implement
the dynamic autotuner present an unavoidable performance overhead. The
reduction of this overhead is one of the greatest challenges facing
the development of dynamic autotuners.

\subsection{Contributions}

The novelty of my solution is to apply online machine learning
techniques to the problem of optimisation parameter selection for
Algorithmic Skeletons. Contributions of a successful project will
include:

\begin{itemize}
\item a first attempt to apply the principles of online machine
  learning to the runtime selection of Algorithmic Skeleton
  optimisation parameters;
\item a dynamic autotuner which specifically targets features relevant
  to multi-GPU parallelism;
\item experimental results comparing the performance of dynamically
  autotuned SkelCL against hand tuned OpenCL across multiple
  benchmarks.
\end{itemize}

% \subsection{Structure}
% The rest of the document is structured as follows:
% Section~\ref{sec:motivation} contains a motivating example;
% Section~\ref{sec:background} briefly outlines related work and
% background; Sections~\ref{sec:methodology} and~\ref{sec:evaluation}
% describe the methodology and evaluation; Section~\ref{sec:work-plan}
% contains the work plan; followed by the conclusion in
% Section~\ref{sec:conclusions}.
