My hypothesis is that the performance of Algorithmic Skeletons can be
improved using dynamic autotuning. This hypothesis will be supported
or rejected by empirical evidence collected from an evaluation of the
implemented prototype. I will use experimental evidence and standard
empirical methods to evaluate the performance of SkelCL across a range
of representative benchmarks.

I will compare experimental performance results against:

\begin{itemize}
\item a baseline implementation provided by an unmodified SkelCL
implementation. This will compare the speedup of the autotuned version
over the baseline;
\item a hand-tuned ``oracle'' implementation using an optimal
configuration discovered through an exhaustive search of the
optimisation space. This will measure the ability of the autotuner
to converge towards optimal parameters over time;
\item a ``gold standard'' implementation using hand tuned OpenCL
without the SkelCL abstractions. This will compare the performance
cost of using the high-level Algorithmic Skeleton abstractions against
the reduction in programmer effort required to implement the
equivalent program in pure OpenCL.
\end{itemize}

An important factor in the quality of the evaluation will be selecting
performance benchmarks that are representative of a range of real
world use cases. For this purpose, I will use existing SkelCL
benchmarks which have been used in previous research: Mandelbrot
sets~\cite{Steuwer2011}, Sobel Edge Detection~\cite{Steuwer2013a}, and
List-mode Ordered Subset Expectation
Maximisation~\cite{Steuwer2013}. Additionally, a standard benchmark
suite for heterogeneous computing such as Rodinia~\cite{Che2009} could
be used by first porting the implementations to use the SkelCL
library.

The stochastic nature of autotuning and machine learning techniques
means that the performance evaluation of representative benchmarks
must be performed with statistical rigour, using appropriate
techniques for profiling benchmark performance over multiple
iterations~\cite{Georges2007}. The evaluation approach must carefully
isolate independent variables and provide a controlled environment for
testing the effects of altering them.

In addition to the overall performance evaluation of the dynamic
autotuner, additional measurements can be made to isolate and record
the overhead introduced by the runtime, the amount of time required to
converge on optimal configurations, and the ability of the dynamic
optimiser to adapt to changes in the runtime environment. This last
measurement may require execution of the benchmarks on multiple
different hardware configurations so as to measure the ability of the
autotuner to adapt to different environments.
