My hypothesis is that the performance of Algorithmic Skeletons will be
improved by using dynamic autotuning. This hypothesis will be
supported or rejected by empirical evidence collected from an
evaluation of the prototype implementation. Experimental evidence and
standard empirical methods will be used to evaluate the performance of
SkelCL across a range of representative benchmarks.

Experimental performance results will be compared against:

\begin{itemize}
\item a baseline implementation provided by an unmodified SkelCL
implementation. This will compare the speedup of the autotuned version
over the baseline;
\item a hand-tuned ``oracle'' implementation using an optimal
configuration discovered through an offline exhaustive search of the
optimisation space. This will measure the autotuner's ability to
converge towards optimal parameters over time;
\item a ``gold standard'' implementation using hand tuned OpenCL
without the SkelCL abstractions. This will compare the performance
cost of using the high-level Algorithmic Skeleton abstractions against
the reduction in programmer effort required to implement the
equivalent program in pure OpenCL.
\end{itemize}

An important factor in the quality of the evaluation will be selecting
performance benchmarks that are representative of a range of real
world use cases. For this purpose, I will use existing SkelCL
benchmarks which have been used in previous research: Mandelbrot
sets~\cite{Steuwer2011}, Sobel Edge Detection~\cite{Steuwer2013a}, and
List-mode Ordered Subset Expectation
Maximisation~\cite{Steuwer2013}. Additionally, a standard benchmark
suite for heterogeneous computing such as Rodinia~\cite{Che2009} can
be used by first porting the implementations to use the SkelCL
library.

% There will be a tight feedback loop between implementation and
% evaluation during the prototype development.

The stochastic nature of autotuning and machine learning techniques
means that the performance evaluation of representative benchmarks
must be performed with strict statistical rigour, using appropriate
techniques for profiling benchmark performance over multiple
iterations~\cite{Georges2007}. The evaluation approach must carefully
isolate independent variables and provide a controlled environment for
testing the effects of altering them.

In addition to the overall performance evaluation of the dynamic
autotuner, additional measurements can be made to isolate and record
the overhead introduced by the runtime, the amount of time required to
converge on optimal configurations, and the ability of the dynamic
optimiser to adapt to changes in the runtime environment. This last
measurement may require execution of the benchmarks on multiple
different hardware configurations so as to measure the system's
ability to adapt to different environments.
