The work required to complete this research can be divided into three
stages:

\begin{enumerate}
\item Modify SkelCL to enable the runtime configuration of
  optimisation parameters.
\item Evaluate the significance of dynamic features and different
  optimisation parameters to select the parameters which provide the
  most profitable optimisation space.
\item Implement a dynamic autotuner which searches and builds a model
  of this optimisation space at runtime.
\end{enumerate}

In the first stage, I will replace compile-time constant parameters in
the SkelCL library with variable parameters, and add an API to support
dynamically setting these parameters. Examples of parameters which can
be set dynamically include the mapping of work items to threads and
the OpenCL compiler configuration. I will also modify the container
types of SkelCL so that dynamic features of input data structures can
be extracted at runtime.
% TODO: complete this sentence:
% Examples of dynamic features include the distribution of elements
% within a container and the data type of elements.

Pilot experiments will then be used to evaluate the effect of
different parameters and features on performance by varying manually
controlled stimuli across a range of different inputs and measuring
their impact on performance. Statistical methods will be used to
analyse these results in order to isolate the parameters and features
with the greatest performance impact, and generate an optimisation
space. For example, Principle Component Analysis can be used to reduce
the dimensionality of this optimisation space by orientating the space
along the directions of greatest variance.

This exploratory phase provides opportunities for the novel use of
dynamic features for the purpose of autotuning Algorithmic Skeletons,
since previous research has focused on offline tuning and so has been
restricted to the set of features which can be either statically
determined or approximated. In addition to dynamic features, SkelCL
compiles OpenCL kernels at runtime immediately before execution. This
enables the setting of arbitrary optimisation parameters without
having to use the multi-versioning techniques of many state of the art
dynamic optimisers, in much multiple versions of a subroutine are
included in a compiled binary.

% TODO: UML sequence diagram of SkelCL

In the final stage, I will construct a dynamic autotuner that uses the
features and parameters selected in the exploratory phase. To the best
of our knowledge, this will be the first attempt to develop a dynamic
autotuner for Algorithmic Skeletons. The focus of the implementation
will be to exploit the advantages of dynamic features to provide
improved performance over existing static Algorithmic Skeleton
autotuners, and to exploit the high-level abstractions of Algorithmic
Skeletons to provide improved performance over existing dynamic
optimisers.

Implementing a dynamic autotuner poses a number of difficult
challenges. The primary challenge is to minimise the runtime overhead
so that it does not outweigh the performance gains of the
optimisations themselves. The proposed approach to dynamically
autotune SkelCL will overcome one of the most significant overheads
associated with dynamic optimising: that of instrumenting the code for
the purpose of profiling and tracing. Since Algorithmic Skeletons
coordinate muscle functions, it is possible to forgo many of the
profiling counters that dynamic optimisers require by making
assumptions about the execution frequency of certain code paths, given
the nature of the skeleton. Profiling counters will be placed by hand
at critical points in the code, allowing the frequency of counter
increments to be minimised.

The convergence time of autotuning can be improved by saving the
results of trial configurations persistently in a central
database. This provides two advantages: first, it allows the results
of autotuning to be used by future program runs; second, it allows the
result of autotuning to be shared amongst any program which uses the
SkelCL library. The challenge of implementing this persistent data
storage is that results must be stored efficiently and compactly, to
allow for indefinite scaling of the dataset as future results are
added. Increasing the size of the training dataset also increases the
time required to compute new results, and there is additional
latencies associated with reading and writing data to and from disk.
