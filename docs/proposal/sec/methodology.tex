\begin{figure*}[t!]
\centering
\input{fig/method}
\caption{The skeleton invocation behaviour of current SkelCL
  % TODO: neaten up these subfigure references.
  (\ref{subfig:skelcl}), and with dynamic autotuning
  (\ref{subfig:skelcl-autotune}). When invoked, the dynamic features
  of a skeleton object are extracted and an online machine learning
  model recommends optimal parameters. The OpenCL compiler is invoked
  on this parameterised skeleton to generate an OpenCL kernel for
  execution on device. Profiling information is gathered during
  execution and added to the training dataset.}
\label{fig:method}
\end{figure*}

The work required to complete this research has been broadly divided
into three stages:

\begin{enumerate}
\item Modify SkelCL to enable the runtime configuration of
  optimisation parameters and the extraction of dynamic features.
\item Evaluate the significance of optimisation parameters and dynamic
  features. Use the results of this evaluation to select an online
  machine learning algorithm for parameter tuning.
\item Implement a low overhead dynamic autotuner which uses this
  online machine learning model to select optimisation parameters at
  runtime.
\end{enumerate}

This section outlines the work required for each stage, listing some
of the possible challenges and approaches to overcoming them.

\subsection{Model features and parameters}
In the first stage, I will replace compile-time constant parameters in
the SkelCL library with variable parameters, and add an API to support
dynamically setting these parameters. This will provide the set of
actions that can be taken based on performance predictions of the
machine learning model. Examples of parameters which can be set
dynamically include the mapping of work items to threads and the
OpenCL compiler configuration. I will then modify the container types
of SkelCL so that properties of input data structures can be extracted
at runtime. These will provide the input features to the machine
learning model. Examples of dynamic features include the
dimensionality and types of data.

\subsection{Online machine learning}
Exploratory experiments will then be used to evaluate the effect of
different parameters and features by varying test stimuli across a
range of different inputs and measuring their impact on
performance. Statistical methods will be used to analyse these results
and isolate the parameters and features with the greatest performance
impact. Principle Component Analysis can be used to reduce the
dimensionality of this optimisation space by orientating the space
along the directions of greatest variance.

The purpose of this exploratory phase is to identify the parameters
and features which can be used to most effectively search the
optimisation space, and to guide the choice of an online machine
learning algorithm. The goal of the online machine learning algorithm
is to generate parameter configurations which will provide the best
performance for a given skeleton and input dataset. Every time the
user invokes a skeleton object, the machine learning algorithm must:

\begin{enumerate}
\item Predict the parameter configuration which will provide the best
  performance based on the features.
\item Compile and execute the skeleton with this configuration.
\item Measure the true performance of the skeleton and use this result
  to refine future predictions.
\end{enumerate}

The primary challenge in developing the machine learning algorithm is
to balance the potentially conflicting requirements to:

\begin{itemize}
\item offer the best performance configurations to maximise
  performance;
\item search the large optimisation space to avoid becoming trapped in
  local minima;
\item build statistical confidence in training data through repeated
  invocations of identical configurations.
\end{itemize}

\subsection{Dynamic autotuner implementation}
In the final stage, I will implement a dynamic autotuner which uses
the online machine learning algorithm, features, and parameters
selected in the exploratory phase. To the best of our knowledge this
will be the first attempt to develop a dynamic autotuner using online
machine learning for Algorithmic Skeletons. The goal of the
implementation will be to exploit the advantages of dynamic features
to provide improved performance over existing static Algorithmic
Skeleton autotuners, and to exploit the high-level abstractions of
Algorithmic Skeletons to provide improved performance over existing
dynamic optimisers. Figure~\ref{subfig:skelcl-autotune} shows a
system-level overview of dynamically autotuned SkelCL.

A major challenge when implementing online autotuning is to minimise
the runtime overhead so that it does not outweigh the performance
gains of the optimisations themselves. The proposed approach to
dynamically autotune SkelCL will overcome a significant overhead
associated with dynamic optimising: that of instrumenting the code to
enable profiling and tracing. Since Algorithmic Skeletons coordinate
muscle functions, it is possible to forgo many of the counters
required for performance profiling by making assumptions about the
execution frequency of certain code paths given the nature of the
skeleton. I will place profiling counters by hand at critical points
in the SkelCL library to minimise the frequency of counter increments.

\begin{figure*}[t!]
\makebox[\textwidth][c]{\input{fig/gantt}}
\caption{Project schedule Gantt chart.}
\label{fig:gantt}
\end{figure*}

The convergence time of autotuning can be improved by using a central
database to store optimisation results. This provides two advantages:
first, it allows the results of autotuning to be used by future
program runs; second, it allows the result of autotuning to be shared
across any program which uses the SkelCL library. The challenge of
implementing this persistent data storage is that results must be
stored efficiently and compactly to allow for indefinite scaling of
the dataset as future results are added. Increasing the size of the
training dataset also increases the time required to compute new
results, and there is additional latencies associated with reading and
writing data to and from disk.
