\begin{figure}[!b]
\centering
\input{fig/dac}
\caption{The performance impact of dynamic features on the
  parallelisation depth parameter: in \ref{subfig:dac-pardepth}, as a
  function of split size $n_s$; in \ref{subfig:dac-in}, as a function
  of input type and size. In both cases, the optimal parameter value
  is highly dependent on the dynamic feature.}
\label{fig:dac}
\end{figure}

Consider a recursive merge sort algorithm. When called, the algorithm
determines whether the input list is short enough to be solved
directly using a linear sorting method, or whether it should split it
into multiple sub-lists and sort them by recursing on each sub-list
before combining the results. This computational pattern of repeatedly
dividing a problem into smaller subproblems which are then recombined
is abstracted by a Divide and Conquer pattern. This can be
parallelised effectively by considering each recursion as a new task
which can be executed concurrently.

% $T_i$ and $T_o$

% \begin{verbatim}
% merge_sort(T):
%     if should_divide(T):
%          T' = divide(T)
%          if recursion depth < parallelisation depth:
%              parallel_map(merge_sort, T')
%          else:
%              sequential_map(merge_sort, T')
%          return combine(T')
%     else:
%          return conquer(T)
% \end{verbatim}

\begin{alignat*}{3}
should\_divide &: T_i & &\rightarrow boolean\\
divide &: T_i & &\rightarrow [T_i]\\
conquer &: T_i & &\rightarrow T_o\\
combine &: [T_o] & &\rightarrow T_o
\end{alignat*}

The parallel Divide and Conquer pattern is a common form of
Algorithmic Skeleton, whereby the user provides muscle functions for
the split, merge, and conquer logic, and the skeleton can coordinate
the allocation of new tasks. When implementing such a Divide and
Conquer skeleton, there are two immediate parameters which will
greatly affect the performance: the maximum depth at which recursion
should occur as a new task, as opposed to sequentially; and the
threshold minimum size of the input problem before the problem is
conquered directly rather than recursively.

Existing iterative compilation techniques can perform an exhaustive
search of the optimisation space generated by these two parameters,
which would reveals a strong interaction between them: the optimum
value for one parameter is strongly influenced by the value of the
other parameter. Additionally, the optimisation space of both
parameters is strongly influenced by an independent factor: the size
and type of the input problem. This means that in the case of a
parallel merge sort algorithm, the optimum values for the max
recursion depth and minimum input threshold parameters will be very
different when sorting lists of integers and lists of bytes, or lists
of arbitrary user-chosen data structures. This cannot be modelled
using iterative compilation techniques, as the size and type of the
input problem a dynamic features, which can only be determined at
runtime.

Static approaches to this problem involve segmenting the dynamic
feature space using heuristics in order to select optimum values for
approximate ranges. The effectiveness of these heuristics is limited
by their complexity and the thoroughness of the optimisation space
search. In addition, the resulting optimisation heuristics would be
very fragile and non-portable, so that the whole tedious process would
need to be repeated for every target architecture, and with every new
generation of hardware. Such an approach is clearly
impractical. Compare this to the alternate approach of a Divide and
Conquer skeleton which is capable of performing this empirical data
gathering online and during normal execution, and which will use
successive iterations to converge naturally upon an optimum
configuration. Such a system would be capable of dealing with varying
dynamic features which would destroy the capabilities of a static
heuristic based system. This is the goal of my research.
