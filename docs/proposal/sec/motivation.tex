\begin{figure*}[!b]
\centering
\input{fig/dac}
\caption{The performance impact of dynamic features on the
  parallelisation depth parameter: in \ref{subfig:dac-pardepth}, as a
  function of split size $n_s$; in \ref{subfig:dac-in}, as a function
  of input type and size. In both cases, the optimal parameter value
  is highly dependent on the dynamic feature.}
\label{fig:dac}
\end{figure*}

Consider a recursive merge sort algorithm. The algorithm takes an
input list $T_i$ and returns a sorted permutation by determining
whether the input list is short enough to be solved directly using a
linear sorting method, or whether it should split it into multiple
sub-lists and sort them by recursing on each sub-list before combining
the results. This computational pattern is abstracted by the Divide
and Conquer skeleton, which, when parameterised with four muscle
functions, can effectively parallelise this task by considering each
recursion as a new task to be executed concurrently. The four muscle
functions operate on an input type $T_i$, return an object of type
$T_o$, and can be formally defined as:

\begin{myalignat}{3}
should\_divide &: T_i & &\rightarrow boolean\\
divide &: T_i & &\rightarrow [T_i]\\
conquer &: T_i & &\rightarrow T_o\\
combine &: [T_o] & &\rightarrow T_o
\end{myalignat}

The degree of a Divide and Conquer skeleton is the number of
sub-problems that the $divide$ function splits an input $T_i$. For a
given degree $k$, the number of tasks $n$ grows exponentially with
depth $d$:

\[n = k^{d} - 1\]

On a given machine, the number of tasks which can be effectively
executed in parallel is limited by the number of available cores.
Since a Divide and Conquer pattern does not constrain the maximum
depth that an algorithm may expand to, the negative performance of
task switching cost can be avoided by imposing a maximum
``parallelisation depth''. Recursion above this depth causes the
creation of parallel tasks, below this depth, recursion occurs
sequentially.

This section describes an experiment to collect empirical data on the
effect of varying this parallelisation under varying input conditions.

\subsection{Experimental setup}
A Divide and Conquer skeleton was implemented and parameterised with
muscle functions to implement merge sort. The skeleton was
parallelised using the C++11 Thread Support Library, and a testbench
recorded the mean time to sort a vector of random unsorted data over
30 iterations.

\subsection{Results}
Figure~\ref{fig:dac} shows the mean performance speedup of different
parallelisation depths over sequential
execution. Figure~\ref{subfig:dac-pardepth} shows the effect of
varying the split size, which is a property of the $should\_divide$
muscle function that determines the maximum list size at which
recursive sort should bottom out and insertion sort should be
used. Figure~\ref{subfig:dac-in} shows the effect of varying the size
and data type of the input vector.

In both cases we observe that changing properties of the input and
muscle functions has a significant impact on the optimal
parallelisation depth parameter. Since neither of those properties can
be known \emph{a priori}, the skeleton author must resort to picking a
value which they expect to provide best average case performance,
which will perform suboptimally for many different input conditions. A
dynamic autotuner could adapt to these different inputs by setting the
parallelisation depth at runtime.
