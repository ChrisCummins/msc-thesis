This section briefly outlines some of the most closely related pieces
of work that address the issue of improving software performance
through the selection of optimal parameters. These can broadly be
categorised as either offline tuning, or dynamic optimisation.
% TODO: reword this weak sentence ^^

\subsection{Offline tuning}\label{subsec:offline-tuning}
Offline tuning involves selecting the set of parameters that provides
the best performance for a given input, based on some model of
performance that is generated offline. Performance models can either
be predictive, in that they attempt to characterise performance as a
function of the optimisation parameters and input, or empirical, in
that they predict performance based on empirical data gathered by
evaluating many different parameter configurations. In both cases, a
performance function $f(p,x)$ models the relationship between a set of
parameters $p$, a specific problem $x$, and some profitability
goal. The purpose of the offline tuning phase is to select the set of
parameters $p_{optimal}$ which maximises the output of the performance
model:
\begin{align*}
  p_{optimal} = \argmax_{p}f(p,x)
\end{align*}
For predictive models, the quality of results is limited by the
ability of the prediction function to accurately capture the behaviour
of a real world system. Given the complexities of modern architectures
and software stacks, such models have become increasingly hard to
develop.
% TODO: good for unknown inputs.
The quality of empirical models is limited by the amount of training
data available to it, and the ability to interpolate between training
data when faced with new unknown inputs.

An example of an empirical approach to offline tuning is iterative
compilation, which uses an offline training phase to perform an
extensive search of the optimisation space of a program by compiling
and evaluating the same program with different combinations of
compiler transforms in order to select the set that provides the best
performance.

Iterative compilation techniques has been successfully applied to a
range of optimisation challenges, particularly when combined with
machine learning techniques to focus the number of evaluations of
training programs which are required~\cite{Agakov}. MILEPOST GCC is a
research compiler that uses iterative compilation to adjust compiler
heuristics for optimising programs on different
architectures~\cite{Fursin2008}. Machine learning techniques are
applied to model the large optimisation space based on static features
extracted from the source code of training programs. A classifier
predicts optimisation parameters for new programs by comparing the
extracted program features against the training data.

An alternative approach to the problem of gathering sufficient
training data is to distribute the task of collecting it by enabling
the results of different program evaluations to be shared with a
central remote database. This has been successfully applied to offline
autotuners, in which a remote server is used to store and retrieve
training data~\cite{Fursin2014, Auler2014}. The overhead of
communicating with this remote server would be too great to use
dynamically, a typical 150ms network round trip time in the
performance critical path of a dynamic autotuner will cause a serious
degradation of performance.

An offline tuning tool with particular relevance to this work is
MaSiF~\cite{Collins2013}, a static autotuner which uses iterative
compilation techniques to perform a focused search of the optimisation
space of FastFlow and Intel Thread Building Blocks, two popular
Algorithmic Skeleton libraries. While sharing the same goal as MaSiF,
the approach of this project focuses on performing optimisation space
searching at runtime, and targets multi-GPU programming.

\subsection{Dynamic optimisation}\label{subsec:dynamic-optimisation}
Dynamic optimisation is a very different approach to the problem of
performance optimisation than offline tuning. Whereas offline tuning
typically requires an expensive training phase to search the space of
possible optimisations, dynamic optimisers perform this optimisation
space exploration at runtime, allowing programs to respond to dynamic
features ``online''. This is a challenging task, as a random search of
an optimisation space will typically result in many configurations
with vastly suboptimal performance. In a real world system, evaluating
many suboptimal configurations will cause a significant slowdown of
the program. Thus a requirement of dynamic optimisers is that
convergence time towards optimal parameters is minimised.

Existing dynamic optimisation research has typically taken a low level
approach to performing optimisations. Dynamo is a dynamic optimiser
which performs binary level transformations of programs using
information gathered from runtime profiling and
tracing~\cite{Bala2000}. While this provides the ability to respond to
dynamic features, it restricts the range of optimisations that can be
applied to binary transformations. These low level transformations
cannot match the performance gains that higher-level parameter tuning
produces.

One of the biggest challenges facing the implementation of dynamic
optimisers is to minimise the runtime overhead so that it does not
outweigh the performance advantages of the optimisations. A
significant contributor to this runtime overhead is the requirement to
compile code dynamically. \citeauthor{Fursin2005} negated this cost by
compiling multiple versions of a target subroutine ahead of
time~\cite{Fursin2005}. At runtime, execution switches between the
available versions, selecting the version with the best
performance. In practice, this technique massively reduces the
optimisation space which can be searched as it is unfeasible to insert
the thousands of different versions of a subroutine that are tested
using offline tuning.

Many existing dynamic optimisation systems do not store the results of
their efforts persistently, allowing the training data to be lost when
the host process terminates. This approach relies on the assumption
that either the convergence time to reach an optimal set of parameters
is short enough to have negligible performance impact, or that the
runtime of the host process is sufficiently long to reach an optimal
set of parameters in good time. Neither assumption can be shown to fit
the general case. This has led to the development of collective
compilation techniques, which involve persistently storing the results
of successive optimisation runs using a database~\cite{Fursin2010}.

PetaBricks attempts to capture higher-level optimisation decisions
than are available to dynamic optimisers~\cite{Ansel2009a}. It
consists of a language and compiler which allows programmers to
express algorithms that target specific dynamic features, and to
select which algorithm to execute at runtime. This provides a
promising optimisation space but has the drawback of increasing
programmer effort by requiring them to implement multiple versions of
an algorithm tailored to different optimisation parameters.

SiblingRivalry poses an interesting solution to the challenge of
providing sustained quality of service~\cite{Ansel2012}. The available
processing units are divided in half, and two copies of a target
subroutine are executed simultaneously, one using the current best
known configuration, and the other using a trial configuration which
is to be evaluated. If the trial configuration outperforms the current
best configuration, then it replaces it as the new best
configuration. By doing this, the tuning framework has the freedom to
evaluate vastly suboptimal configurations while still providing
adequate performance for the user. However, a large runtime penalty is
incurred by dividing the available resources in half.
% TODO: reword this week last sentence ^^
