%%%%%%%%%%%%%%%%%%%%%%
%% Document details %%
%%%%%%%%%%%%%%%%%%%%%%

% Author
\author{Chris Cummins}

% Date (Month Year)
\date{December 2014}

% Paper title
\title{Dynamic Autotuning\\of Algorithmic Skeletons}
%\newcommand{\multilinetitle}{}

% Subtitle
\newcommand{\subtitle}{MSc Research Proposal}

% Degree title
\newcommand{\degreeTitle}{MSc by Research\\ Pervasive Parallelism}

% Institution
\newcommand{\institution}{School of Informatics,\\
  The University of Edinburgh}

\input{_\jobname.tex}

%%%%%%%%%%
%% Body %%
%%%%%%%%%%
\section{Introduction}
Parallel computing is increasingly seen as the only viable approach to
maintaining continued performance improvements in a multicore
world. Despite this, the adoption of parallel programming practises
has been slow and awkward, due to the prohibitive complexity and low
level of abstraction available to parallel programmers.

Algorithmic Skeletons address this issue by providing reusable
patterns for parallel programming, offering higher level abstractions
and reducing programmer effort~\cite{Cole1989, Cole2004}. Tuning these
Algorithmic Skeletons is a manual process which requires exhaustively
searching the optimisation space to select optimum parameters.

The aim of this project is to demonstrate that the tuning of
optimisation parameters can be successfully performed at runtime. This
would enable self-tuning programs which adapt to their execution
environment by selecting optimum configurations dynamically. Such
configurations could be learned over time, allowing each successive
iteration of a program to benefit from its predecessors.

The case for self-tuning applications is strong. Previous research has
demonstrated that there are many factors which contribute to
performance that cannot be determined by when developing a program. As
a result, program optimisation requires the programmer to either
overfit the choice of parameters to optimise for a specific task and
environment, or laboriously create heuristics which segment the
possible space of different environments with sets of optimisation
parameters for each. Both approaches have significant drawbacks:
optimising for a specific task and environment creates brittle and
non-portable optimisations that do not generalise to other
architectures and problems; and the task of creating heuristics which
cover every possible combination of factors is doomed to fail due to
the prohibitive amount of time required to develop them.

This project proposes two hypotheses:
\begin{itemize}
\item a dynamic autotuner will improve the performance of Algorithmic
  Skeletons by enabling them to adapt to their environment at runtime;
\item and a dynamic autotuner will provide better performance than a
  statically tuned equivalent implementation, since it is able to
  adapt to features which can only be determined dynamically.
\end{itemize}

It is my hypothesis that the performance an optimisation system which
will be improved by developing a dynamic autotuner.  which considers
dynamic features which cannot be determined at compile time. The
premise is that the optimisation spaces of Algorithmic Skeletons are
shaped by features which can only be determined at runtime. Effective
searching of these spaces can only be performed by collecting
empirical data rather than building predictive models. To justify this
hypothesis, support for dynamic autotuning will be added to SkelCL, a
C++ Algorithmic Skeleton Framework which targets heterogeneous
parallel programming using OpenCL.

% mapreduce
~\cite{Dean2008}
% intel tbb
~\cite{IntelTBB}

% snippet
Parallel computing, traditionally the remit of scientific programming,
is increasingly being seen as the only viable approach for maintaining
continued performance improvements in a multicore computing world.
Despite its growing popularity, writing robust parallel software is an
inherently challenging task, requiring the programmer to think in
unfamiliar paradigms, and with many associated problems raised by race
conditions, deadlock, and managing access to shared resources.

% snippet
Equivalent parallel programs require many more lines of code than
their sequential counterparts, and the additional programmer effort
that is required is dedicated to writing coordination logic -- the
logic responsible for allocating and coordinating access to shared
resources. Algorithmic Skeletons address the difficulties of parallel
programming by providing a higher level abstraction that encapsulates
this coordination logic, ridding the application programmer of these
responsibilities and allowing them to focus instead on the
problem-solving logic.

% snippet
Previous research has attempted to address this issue using iterative
compilation techniques, by tuning the performance of Algorithmic
Skeletons through searching the optimisation space offline and
selecting a set of parameter values that provide optimum performance
for a given program. Such tools perform static autotuning, that is,
they automatically tune optimisation parameters based on static
features which can be selected at compile time.

% snippet
Research interest in Algorithmic Skeletons is high, and while
frameworks of Algorithmic Skeletons abound, widespread adoption has so
far been restricted largely to established use cases that rely heavily
on high performance computing, for example, Google's MapReduce, and
Intel's Thread Building Blocks. While the demand for such frameworks
in the field of High Performance Computing (HPC) is self-evident, this
should by no means blinker the ambitions of skeletons research. The
benefits of Algorithmic Skeletons extend beyond that of HPC and cover
general purpose computing.

% snippet
The popularity of programming with parallel patterns is rapidly
increasing, as they have been demonstrated as a means of providing
re-usability to the thousands of man-hours that is required to write a
tuned and stable parallel application. For example, Google's
MapReduce, which allows their programmers to write data sorting
programs in 55 lines of code, while taking advantage of the over
800,000 lines of code present in a MapReduce implementation. Any
research forwarding the cause of parallel patterns will prove
extremely valuable to both application developers and future
researchers.

\subsection{Motivating Example}
% TODO: Static parameter tuning example

To give a concrete example, consider a recursive merge sorting
algorithm. When called, the algorithm determines whether the input
list is short enough to be solved directly using a linear sorting
method, or whether it should split it into multiple sub-lists and sort
them by recursing on each sub-list before combining the results. This
computational pattern of repeatedly dividing a problem into smaller
subproblems which are then recombined is abstracted by a Divide and
Conquer pattern. This can be parallelised effectively by considering
each recursion as a new task which can be executed concurrently.

The parallel Divide and Conquer pattern is a common form of
Algorithmic Skeleton, whereby the user provides muscle functions for
the split, merge, and conquer logic, and the skeleton can coordinate
the allocation of new tasks. When implementing such a Divide and
Conquer skeleton, there are two immediate parameters which will
greatly affect the performance: the maximum depth at which recursion
should occur as a new task, as opposed to sequentially; and the
threshold minimum size of the input problem before the problem is
conquered directly rather than recursively.

Existing iterative compilation techniques can perform an exhaustive
search of the optimisation space generated by these two parameters,
which would reveals a strong interaction between them: the optimum
value for one parameter is strongly influenced by the value of the
other parameter. Additionally, the optimisation space of both
parameters is strongly influenced by an independent factor: the size
and type of the input problem. This means that in the case of a
parallel merge sort algorithm, the optimum values for the max
recursion depth and minimum input threshold parameters will be very
different when sorting lists of integers and lists of bytes, or lists
of arbitrary user-chosen data structures. This cannot be modelled
using iterative compilation techniques, as the size and type of the
input problem a dynamic features, which can only be determined at
runtime.

Static approaches to this problem involve segmenting the dynamic
feature space using heuristics in order to select optimum values for
approximate ranges. The effectiveness of these heuristics is limited
by their complexity and the thoroughness of the optimisation space
search. In addition, the resulting optimisation heuristics would be
very fragile and non-portable, so that the whole tedious process would
need to be repeated for every target architecture, and with every new
generation of hardware. Such an approach is clearly
impractical. Compare this to the alternate approach of a Divide and
Conquer skeleton which is capable of performing this empirical data
gathering online and during normal execution, and which will use
successive iterations to converge naturally upon an optimum
configuration. Such a system would be capable of dealing with varying
dynamic features which would destroy the capabilities of a static
heuristic based system. This is the goal of my research.

\section{Background}

It is helpful to draw on existing research from the fields of
iterative compilation and dynamic optimisation.

% snippet
Many existing dynamic optimisation systems do not store the results of
their efforts persistently, allowing the work to die along with the
host process. This approach relies on the assumption that either that
the convergence time to reach an optimal set of parameters is short
enough to have negligible performance impact, or that the runtime of
the host process is sufficiently long to reach an optimal set of
parameters in good time. Neither assumption can be shown to fit the
general case.

% snippet PETABRICKS
PetaBricks is a language and runtime which supports dynamic
algorithmic choice determined by properties of the data input. This
provides a promising space for optimisation but has the drawback of
increasing programmer effort by requiring them to implement multiple
versions of an algorithm tailored to different optimisation
parameters. SkelCL has the advantage of being able to localise this
extra programmer effort into a single library implementation.

% snippet SIBLING RIVALRY
SiblingRivalry poses an interesting solution to the challenge of
providing sustained quality of service. Resources are divided in half,
and two copies of a target subroutine are executed simultaneously, one
using the current best known configuration, and the other using a
trial configuration which is to be evaluated. If the trial
configuration outperforms the current best configuration, then it
replaces it as the new best configuration. By doing this, the tuning
framework has the freedom to evaluate vastly suboptimal configurations
while still providing adequate performance for the user. However, a
large runtime penalty is incurred by dividing the available resources
in half.

% snippet WHY MAP REDUCE SUCKS
In such cases, the overhead introduced by these massively scaleable
high performance skeleton libraries would likely outweigh the
performance gains. If Algorithmic Skeletons are to achieve widespread
adoption, they must provide scalable performance benefits not only to
the upper-tier of high performance computers but also to modest
consumer hardware, which is increasingly reliant on software
parallelism in order to achieve performance improvements.

% snippet
MapReduce is a hugely successful framework for writing high
performance massively distributed applications at a fraction of the
effort required for a hand tuned implementation. The source code of
Hadoop is over 800,000 lines of code, but efficient user programs can
be written in well under 100 lines. While is an incredible technical
feat, the overhead the huge runtime would negate the performance
advantages for all but the largest computations. Users writing
programs for more modestly specced off the shelf hardware will not be
able to take advantage of the engineering achievement.

% snippet What are your claims or hypotheses?
The problem with attempting to model optimisation spaces is that they
are fundamentally stochastic. As a result, they can only properly be
built using empirical evidence, and so the problem becomes one of
trying to extrapolate complicated many-dimensional spaces using the
least amount of data points, since the time taken to acquire data is
prohibitively expensive. Previous static autotuners have taken as long
as three months to sample the optimisation space.

\subsection{Iterative compilation}
Iterative compilation is an approach to autotuning which uses an
offline training phase to perform an extensive search of the
optimisation space of a program.  Empirical data is gathered through
repeatedly compiling and evaluating different trial configurations,
before selecting the configuration which proved the most
profitable. Iterative compilation techniques has been successfully
applied to a range of optimisation challenges. Of particular relevance
to this work is MaSiF, a static autotuning tool which combines
iterative compilation techniques with machine learning.  It performs a
focused search of the optimisation space of FastFlow and Intel Thread
Building Blocks, two popular Algorithmic Skeleton libraries. While
sharing the same goal as MaSiF, the approach of this project focuses
on performing optimisation space searching at runtime, without the
need for the expensive offline training phase, which is a prohibitive
drawback of iterative compilation.

% collective optimisation
~\cite{Fursin2010}

\subsection{Dynamic optimisation}
Whereas iterative compilation requires an expensive offline training
phase to search an optimisation space, dynamic optimisers perform this
optimisation space exploration at runtime, allowing programs to
respond to dynamic features ``online''. This is a challenging task, as
a random search of the optimisation space will result in many
configurations with vastly suboptimal performance. In a real world
system, evaluating many suboptimal configurations will cause a
significant slowdown of the program. Thus a requirement of dynamic
optimisers is that convergence time towards optimal parameters is
minimal.

Existing dynamic optimisation research has typically taken a low level
approach to performing optimisations. Dynamo is a dynamic optimiser
which performs binary level transformations of programs using
information gathered from runtime profiling and tracing. While this
provides the ability to respond to dynamic features, it restricts the
range of optimisations that can be applied to binary
transformations. These low level transformations cannot match the
performance gains that higher level parameter tuning produces.

One of the biggest challenges facing the implementation of dynamic
optimisers is to minimise the runtime overhead so that it does not
outweigh the performance advantages of the optimisations. A
significant contributor to this runtime overhead is the requirement to
compile code dynamically. Previous research has negated this cost by
compiling multiple versions of a target subroutine ahead of time. At
runtime, execution switches between the available versions, selecting
the version with the best performance. In practice, this technique
massively reduces the optimisation space which can be searched as it
is unfeasible to insert the thousands of different versions of a
subroutine that are tested using offline autotuning.

\section{Methodology}

The novelty of the approach posed in this research is to combine the
advantages of offline training phases and online parameter tuning by
implementing a dynamic autotuner which maintains persistent data
in-between program executions using SkelCL.

Michel Steuwer, a research associate at the University of Edinburgh,
developed SkelCL as an approach to high-level programming for
multi-GPU systems. Steuwer demonstrated an $11\times$ reduction in
programmer effort compared to implement equivalent programs written in
pure OpenCL, while suffering only a modest 5\% overhead. The core of
SkelCL comprises a set of parallel container data types for vectors
and matrices, and an automatic distribution mechanism which performs
implicit transfer of these data structures between the host and device
memory. Application programmers express computations on these data
structures using Algorithmic Skeletons that are parameterised with
small sections of OpenCL code. At runtime, SkelCL compiles the OpenCL
code into compute kernels for execution on GPUs. This makes SkelCL an
excellent candidate for dynamic autotuning, as it exposes both the
optimisation space of the OpenCL compiler, and the high level tunable
parameters provided by the structure of Algorithmic Skeletons. SkelCL
offers the unique advantage of being able to amortise many of the
costs associated with dynamic compilation due to its JIT-like nature
of compiling OpenCL kernels immediately before execution.

Implementing a dynamic optimiser poses a number of difficult
challenges which must be overcome.
% TODO: What are these challenges? ^^
There is a risk that the runtime overhead of the dynamic optimiser
will exceed the performance gained by the optimisations
themselves. The proposed approach to dynamically autotune SkelCL will
overcome one of the most significant overheads associated with dynamic
optimising: that of instrumenting the code the purposes of profiling
and tracing. Since Algorithmic Skeletons coordinate muscle functions,
it is possible to forgo many of the profiling counters that dynamic
optimisers require by making assumptions about the execution frequency
of certain code paths, given the nature of the skeleton. Additionally,
the placement of profiling counters can be optimised manually.

% skelcl
~\cite{Steuwer2011, Steuwer2013a}

% skelcl 11x 5%
~\cite{Steuwer2012}.

% snippet
Contributors to this overhead include: time spent evaluating dynamic
features and deciding on which optimisations to select (extra
instructions to execute), and either increased code size from having
multiple copies of procedures (bad for branch predictors / instruction
prefetch), or decreased ability for optimisations (because of setting
parameters at runtime instead of at compiled).

% snippet
Whereas current approaches to Algorithmic Skeleton autotuning have
largely relied on huge offline training periods and optimising for
static features, this proposed research will develop an online
autotuner which is capable of adapting to dynamic features at runtime.

\section{Evaluation}
My hypothesis is that the performance of Algorithmic Skeletons will be
improved by using dynamic autotuning. After implementing a prototype
dynamic autotuner, experimental evidence will be collected to support
or refute this hypothesis: standard empirical methods will be used to
evaluate the performance of a range of representative
benchmarks. Careful selection of these benchmarks

The evaluation methodology must incorporate statistically rigorous
performance evaluation techniques \cite{Georges2007}.

As a result, evidence must be collected to support

To test this hypothesis, I will
collect empirical data from a suite of representative benchmarks,
comparing the performance of my implementation against: baseline
performance provided by an unmodified SkelCL implementation;\ and a
hand-tuned ``oracle'' implementation using an optimal configuration
discovered through an offline exhaustive search of the optimisation
space.

Other measurable success metrics include: the overhead introduced by
the runtime; the amount of time required to converge to a sufficiently
good configuration; and the ability of the dynamic optimiser to adapt
to changes in dynamic features (e.g.\ system load). All of these
metrics will be evaluated by profiling performance benchmarks.

% important factors: amount of training data, variance in inputs

\section{Work plan}
The schedule for this project is shown in Figure~\ref{fig:gantt}. In
addition to the internal progress presentation in April, a set of
personal milestones will be used to provide intermediate progress
checks. All source code and experimental results will be tracked using
the git version control system\footnote{\url{http://git-scm.com/}},
and GitHub\footnote{\url{https://github.com/}} will be used to track
issues and milestone progress.

\begin{figure}[b]
\makebox[\textwidth][c]{\input{fig/gantt}}
\caption{Project schedule Gantt chart.}
\label{fig:gantt}
\end{figure}

\section{Conclusion}
Existing research has shown that Algorithmic Skeletons improve
programmer effectiveness for a range of tasks from general purpose
computing, to bioinformatics, and complex simulations. For example,
the SkelCL library has been used to implement high performance medical
imaging applications. A dynamic autotuner for SkelCL will improve the
performance of these applications, and provide a starting point for
future research into the online autotuning of Algorithmic Skeletons.

% snippet
We are ideally suited for tackling this difficult problem at
University of Edinburgh. Not only have academic members been
responsible for introducing and developing Algorithmic
Skeletons~\cite{Cole1989, Cole2004, Benoit2005a}, but there is a large
and active research interest in iterative compilation and machine
learning based optimisation~\cite{Fursin2008, Agakov,
Fursin2005}. Previous research at the University of Edinburgh has also
approach the static autotuning of Algorithmic
Skeletons~\cite{Collins2012, Collins2013}, which will provide a solid
source of inspiration and an interesting counterpoint for evaluating
the performance of a dynamic autotuning approach.

% snippet
While iterative compilation is a very well studied field, fewer papers
have been published about dynamic optimisation. Therefore work in this
field has a greater chance of influencing future research, besides the
primary benefit of improving the performance of algorithmic skeletons.

\input{_\jobname-post.tex}
