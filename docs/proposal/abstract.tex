The rapid transition towards multicore hardware which has left
application programmers requiring higher-level abstractions for
dealing with the complexity of parallel programming. Algorithmic
Skeletons provide such abstractions, but typically cannot compete with
the performance of hand optimised parallel algorithms without
extensive tuning. This paper proposes the development of a dynamic,
``always-on'' autotuner for SkelCL, an Algorithmic Skeleton library
which enables high-level programming of multi-GPU systems. An online
machine learning system will create a feedback loop of constant
testing and evaluation of skeleton parameters across the lifespan of
programs. Dynamically extracted features will characterise muscle
functions and input data, and runtime performance data will be used to
select optimisation parameters. Such a system will extend the state of
the art by enabling empirical optimisation without the huge offline
training phases associated with iterative compilation.
