Autotuning has been shown to provide significant performance
improvements for a range of problems, and has led to the development
of empirical driven optimisation systems. These systems typically
require huge offline training periods in which many thousands of
different configurations are tested, which severely impacts the
practicality of such systems. If widespread adoption is to be
achieved, these systems must be implemented dynamically. This paper
proposes the development of a dynamic autotuner for SkelCL, an
Algorithmic Skeleton library which targets heterogeneous
parallelism. By combining always-on autotuning with persistent
training data and dynamic features, a self-tuning library for
high-level GPU programming will be developed.
