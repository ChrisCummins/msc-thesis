\title{Dynamic Autotuning\\of Algorithmic Skeletons}
\author{Chris Cummins}
\date{November 2014}

\input{_\jobname.tex}

\begin{document}

% Title.
\begin{center}
  \huge
  \textbf{\@title}
  \vspace{.5em}

  \normalsize
  \@author\\
  \@date
  \vspace{.5em}
\end{center}

Algorithmic Skeletons enable the rapid development of parallel
software by equipping programmers with a set of reusable patterns for
parallel programming. While they have been shown to reduce programmer
effort by raising the providing higher level abstractions, this comes
at the expense of a performance overhead, so they cannot compete with
the performance of hand tuned parallel programs.

Previous research has attempted to address this issue using iterative
compilation techniques, by tuning the performance of Algorithmic
Skeletons through searching the optimisation space and selecting a set
of parameter values that provide optimum performance for a given
program. Such tools perform static autotuning, that is, they
automatically tune optimisation parameters based on static features
which can be selected at compile time.

It is my hypothesis that the performance of Algorithmic Skeletons can
be improved by developing an autotuner which considers dynamic
features, that is, features which cannot be determined at compile
time. The premise is that the optimisation spaces of Algorithmic
Skeletons are shaped significantly by features which can only be
determined at runtime, and that effective searching of these spaces
can only be performed by collecting empirical data rather than
building predictive models.

The objective of this research is improve the performance of
Algorithmic Skeletons by enabling them to adjust their behaviour at
runtime. This will be demonstrated by adding dynamic autotuning to
SkelCL, a C++ Algorithmic Skeleton Framework which targets
heterogeneous parallel programming using OpenCL.

% What are the main pieces of related work?
In developing a dynamic autotuner for Algorithmic Skeletons, it is
helpful to draw on existing research from the fields of iterative
compilation and dynamic optimisation.

% Has a similar solution to yours been applied to different problems?
Iterative compilation is an approach to autotuning which uses an
offline training phase to perform an extensive search of a program's
optimisation space by gathering empirical data through repeatedly
compiling and evaluating different trial configurations. Iterative
compilation techniques has been successfully applied to a range of
optimisation challenges. Of particular relevance to this work is
MaSiF, a static autotuning tool which combines iterative compilation
techniques with machine learning to perform a focused search on the
optimisation space of FastFlow and Intel Thread Building Blocks, two
popular Algorithmic Skeleton libraries. While sharing the same goal as
MaSiF, the approach of this project focuses on performing optimisation
space searching at runtime, without the need for the expensive offline
training phase, which is a prohibitive drawback of iterative
compilation.

Whereas iterative compilation requires an expensive offline training
phase to search an optimisation space, dynamic optimisers perform this
search space exploration at runtime, allowing programs to respond to
dynamic features ``online''.

This is a challenging task, as a random search of the optimisation
space will result in many configurations with vastly suboptimal
performance. In a real world system, it is not satisfactory to be
evaluating many suboptimal configurations, as this will cause
significant slowdown of the program. Thus dynamic optimisers must be
designed so that convergence time towards optimal parameters is
minimal.

Existing dynamic optimisation research has typically taken a low level
approach to performing optimisations. Dynamo is a dynamic optimiser
which performs binary level transformations of programs using
information gathered from runtime profiling and tracing. While this
provides the ability to respond to dynamic features, the range of
optimisations that can be applied is limited to binary
transformations, and so it cannot perform the high level parameter
tuning which can often have greatest impact on performance.

One of the biggest challenges facing the implementation of dynamic
optimisers is that of minimising the runtime overhead so that it does
not outweigh the performance advantages of the optimisations. A
significant contributor to this runtime overhead is provided by the
required to compile code dynamically. This cost can be negated by
compiling multiple versions of a target subroutine ahead of time. At
runtime, execution switches between the many versions, selecting the
version with the best performance. This technique can prove very
successful for selecting optimum configurations, but massively reduces
the optimisation space which can be searched, as it is unfeasible to
insert the thousands of different versions of a subroutine that is
performed in offline autotuning.

% SIBLING RIVALRY
SiblingRivalry poses an interesting solution to the challenge of
providing sustained quality of service. Resources are divided in half,
and two copies of a target subroutine are executed simultaneously, one
using the current best known configuration, and the other using a
trial configuration which is to be evaluated. If the trial
configuration outperforms the current best configuration, then it
replaces it as the new best configuration. By doing this, the tuning
framework has the freedom to evaluate vastly suboptimal configurations
while still providing adequate performance for the user. However, a
large runtime penalty is incurred by dividing the available resources
in half.

My novel approach to the problem of Algorithmic Skeleton autotuning
will be to take the advantages of offline training phases by
implementing a dynamic optimiser which maintains persistent data
in-between program executions, while having the advantages of dynamic
optimisation by performing the search of the optimisation space at
runtime, using SkelCL.

Michel Steuwer, a research associate at the University of Edinburgh,
developed SkelCL as an approach to high-level programming for
multi-GPU systems, demonstrating an $11\times$ reduction in programmer
effort compared to implement equivalent programs written in pure
OpenCL, while suffering only a modest 5\% overhead.

The core of SkelCL comprises a set of parallel container data types
for vectors and matrices, with an automatic distribution mechanism
which performs implicit transfer of these data structures between the
host and device memory. Computations on these data structures are
expressed using Algorithmic Skeletons which are parameterised with
small sections of OpenCL code. At runtime, the OpenCL code is compiled
into compute kernels for execution of GPUs. This makes SkelCL an
excellent candidate for dynamic autotuning, as it exposes not only the
optimisation space of the OpenCL compiler, but also the high level
tunable parameters provided by the structure of Algorithmic
Skeletons. SkelCL offers the unique advantage of being able to
amortise many of the costs associated with dynamic compilation due to
it's JIT-like nature of compiling OpenCL kernels immediately before
execution.

Implementing a dynamic optimiser poses many difficult challenges which
must be overcome. There is a risk that the runtime overhead of the
dynamic optimiser will exceed the performance gained by the
optimisations themselves. The proposed approach to dynamically
autotune SkelCL will overcome one of the most significant overheads
associated with dynamic optimising, which is that of profiling and
tracing. Since Algorithmic Skeletons perform the coordination of
muscle functions, it is possible to forgo the use of many profiling
counters by being to make assumptions about the execution frequency of
certain code paths, given the nature of the skeleton. Additionally,
the placement of profiling counters can be optimised manually.

% What kind of evidence will be needed to support these claims or
% hypotheses? Is your evidence experimental or theoretical? Is it
% amenable to statistical analysis?
My hypothesis is that the performance of Algorithmic Skeletons can be
improved using dynamic autotuning. To test this hypothesis, empirical
data must be collected from several benchmarks by comparing the
performance of my implementation against: a baseline performance
provided by an unmodified SkelCL implementation;\ and a hand-tuned
``oracle'' implementation, using an optimal configuration discovered
through an offline exhaustive search of the optimisation space.

Other measurable success metrics include: the overhead introduced by
the runtime; the amount of time required to converge to a sufficiently
good configuration; and the ability of the dynamic optimiser to adapt
to a change in dynamic features (e.g.\ system load). All of these
metrics can be evaluated by profiling performance benchmarks.

% Who would benefit from a solution to the problem you have set
% yourself?
Algorithmic Skeletons have been shown to improve programmer
effectiveness for parallel programming, and have many uses, from
general purpose computing to bioinformatics and complex
simulations. For example, the SkelCL library has been used to
implement high performance medical imaging applications. A dynamic
autotuner for SkelCL would improve the performance of these
applications, and provide a starting point for future research into
the online autotuning of Algorithmic Skeletons.

% At what stage is your project? Are you just formulating your project
% proposal, is the work in progress or is it finished or nearly
% finished? What further work do you still have to do or have you
% identified for others to do after you?

\end{document}
