\title{Dynamic Autotuning\\of Algorithmic Skeletons}
\author{Chris Cummins}
\date{November 2014}

\input{_\jobname.tex}

\begin{document}

% Title.
\begin{center}
  \huge
  \textbf{\@title}
  \vspace{.5em}

  \normalsize
  \@author\\
  \@date
  \vspace{.5em}
\end{center}

\section*{Introduction}
Algorithmic Skeletons enable the rapid development of parallel
software by equipping application programmers with a set of reusable
patterns for parallel programming. While they have been shown to
reduce programmer effort, the higher level abstractions offered by
Algorithmic Skeletons result in lower performance when compared to
equivalent hand tuned programs. Previous research has demonstrated
that algorithmic skeletons are ameanable

%% What are your aims and objectives?
The objective of this research is to demonstrate that the performance
of Algorithmic Skeletons can be improved by enabling them to adjust
their behaviour at runtime, which will not only help amortise the
computational cost of these higher level abstractions, but
additionally provide a platform for performing empirical
results-driven optimisations that would take enormous programmer
effort to perform by hand. This will be achieved by enabling the
dynamic autotuning of Algorithmic Skeletons.

The central premise of this research is that the large range of
tunable parameters and optimisations that influence the performance of
parallel skeletons can only be effectively searched using empirical
data rather than predictive models, and that the gathering of
empirical data that is required to find optimal parameter settings is
prohibitively time consuming.

\subsection{Motivating Example}

To give a concrete example, consider a recursive merge sorting
algorithm. When called, the algorithm determines whether the list is
short enough to be solved directly using a linear sorting method, or
whether it should split the input list into multiple sub-lists and
sort them by recursing on each sub-list before combining the
results. This computational pattern of repeatedly dividing a problem
into smaller subproblems which are then recombined is abstracted by a
Divide and Conquer pattern. This can be parallelised effectively by
considering each recursion as a new task which can be executed
concurrently.

A parallel Divide and Conquer pattern is a common form that can be
implemented as an Algorithmic Skeleton, allowing the user to supply
muscle functions for the split, merge, and conquer logic, and the
skeleton can coordinate the allocation of new tasks. When implementing
such a Divide and Conquer skeleton, there are two immediate parameters
which will greatly affect the performance: the maximum depth at which
recursion should occur as a new task, as opposed to sequentially; and
the threshold minimum size of the input problem before the problem is
conquered directly rather than recursively.

By performing an iterative search of the optimisation space generated
by these two parameters reveals a strong interaction between the two
parameters. The optimum value for one parameter is strongly influenced
by the value of the other parameter. Additionally, the optimisation
space of both parameters is strongly influenced by an independent
factor: the size and type of input problem. This means that in the
case of a parallel merge sort algorithm, the optimum values for the
max recursion depth and minimum input threshold parameters will be
very different when sorting lists of integers and lists of bytes, or
lists of arbitrary user-chosen data structures.

If the programmer were to attempt to optimise the selection of these
parameter values using static heuristics, he would need to perform an
extensive empirical search of the optimisation space, and then segment
the space using crude heuristics in order to pick best approximate
values. The effectiveness of these heuristics is limited by their
complexity and the thoroughness of the optimisation space search. In
addition, the resulting optimisation heuristics would be very fragile
and non-portable, so that the whole tedious process would need to be
repeated for every target architecture, and with every new generation
of hardware. Such an approach is clearly impractical. Compare this to
the alternate approach of a Divide and Conquer skeleton which is
capable of performing this empirical data gathering online and during
normal execution, and which will use successive iterations to converge
naturally upon an optimum configuration. Such a system would be
capable of dealing with varying dynamic features which would destroy
the capabilities of a static heuristic based system. This is the goal
of my research.

\section*{Background}
Parallel programming, traditionally the remit of scientific computing,
is increasingly being seen as the only viable approach for maintaining
continued performance improvements in a multicore computing world.
Despite its growing popularity, writing robust parallel software is an
inherently challenging task, requiring the programmer to think in
unfamiliar paradigms, and with many associated problems raised by race
conditions, deadlock, and managing access to shared resources.

A functionally equivalent parallel program will require many more
lines of code than its sequential counterpart, and this additional
programmer effort is dedicated to writing coordination logic - the
logic responsible for allocating and coordinating access to shared
resources. Algorithmic Skeletons address the difficulties of parallel
programming by providing a higher level abstraction that encapsulates
this coordination logic, ridding the application programmer of these
responsibilities and allowing them to focus instead on the
problem-solving logic.

% What are the main pieces of related work?
In developing a dynamic autotuner for Algorithmic Skeletons, it is
helpful to draw on existing research in the fields of iterative
compilation and dynamic optimisation.

One of the biggest challenges facing the implementation of dynamic
optimisers is that of minimising the runtime overhead so that it does
not outweigh the performance advantages of the optimisations. JIT
compilation is an expensive task and does not have access to high
level information about the program structure. Dynamic optimisers
require extensive profiling and tracing to identify hot paths and
regularly executed code, and recompiling these hot paths is an
expensive task. One possible workaround for this challenge is to
compile multiple versions of a routine ahead of time, and to switch
between them at runtime, selecting the version with the best
performance. This technique can prove very successful for optimising
programs dynamically, but massively reduces the search space, as it is
not feasible to compile thousands of different versions as is
performed in offline autotuning, but only a dozen or so different
versions.

Another challenge facing dynamic optimisation is that of the necessity
to provide Quality of Service. A random search of the optimisation
space will result in many configurations with vastly suboptimal
performance. If these configurations are being evaluated online on a
working program, then it is often not satisfactory to be running
massively slower implementations. The search of the optimisation space
must be designed so that convergence time towards optimal parameters
is minimal.

% TODO: reword
Many existing dynamic optimisation systems do not store the results of
their efforts persistently, allowing the work to die along with the
host process. This approach relies on the assumption that either that
the convergence time to reach an optimal set of parameters is short
enough to be amortized by the overhead of persistent storage, or that
the runtime of the host process is sufficiently long to reach an
optimal set of parameters in good time. Neither assumption can be
shown to fit the general case.

% SIBLING RIVALRY
SiblingRivalry poses an interesting solution to this problem, by
dividing available resources in half and executing two copies of the
target code, one using the current best configuration, and the other
using a trial configuration which is being evaluated. If the trial
configuration performs better than the best configuration, then it
replaces it as the new current best configuration. By doing this, the
tuning framework has the freedom to evaluate vastly suboptimal
configurations while still providing adequate performance for the
user. However, by dividing the available resources in half, you can
expect only 50\% of the performance versus a statically optimised
version.

% PETABRICKS
PetaBricks is a language and runtime which supports dynamic
algorithmic choice determined by properties of the data input. This
provides a promising space for optimisation but has the drawback of
increasing programmer effort by requiring them to implement multiple
versions of an algorithm tailored to different optimisation
parameters. SkelCL has the advantage of being able to localise this
extra programmer effort into a single library implementation.

% WHY MAP REDUCE SUCKS
In such cases, the overhead introduced by these massively scaleable
high performance skeleton libraries would likely outweigh the
performance gains. If Algorithmic Skeletons are to achieve widespread
adoption, they must provide scalable performance benefits not only to
the upper-tier of high performance computers but also to modest
consumer hardware, which is increasingly reliant on software
parallelism in order to achieve performance improvements.

MapReduce is a hugely successful framework for writing high
performance massively distributed applications at a fraction of the
effort required for a hand tuned implementation. The source code of
Hadoop is over 800,000 lines of code, but efficient user programs can
be written in well under 100 lines. While is an incredible technical
feat, the overhead the huge runtime would negate the performance
advantages for all but the largest computations. Users writing
programs for more modestly specced off the shelf hardware will not be
able to take advantage of the engineering achievement.

% Have other people tried to solve the same problem using a different
% solution?

% Has a similar solution to yours been applied to different problems?

% What is the state of the art in this field? What are the limitations
% of previous work on this problem?

\section*{Objectives}
% What do you hope to have achieved when you are finished?
The result of this research will be to embed an online, dynamic
autotuner in SkelCL, is a C++ Algorithmic Skeleton Framework which
targets heterogeneous parallel programming using OpenCL. Steuwer, a
research associate at the University of Edinburgh, developed SkelCL as
an approach to high-level programming of multi-GPU systems,
demonstrating an $11\times$ reduction in programmer effort compared to
equivalent programs written in pure OpenCL, while suffering only a
modest 5\% overhead.

The core of SkelCL comprises a set of parallel container data types
for vectors and matrices, with an automatic distribution mechanism
which performs implicit transfer of these data structures between the
host and device memory. Computations on these data structures are
expressed using Algorithmic Skeletons which are parameterised with
small sections of OpenCL code.

% What is your novel idea? What do you bring to this project that is
% new?
Whereas current approaches to Algorithmic Skeleton autotuning have
largely relied on huge offline training periods and optimising for
static features, this proposed research will develop an online
autotuner which is capable of adapting to dynamic features at runtime.

\section*{Methodology}
% What are your claims or hypotheses?
The problem with attempting to model optimisation spaces is that they
are fundamentally stochastic. As a result, they can only properly be
built using empirical evidence, and so the problem becomes one of
trying to extrapolate complicated many-dimensional spaces using the
least amount of data points, since the time taken to acquire data is
prohibitively expensive. Previous static autotuners have taken as long
as three months to sample the optimisation space.

One risk is that the overhead required to implement this dynamic
optimisation will exceed the performance gained from the optimisations
themselves. Contributors to the overhead include: time spent
evaluating dynamic features and deciding on which optimisations to
select (extra instructions to execute), and either increased code size
from having multiple copies of procedures (bad for branch predictors /
instruction prefetch), or decreased ability for optimisations (because
of setting parameters at runtime instead of at compiled).

Another risk is that optimising for dynamic features will not provide
a sufficient advantage over considering static features alone.

One of the greatest problems facing dynamic optimisers is the cost of
compiling code online. SkelCL offers the unique advantage of being
able to amortize many of the costs associated with dynamic
compilation, due to it's JIT-like nature of compiling OpenCL kernels
immediately before execution.

% WHAT IS SKELCL
The unique advantage of SkelCL is that it has the potential to take
advantage of both high level information about the structure and
intent of programs, while still having access to the low level
compilation optimisation processes of dynamic optimisers.

\section*{Evaluation}
% What kind of evidence will be needed to support these claims or
% hypotheses? Is your evidence experimental or theoretical? Is it
% amenable to statistical analysis?
The primary goal of this research is to modify the behaviour of SkelCL
so that it can autotune its performance at runtime. The question which
must be answered when evaluating this goal is: has the performance of
SkelCL been improved? This is itself is not an easy answer to
quantify.

% BASELINE: SkelCL
% Oracle: *perfect* parameters

The final success metrics will include:

\begin{itemize}
\item The overhead introduced by the runtime.
\item The amount of time to converge to a good set of optimised
parameters.
\item The speedup of the optimised skeletons relative to an
out-of-the-box implementation.
\item The speedup of the optimised skeletons relative to a state of the
art auto-tuned implementation.
\item The ability of the skeletons to adapt to a change in dynamic features (
e.g. system load).
\end{itemize}

This can be determined experimentally by timing the execution of three
versions of skeleton benchmark suite: a control group, a state of the
art autotuned version, and my dynamically optimised version.
% TODO: stat rigour

% Will you require a large corpus of test data?  How will your ensure
% that your test data is representative? What will provide the "gold
% standard" by which you judge the correctness and/or quality of your
% project's results?

It should be possible to get convincing results that such an autotuner
can improve performance at an early stage, by simply hand-hacking a
few benchmarks to show the differences between optimal and suboptimal
configurations.

\section*{Conclusion}

% Who would benefit from a solution to the problem you have set
% yourself?
Algorithmic Skeletons have been shown to improve programmer
effectiveness for parallel programming, and has many use cases, from
general purpose computing to bioinformatics and physics
simulations. The SkelCL library has been used to implement high
performance medical imaging applications. A dynamic autotuner for
SkelCL would improve the performance of these applications, and
provide a starting point for future research into online autotuning
for other applications, as well as being directly applicable to the
implementation of other Algorithmic Skeleton libraries.

% At what stage is your project? Are you just formulating your project
% proposal, is the work in progress or is it finished or nearly
% finished? What further work do you still have to do or have you
% identified for others to do after you?

We are ideally suited for tackling this difficult problem at
University of Edinburgh. Not only have academic members been
responsible for introducing and developing Algorithmic Skeletons, but
there is a large and active research interest in iterative compilation
and machine learning based optimisation. Previous research at the
University of Edinburgh has also approach the static autotuning of
Algorithmic Skeletons, which will provide a solid source of
inspiration and an interesting counterpoint for evaluating the
performance of a dynamic autotuning approach.

\newpage
\section*{--break--}

They achieve this by abstracting common patterns of parallel
programming into specialised parallel higher order functions that
orchestrate the execution of ``muscle functions'' - small sections of
problem-solving logic provided by the user.
% TODO: example

Research interest in Algorithmic Skeletons is high, and while
frameworks of Algorithmic Skeletons abound, widespread adoption has so
far been restricted largely to established use cases that rely heavily
on high performance computing, for example, Google's MapReduce, and
Intel's Thread Building Blocks. While the demand for such frameworks
in the field of High Performance Computing (HPC) is self-evident, this
should by no means blinker the ambitions of skeletons research. The
benefits of Algorithmic Skeletons extend beyond that of HPC and cover
general purpose computing.

% Why do you think you can extend the state of the art? Can you solve
% any previously unsolved problems or overcome limitations of previous
% work? Why might you succeed where others have failed?

%%%%%%%%%%%%%%
% MOTIVATION %
%%%%%%%%%%%%%%

% What is the motivation of your project? That is, why do you think it
% will make a significant and original contribution to the scientific
% and/or engineering progress of Informatics?
While iterative compilation is a very well studied field, fewer papers
have been published about dynamic optimisation. Therefore work in this
field has a greater chance of influencing future research, besides the
primary benefit of improving the performance of algorithmic skeletons.

The project is motivating by empirical evidence showing that
optimising algorithms requires both static information about the
structure of the algorithm, and dynamic information about the type and
nature of algorithm inputs. Or rather, static optimisations cannot
provide a universally optimum configuration for all inputs. For
example, a sorting algorithm may display different asymptotic
performance based on the type of data being sorted, the size of the
list to be sorted, and the extent to which the list is already
sorted. Since these factors cannot be determined at compilation time,
programmers are reliant on crude static heuristics to switch been
different optimisations, or attempting to build models to predict
dynamic optimisation configurations. The modelling approach has been
know to fail due to the time required to build models - varying
architectures exhibit wildly different behaviours, etc.

% WHO CARES

% Why is it timely to tackle this project now and do you think it is
% feasible to achieve in the timescale available to you?

The popularity of programming with parallel patterns is rapidly
increasing, as they have been demonstrated as a means of providing
re-usability to the thousands of man-hours that is required to write a
tuned and stable parallel application. For example, Google's
MapReduce, which allows their programmers to write data sorting
programs in 55 lines of code, while taking advantage of the over
800,000 lines of code present in a MapReduce implementation. Any
research forwarding the cause of parallel patterns will prove
extremely valuable to both application developers and future
researchers.

\end{document}
