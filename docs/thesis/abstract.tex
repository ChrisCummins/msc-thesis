The physical limitations of microprocessor design have forced the
industry towards increasingly heterogeneous architectures to extract
performance. This trend has not been matched with software tools to
cope with such parallelism, leading to a growing disparity between the
levels of available performance and the ability for application
developers to exploit it.

Algorithmic Skeletons simplify parallel programming by providing
high-level, reusable patterns of computation. Achieving performant
skeleton implementations is a difficult task; developers must attempt
to anticipate and tune for a wide range of architectures and use
cases. This results in implementations that target the general case
and cannot provide the performance advantages that are gained from
tuning low level optimisation parameters.

This thesis examines the effect of one such optimisation parameter ---
setting the workgroup size of OpenCL kernels --- on the performance of
stencil codes on GPUs and CPUs. Through an empirical evaluation of
\input{gen/num_runtime_stats} test cases, I demonstrate that there is
no static value or simple heuristic which provides portable
performance across the range of architectures, kernels, and data sets
which Algorithmic Skeletons must target.

To address this, I present an extensible and distributed autotuner
that performs adaptive tuning of workgroup size at runtime. The
presented autotuner uses offline training with synthetic stencils to
rapidly sample the optimisation space. In an evaluation of
\input{gen/num_scenarios} combinations of architecture and benchmark,
the autotuner is able to achieve
$\input{gen/best_avg_classification_performance}\%$ of the available
performance, providing an average speedup of
$\input{gen/best_avg_classification_speedup}\times$ (max
$\input{gen/best_max_classification_speedup}\times$) over the best
possible performance which can be achieved without autotuning.

\ifdraft{
  \newpage
  \textcolor{red}{\section*{Notes for draft version \version{}}}
  \begin{itemize}
  \item \textcolor{blue}{Blue} figures and all graphs are auto
    generated and may change before submission.
  \item There are significant chunks of work I need to do before I can
    do a ``complete'' write-up. Notable missing stuff (and target
    deadlines):
    \begin{enumerate}
    \item ALL data from CPUs. This has a big impact on the evaluation
      since the CPUs exhibited the most unintuitive performance
      results. Deadline: next week.
    \item Conclusions chapter, and most of the background and related
      work. Deadline: four weeks.
    \item I haven't evaluated the \emph{time} cost of autotuning using
      the different approaches (e.g.\ using decision trees vs linear
      regression). For example, if the cost of picking the right
      workgroup size using technique $x$ is $y$ --- how many
      iterations of a stencil do we need to perform before the
      benefits of the autotuning outweigh $y$? Deadline: three weeks.
    \item I talk about ``procedurally generated synthetic
      benchmarks'', but the synthetic benchmarks I use have been
      written by hand. Either implement a stencil generator or
      reword. Deadline: four weeks.
    \item There's no real analysis of the data to show statistical
      soundness. I should run an experiment to show how many samples
      we need to make statistically sound comparisons between
      different workgroup sizes, and whether that number depends on
      other factors, i.e. device type or mean runtime of a
      stencil. Deadline: two weeks.
    \end{enumerate}
  \end{itemize}
}
