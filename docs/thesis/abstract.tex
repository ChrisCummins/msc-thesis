The physical limitations of microprocessor design have forced
manufactures towards increasingly heterogeneous architectures to
extract performance. This trend has not been matched with software
tools to cope with such parallelism, leading to a disparity between
the levels of available performance and the ability for application
developers to exploit it.

Algorithmic Skeletons simplify parallel programming by providing
high-level, reusable patterns of computation. Achieving performant
skeleton implementations is a difficult task; developers must attempt
to anticipate and tune for a wide range of architectures and use
cases. This results in implementations that target the general case
and cannot provide the performance advantages that are gained from
tuning low level optimisation parameters.

% TODO: Rephase this next sentence, to emphasise that the space we're
% such is REALLY big (i.e. all possible stencil codes), and there are
% a huge number of factors which determine the suitability of a
% workgroup size.
%
% TODO: Clarify the phrase ``optimisation space''.
This thesis examines the effect of one such optimisation parameter ---
setting the workgroup size of OpenCL kernels --- on the performance of
stencil codes on GPUs and CPUs. Through an empirical evaluation of a
large optimisation space, I demonstrate that there is no one
% TODO: rephrase ``sensible value''. It should be clear that there's
% no static value OR simple heuristic which would work.
sensible value or heuristic which provides portable performance across
the range of architectures, kernels, and data sets which Algorithmic
Skeletons must target.

% TODO: Add a comparison of autotuner performance vs. human expert.
%
% TODO: Don't downplay the size of the evaluation. ``5 benchmarks and
% 4 GPUs'' sounds really crap.
To address this, I present an extensible and distributed autotuner
that performs adaptive tuning of workgroup size at runtime. The
presented autotuner uses offline training with synthetic stencils to
rapidly sample the optimisation space. In an evaluation of 189 test
cases across the range of architectures and programs, the autotuner is
able to achieve 97\% of the available performance, providing an
average speedup of $1.35\times$ (max $4.00\times$) over the best
possible performance which can be achieved without autotuning.

\ifdraft{
  \newpage
  \textcolor{red}{\section*{Notes for draft version \version{}}}
  \begin{itemize}
  \item There are significant chunks of work I need to do before this
    write-up can be considered ``complete''. Notable missing stuff
    (and target deadlines):
    \begin{enumerate}
    \item ALL data from CPUs. This has a big impact on the evaluation
      since the CPUs exhibited the most unintuitive performance
      results. Deadline: next week.
    \item Most of the background chapter and conclusions. The related
      work is really thin. Deadline: four weeks.
    \item I haven't evaluated the \emph{time} cost of autotuning using
      the different approaches (e.g.\ using decision trees vs linear
      regression). For example, if the cost of picking the right
      workgroup size using technique $x$ is $y$ --- how many
      iterations of a stencil do we need to perform before the
      benefits of the autotuning outweigh $y$? Deadline: three weeks.
    \item I talk about ``procedurally generated synthetic
      benchmarks'', but the synthetic benchmarks I use were written by
      hand. Either implement a stencil generator or reword. Deadline:
      four weeks.
    \item I should run an experiment to show how many samples we need
      to make statistically sound comparisons between different
      workgroup sizes, and whether that number depends on other
      factors, i.e. device type or mean runtime of a
      stencil. Deadline: two weeks.
    \end{enumerate}

  \item Abstract: based on feedback from supervisor meeting on 6/7/15,
    I've: added a sense of crisis to paragraph 1, changed the wording
    of paragraph 2, and added a reference to human expert in paragraph
    3.

  \item Reshuffled chapter structure: background and related work are
    distinct. Main body has separate sections for method, experimental
    setup, and results/evaluation.

  \item All figures, tables, and listings are generated by scripts and
    will need some tidying up by hand.
  \end{itemize}
}
