The physical limitations of microprocessor design have forced the
industry towards increasingly heterogeneous architectures to extract
performance. This trend has not been matched with software tools to
cope with such parallelism, leading to a growing disparity between the
levels of available performance and the ability for application
developers to exploit it.

Algorithmic Skeletons simplify parallel programming by providing
high-level, reusable patterns of computation. Achieving performant
skeleton implementations is a difficult task; developers must attempt
to anticipate and tune for a wide range of architectures and use
cases. This results in implementations that target the general case
and cannot provide the performance advantages that are gained from
tuning low level optimisation parameters.

To address this, I present OmniTune --- an extensible and distributed
framework for runtime autotuning of optimisation parameters. Targeting
the workgroup size of OpenCL kernels, I demonstrate an implementation
of OmniTune for stencil skeletons on CPUs and multi-GPU systems. I
show in a comprehensive evaluation of \input{gen/num_runtime_stats}
test cases that simple heuristics cannot provide portable performance
across the range of architectures, kernels, and datasets which
Algorithmic Skeletons must target.

% TODO: static values can achieve only
% \input{gen/baseline_perf_perc}\% of the available performance.

OmniTune uses procedurally generated synthetic benchmarks and machine
learning to predict workgroup sizes for unseen programs. In an
evaluation of \input{gen/num_scenarios} combinations of programs,
architectures, and datasets, with up to $1.6\times10^4$ parameter
values for each, OmniTune is able to achieve
$\input{gen/best_avg_classification_performance}\%$ of the available
performance, an improvement of
$\input{gen/best_avg_classification_speedup_he_perc}\%$ over the
values selected by human experts, requiring no user intervention. This
adaptive tuning provides an average speedup of
$\input{gen/best_avg_classification_speedup}\times$ (max
$\input{gen/best_max_classification_speedup}\times$) over the best
possible performance which can be achieved without adaptive tuning.

\ifdraft{
  \newpage
  \textcolor{red}{\section*{Notes for draft version \version{}}}
  \begin{itemize}
  \item \textcolor{blue}{Blue} figures and all graphs are auto
    generated and may change before submission.
  \item Addressed all comments from first draft review on 20/7/2015.
  \item Chapter restructuring: after related work, explain the idea,
    the implementation, the methodology, then the results.
  \item There is significantly more data in the evaluation now.
  \item I have a quick-and-dirty implementation of a stencil generator
    which uses template subsitute to make synthetic training
    benchmarks.
  \item Stuff I haven't yet done (and target deadlines):
    \begin{enumerate}
    \item Conclusions chapter, and most of the background. Deadline:
      two weeks.
    \item I haven't evaluated the \emph{time} cost of autotuning using
      the different approaches (e.g.\ using decision trees vs linear
      regression). For example, if the cost of picking the right
      workgroup size using technique $x$ is $y$ --- how many
      iterations of a stencil do we need to perform before the
      benefits of the autotuning outweigh $y$? Deadline: one week.
    \end{enumerate}
  \end{itemize}
}
