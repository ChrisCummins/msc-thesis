% Lit Review:


% OPTIMISING PROGRAMS FOR GPUs

% The challenges and approaches to optimising GPU programs. CUDA
% programming requires the developer to explicitly manage data layout
% in DRAM, caching, thread communication and other
% resources. Performance of such programs depends heavily on fully
% utilizing zero-overhead thread scheduling, memory bandwidth, thread
% grouping, shared control flow, and intra-block thread
% communication. The paper gives an example of optimising matrix
% multiplication by utilising shared memory through tiling, and loop
% unrolling.
%
% S. Ryoo, C. I. Rodrigues, S. S. Baghsorkhi, S. S. Stone, D. B. Kirk,
% and W. W. Hwu, “Optimization principles and application performance
% evaluation of a multithreaded GPU using CUDA,” Proc. 13th ACM
% SIGPLAN Symp. Princ. Pract. parallel Program. - PPoPP ’08, p. 73,
% 2008.
\cite{Ryoo2008a}


%%%%%%%%%%
% TIER 3 %
%%%%%%%%%%


% ALGORITHMIC SKELETONS


% Cole, M. I. (1989). Algorithmic Skeletons: Structured Management of
% Parallel Computation. Pitman London. Retrieved from
% http://homepages.inf.ed.ac.uk/mic/Pubs/skeletonbook.pdf
\cite{Cole1989}

% Cole, M. I. (2004). Bringing skeletons out of the closet: a
% pragmatic manifesto for skeletal parallel programming. Parallel
% Computing, 30(3), 389–406. doi:10.1016/j.parco.2003.12.002
\cite{Cole2004}

% Striegnitz, J. (2000). Making C ++ Ready for Algorithmic Skeletons
% (Vol. 2000). Retrieved from http://www.fz-juelich.de/zam/FACT
\cite{Striegnitz2000}


% Jeffrey, O., & David, M. (2003). The Vision of Autonomic
% Computing. Computer, 36(1), 41–50.
\cite{Jeffrey2003}


% INTEL TBB


% Contreras, G., & Martonosi, M. (2008). Characterizing and improving
% the performance of Intel Threading Building Blocks. In Workload
% Characterization, 2008. IISWC 2008. IEEE International Symposium on
% (pp. 57–66). IEEE. doi:10.1109/IISWC.2008.4636091
\cite{Contreras2008}


% Guidelines for optimising, using an automatic code generator and a
% search engine to optimise parameters:
%
% J. Bilmes, K. Asanovic, C.-W. Chin, and J. Demmel, “Optimizing
% Matrix Multiply Using PHiPAC: A Portable, High-performance, ANSI C
% Coding Methodology,” in Proceedings of the 11th International
% Conference on Supercomputing, 1997, pp. 340–347.
\cite{Bilmes1997}


% The ideal vision of self-governing agents in complex
% environments. We can draw a parallel to the use of self-optimizing
% agents:
%
% O. Jeffrey and M. David, “The Vision of Autonomic Computing,”
%Computer (Long. Beach. Calif)., vol. 36, no. 1, pp. 41–50, 2003.
\cite{Jeffrey2003}


% Background info about optimizing for GPUs, including a checklist of
% optimization metrics:
%
% S. Ryoo, C. I. Rodrigues, S. S. Stone, S. S. Baghsorkhi, S.-Z. Ueng,
% J. a. Stratton, and W. W. Hwu, “Program optimization space pruning
% for a multithreaded GPU,” in Proceedings of the 6th annual IEEE/ACM
% international symposium on Code generation and optimization, 2008,
% pp. 195–204.
\cite{Ryoo2008}


% SUPEROPTIMIZERS

% Searching for *actual* optimal programs by exhaustively enumerating
% instruction set search space:
%
% H. Massalin, “Superoptimizer -- A Look at the Smallest Program,” ACM
% SIGPLAN Not., vol. 22, no. 10, pp. 122–126, 1987.
\cite{Massalin1987}

% Using re-write rules to translate high-level programs to OpenCL:
%
% Steuwer, M., Fensch, C., & Dubach, C. (2015). Patterns and Rewrite
% Rules for Systematic Code Generation From High-Level Functional
% Patterns to High-Performance OpenCL Code. arXiv Preprint
% arXiv:1502.02389.
\cite{Steuwer2015}

% Creating *actual* optimal programs through logical proofs:
%
% R. Joshi, G. Nelson, and K. Randall, “Denali: a goal-directed
% superoptimizer,” ACM SIGPLAN Not., vol. 37, no. 5, p. 304, 2002.
\cite{Joshi2002}


% APPLYING ML TO OPTIMISATION PROBLEMS

% Artificial Neural Networks for resource allocation of CMPS:
%
% R. Bitirgen, E. Ipek, and J. F. Martinez, “Coordinated Management of
% Multiple Interacting Resources in Chip Multiprocessors: A Machine
% Learning Approach,” in 2008 41st IEEE/ACM International Symposium on
% Microarchitecture, 2008, pp. 318–329.
\cite{Bitirgen2008}


%%%%%%%%%%
% TIER 2 %
%%%%%%%%%%

% Ryoo, S., Rodrigues, C. I., Stone, S. S., Baghsorkhi, S. S., Ueng,
% S.-Z., Stratton, J. a., & Hwu, W. W. (2008). Program optimization
% space pruning for a multithreaded GPU. In Proceedings of the 6th
% annual IEEE/ACM international symposium on Code generation and
% optimization (pp. 195–204). New York, New York, USA: ACM
% Press. doi:10.1145/1356058.1356084
\cite{Ryoo2008}

% Offline ML for partitioning streaming applications:
%
% Z. Wang and M. F. P. O. Boyle, “Partitioning Streaming Parallelism
% for Multi-cores: A Machine Learning Based Approach,” in Proceedings
% of the 19th international conference on Parallel architectures and
% compilation techniques, 2010, pp. 307–318.
\cite{Wang2010}

% ONLINE AUTOTUNING


% Ansel, J., & Reilly, U. O. (2012). SiblingRivalry: Online Autotuning
% Through Local Competitions. In Proceedings of the 2012 International
% Conference on Compilers, Architectures and Synthesis for Embedded
% Systems (pp. 91–100). ACM. doi:10.1145/2380403.2380425
\cite{Ansel2012}


% AUTOTUNING OPENCL


% Rul, S., Vandierendonck, H., Haene, J. D., & Bosschere,
% K. De. (2010). An Experimental Study on Performance Portability of
% OpenCL Kernels. In 2010 Symposium on Application Accelerators in
% High Performance Computing (SAAHPC’10) (pp. 4–6).
\cite{Rul2010}


% Optimising data placement on GPUs using a description of the
% hardware and a memory-placement-agnostic compiler. Shows impressive
% speedups (2.08x max, 1.59x avg) from just optimising data placement.
%
% G. Chen and B. Wu, “PORPLE: An Extensible Optimizer for Portable
% Data Placement on GPU,” in Microarchitecture (MICRO), 2014 47th
% Annual IEEE/ACM International Symposium on, 2014, pp. 88–100.
\cite{Chen2014}


% SKEPU


% Enmyren, J., & Kessler, C. (2010). SkePU: a multi-backend skeleton
% programming library for multi-GPU systems. In Proceedings of the
% fourth international workshop on High-level parallel programming and
% applications (pp. 5–14). ACM. Retrieved from
% http://dl.acm.org/citation.cfm?id=1863487
\cite{Enmyren2010}

% Selecting between different back-ends ("execution plans"):
%
% Dastgeer, U., Enmyren, J., & Kessler, C. W. (2011). Auto-tuning
% SkePU: a multi-backend skeleton programming framework for multi-GPU
% systems. In Proceedings of the 4th International Workshop on
% Multicore Software Engineering (pp. 25–32). ACM. Retrieved from
% http://dl.acm.org/citation.cfm?id=1984697
\cite{Dastgeer2011}


% PETABRICKS


% Ansel, J. (2009). PetaBricks: a language and compiler for
% algorithmic choice. MIT.
\cite{Ansel2009}

% Ansel, J., & Chan, C. (2010). PetaBricks. XRDS: Crossroads, The ACM
% Magazine for Students, 17(1), 32. doi:10.1145/1836543.1836554
\cite{Ansel2010}

% Chan, C., Ansel, J., Wong, Y. L., Amarasinghe, S., & Edelman,
% A. (2009). Autotuning multigrid with PetaBricks. In ACM/IEEE
% Conference on Supercomputing. New York, New York, USA: ACM
% Press. doi:10.1145/1654059.1654065
\cite{Chan2009}

% Ansel, J. (2014). Autotuning Programs with Algorithmic
% Choice. Massachusetts Institute of Technology.
\cite{Ansel2014}


%%%%%%%%%%
% TIER 1 %
%%%%%%%%%%


% AUTOTUNING on GPUs

% T. Lutz, C. Fensch, and M. Cole, “PARTANS: An Autotuning Framework
% for Stencil Computation on Multi-GPU Systems,” ACM
% Trans. Archit. Code Optim., vol. 9, no. 4, pp. 1–24, 2013.
\cite{Lutz2013}


% MASIF


% Collins, A., Fensch, C., & Leather, H. (2012). Auto-Tuning Parallel
% Skeletons. Parallel Processing Letters, 22(02),
% 1240005. doi:10.1142/S0129626412400051
\cite{Collins2012}

% Collins, A., Fensch, C., Leather, H., & Cole, M. (2013). MaSiF:
% Machine Learning Guided Auto-tuning of Parallel Skeletons. 20th
% Annual International Conference on High Performance Computing -
% HiPC, 186–195. doi:10.1109/HiPC.2013.6799098
\cite{Collins2013}


% ONLINE RL


% Eastep, J., Wingate, D., & Agarwal, A. (2011). Smart Data
% Structures: An Online Machine Learning Approach to Multicore Data
% Structures. In Proceedings of the 8th ACM International Conference
% on Autonomic Computing (pp. 11–20). New York, NY, USA:
% ACM. doi:10.1145/1998582.1998587
\cite{Eastep2011}

% Tesauro, G. (2005). Online Resource Allocation Using Decompositional
% Reinforcement Learning. In AAAI (pp. 886–891).
\cite{Tesauro2005}


% GPU benchmarking:
%
% Lee, H., Brown, K. J., Sujeeth, A. K., Rompf, T., & Olukotun,
% K. (2014). Locality-Aware Mapping of Nested Parallel Patterns on
% GPUs. In Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM
% International Symposium on
% (pp. 63–74). IEEE. doi:10.1109/MICRO.2014.23
\cite{Lee}


% SKELCL

% The introductory SkelCL paper. Most highly cited:
%
% Steuwer, M., Kegel, P., & Gorlatch, S. (2011). SkelCL - A Portable
% Skeleton Library for High-Level GPU Programming. In Parallel and
% Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE
% International Symposium on
% (pp. 1176–1182). IEEE. doi:10.1109/IPDPS.2011.269
\cite{Steuwer2011}

% Support for multi-GPU systems:
%
% Steuwer, M., & Gorlatch, S. (2013). SkelCL: Enhancing OpenCL for
% High-Level Programming of Multi-GPU Systems. Parallel Computing
% Technologies, 7979, 258–272. doi:10.1007/978-3-642-39958-9_24
\cite{Steuwer2013a}

% Stencil computations:
%
% S. Breuer, M. Steuwer, and S. Gorlatch, “Extending the SkelCL
% Skeleton Library for Stencil Computations on Multi-GPU Systems,”
% HiStencils 2014, pp. 23–30, 2014.
\cite{Breuer2014}

% Allpairs skeleton:
%
% M. Steuwer, M. Friese, S. Albers, and S. Gorlatch, “Introducing and
% implementing the allpairs skeleton for programming multi-GPU
% Systems,” Int. J. Parallel Program., vol. 42, pp. 601–618, 2014.
\cite{Steuwer2014}


% An example of reduced programmer effort for real world application
% using SkelCL:
%
% Steuwer, M., & Gorlatch, S. (2013). High-level Programming for
% Medical Imaging on Multi-GPU Systems Using the SkelCL
% Library. Procedia Computer Science, 18,
% 749–758. doi:10.1016/j.procs.2013.05.239
\cite{Steuwer2013}

% Steuwer, M., Kegel, P., & Gorlatch, S. (2012). Towards High-Level
% Programming of Multi-GPU Systems Using the SkelCL Library. In
% Parallel and Distributed Processing Symposium Workshops & PhD Forum
% (IPDPSW), 2012 IEEE 26th International
% (pp. 1858–1865). Ieee. doi:10.1109/IPDPSW.2012.229
\cite{Steuwer2012}
