\section{Introduction}

This chapter evaluates the performance of OmniTune when tasked with
selecting workgroup sizes for SkelCL stencil codes. First I discuss
measurement noise present in the experimental results, and the methods
used to accommodate for it. Then I examine the observed effect that
workgroup size has on the performance of SkelCL stencils. The
effectiveness of each of the autotuning techniques described in the
previous chapters are evaluated using multiple different machine
learning algorithms, and the prediction quality is scrutinised for
portability across programs, devices, and datasets.


\subsubsection{Overview of Experimental Results}

The experimental results consist of measured runtimes for a set of
\emph{test cases}, collected using the methodology explained int he
previous chapter. Each test case $\tau_i$ consists of a scenario,
workgroup size pair $\tau_i = (s_i,w_i)$, and is associated with a
\emph{sample} of observed runtimes from multiple runs of the
program. A total of \input{gen/num_runtime_stats} test cases were
evaluated, which represents an exhaustive enumeration of the workgroup
size optimisation space for \input{gen/num_scenarios} scenarios. For
each scenario, runtimes for an average of \input{gen/avg_num_params}
(max \input{gen/max_num_params}) unique workgroup sizes were
measured. The average sample size of runtimes for each test case is
\input{gen/avg_sample_count} (min \input{gen/min_sample_count}, total
\input{gen/num_samples}).


\section{Statistical Soundness}

\begin{figure}
\input{fig/runtime-histograms}
\caption{%
  Distribution of runtime samples for 9 test cases with average
  runtimes between 2--200ms. Each plot contains a 35-bin histogram of
  1000 samples, and a fitted kernel density estimate with bandwidth
  0.3. The sample mean is shown as a vertical dashed line. In some of
  the plots, the distribution of runtimes is bimodal, and skewed to
  the lower end of the runtimes range. The long tail in some
  distributions suggests delays in program execution caused by other
  processes on the test systems. The similarity between arithmetic
  mean runtimes and peak density estimates suggests that a
  sufficiently accurate estimation of the true mean can be found using
  the arithmetic mean of a large number of observations.%
}
\label{fig:runtime-histograms}
\end{figure}

The complex interaction between processes competing for the finite
resources of a system introduces many sources for noise in program
runtime measurements. Before making any judgements about the relative
performance of optimisation configurations, we must establish the
level of noise present in these measurements. To do this, we evaluate
the distribution of runtimes for a randomly selected 1000 test cases,
recording 1000 1000 runtime observations for each. We can then produce
fine-grained histograms of runtimes for individual test
cases. Figure~\ref{fig:runtime-histograms} shows an example nine of
these, for test cases with mean runtimes between 2--200ms. The plots
show that the distribution of runtimes is not always Gaussian; rather,
it is sometimes bimodal, and generally skewed to the lower end of the
runtime range, with a long ``tail'' to the right. This fits our
intuition that programs have a hard \emph{minimum} runtime enforced by
the time taken to execute the instructions of a program, and that
noise introduced to the system extends this runtime by, for example,
preempting a process so that another may run.

The central limit theorem allows the assumption of an underlying
Gaussian distribution for samples of size $\ge 30$~\cite{Georges2007}.
Given our minimum sample size of \input{gen/min_sample_count}, we can
use 95\% confidence intervals to provide statistical confidence that
the arithmetic mean of observed runtimes with respect to the true
mean. As the number or samples increases, we should expect the size of
the confidence interval to shrink. This is illustrated in
Figure~\ref{fig:ci-trends}, which plots the average size of 95\%
confidence intervals across the 1000 test cases, normalised to their
respective means, as a function of sample size. It shows the
diminishing returns that increasing sample size provides. For example,
increasing the sample count from 10 to 30 results in an approximate
50\% reduction in confidence interval size. Increasing the sample size
from 30 to 50 results in only a 25\% reduction.

\begin{figure}
\centering
\includegraphics{gen/img/ci_trend}
\caption{%
  % FIXME: Check against \input{gen/max_ci} and \input{gen/mean_ci}
  Ratio of confidence interval to mean as a function of sample
  count. The two dashed lines indicate the confidence interval at the
  minimum sample count (\fixme{3.7}\%), and mean sample count
  (\fixme{2.5}\%).%
}
\label{fig:ci-trends}
\end{figure}

By comparing the average confidence interval at different sample
counts against the full experiment results of
\input{gen/num_runtime_stats} test cases, we can assert with 95\%
confidence that the true mean for each test case is within
\input{gen/mean_ci}\% of the sample mean (given the average number of
samples per test case), or \input{gen/max_ci}\% in the worst case (at
the minimum number of samples). This demonstrates a sufficiently low
level of noise that meaningful comparisons can be made between the
performance of different configurations. \FIXME{justify\ldots}


\section{Workgroup Size Optimisation Space}

In this section we explore the impact that the workgroup size
optimisation space has on the performance of stencil codes.

\begin{figure}
\centering
\includegraphics{gen/img/max_wgsizes.pdf}
\vspace{-1.5em} % Shrink vertical padding
\caption{%
  A subset of the workgroup size optimisation space,
  $w_c \le 100, w_r \le 100$, showing, for each workgroup size, the
  ratio of scenarios for which the workgroup size satisfies
  architecture and kernel enforced constraints ($W_{\max}(s)$).
  Workgroup sizes with a coverage of $< 1$ fail to satisfy these
  constraints in one or more scenarios.  For static tuning, only
  workgroup sizes with a coverage of 1 are acceptable.%
}
\label{fig:max-wgsizes}
\end{figure}

\subsection{Workgroup Size Legality}

As explained in Section~\ref{sec:op-params}, the space of legal
workgroup sizes $W_{legal}(s)$ for a given scenario $s$ comprises all
workgroup sizes which neither: exceed the maximum allowed by the
OpenCL device and kernel $W_{\max}(s)$, and are not refused by the
OpenCL runtime.

\subsubsection{Maximum workgroup sizes}

We define the \emph{coverage} of a workgroup size to be the ratio
$0 \le x \le 1$ between the number of scenarios for which the
workgroup size was less than $W_{\max}(s)$, normalised to the total
number of workgroup sizes. A coverage of 1 implies a workgroup size
which is always legal for all combinations of stencil and
architecture. A workgroup size with a coverage of 0 is never
legal. Figure~\ref{fig:max-wgsizes} plots the coverage of a subset of
the workgroup size optimisation space.

Note that since $W_{\max}(s)$ defines a hard limit for a given $s$, if
statically selecting a workgroup size, one must limit the optimisation
space to the smallest $W_{\max}(s)$ value, i.e.\ only the workgroup
sizes with a coverage of 1. The observed $W_{\max}(s)$ values range
from 256--8192, which results in up to a 97\% reduction in the size of
the optimisation space when $W_{\max}(s) = 8192$, even though only
14\% of scenarios have this minimum value of $W_{\max}(s) = 256$.

% Size of optimisation space for Wmax =  256: 273
% Size of optimisation space for Wmax = 8192: 15925

\subsubsection{Refused Parameters}

\begin{table}
\parbox{.32\linewidth}{
  \centering
  \scriptsize
  \rowcolors{2}{white}{gray!25}
  \input{gen/tab/top_refused_params_1}
}
\hfill
\parbox{.32\linewidth}{
  \centering
  \scriptsize
  \rowcolors{2}{white}{gray!25}
  \input{gen/tab/top_refused_params_2}
}
\hfill
\parbox{.32\linewidth}{
  \centering
  \scriptsize
  \rowcolors{2}{white}{gray!25}
  \input{gen/tab/top_refused_params_3}
}
\caption{%
  The thirty most refused parameters, ranked in descending
  order. There is little correlation between the size of workgroup and the
  likelihood that is refused, suggesting that the cause of refused
  parameters is not a resource constraint, but a behavioural issue.%
}
\label{tab:top-refused-params}
\end{table}

\begin{figure}
\centering
\begin{subfigure}[h]{.45\textwidth}
  \centering
  \includegraphics{gen/img/refused_params_by_device}
  \caption{}
  \label{fig:refused-params-by-device}
\end{subfigure}
\hfill
\begin{subfigure}[h]{.45\textwidth}
  \centering
  \includegraphics{gen/img/refused_params_by_vendor}
  \caption{}
  \label{fig:refused-params-by-vendor}
\end{subfigure}
\caption{%
  The ratio of test cases with refused workgroup sizes, grouped by:
  (\subref{fig:refused-params-by-device}) OpenCL device ID;\
  (\subref{fig:refused-params-by-vendor}) device vendor ID. Parameters
  were refused most frequently by Intel i5 CPUs, then by
  previous-generation NVIDIA GPUs. No parameters were refused by AMD
  devices.%
}
\label{fig:refused-params-by-dev-vendor}
\end{figure}

In addition to the hard constraints imposed by the maximum workgroup
size, there are also refused parameters, which are workgroup sizes
which are rejected by the OpenCL runtime and do not provide a
functioning program. Of the \input{gen/num_params} unique workgroup
sizes tested, \input{gen/ratio_refused_params}\% were refused in one
or more test cases. An average of
\input{gen/ratio_refused_test_cases}\% of all test cases lead to
refused parameters. For a workgroup size to be refused, it must
satisfy the architectural and program-specific constraints which are
exposed by OpenCL, but still lead to a \texttt{CL\_OUT\_OF\_RESOURCES}
error when the kernel is enqueued. Table~\ref{tab:top-refused-params}
lists the most frequently refused parameters, and the percentage of
test cases for which they were refused. While uncommon, a refused
parameters is an obvious inconvenience to the user, as one would
expect that any workgroup size within the specified maximum should
behave \emph{correctly}, if not efficiently. Figure~\ref{fig:coverage}
visualises the space of legal workgroup sizes by showing the frequency
counts that a workgroup size is legal. Smaller workgroup sizes are
legal most frequently due to the $W_{\max}(s)$ constraints. Beyond
that, workgroup sizes which contain $w_c$ and $w_r$ values which are
multiples of 8 are more frequently legal.

Experimental results suggest that the problem is vendor --- or at
least device --- specific. By grouping the refused test cases by
device and vendor, we see a much greater quantity of refused
parameters for test cases on Intel CPU devices than any other type,
while no workgroup sizes were refused by the AMD
GPU. Figure~\ref{fig:refused-params-by-dev-vendor} shows these
groupings. The underlying cause for these refused parameters is
unknown, but can likely by explained by inconsistencies or errors in
specific OpenCL driver implementations. As these OpenCL
implementations are still in active development, it is anticipated
that errors caused by unexpected behaviour will become more infrequent
as the technology matures. For now, it is imperative that any
autotuning system is capable of adapting to these refused parameters
by suggesting alternatives when they occur.

% \TODO{Bug reports\ldots}


\subsection{Oracle Workgroup Sizes}

\begin{figure}
\begin{subfigure}[t]{0.98\textwidth}
\centering
\includegraphics{gen/img/coverage_space.pdf}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:coverage}
\end{subfigure}
\\
\begin{subfigure}[t]{0.98\textwidth}
\centering
\includegraphics{gen/img/oracle_param_space.pdf}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:oracle-wgsizes}
\end{subfigure}
\caption{%
  Log frequency counts for (\subref{fig:coverage}) legality, and
  (\subref{fig:oracle-wgsizes}) optimality, for a subset of the
  workgroup size optimisation space, $w_c \le 100, w_r \le 100$.
  Legality frequencies are highest for smaller row and column counts
  (where $w < W_{\max}(s) \forall S$), and $w_c$ and $w_r$ values
  which are multiples of 8. The space of oracle workgroup size
  frequencies is highly irregular and uneven.%
}
\label{fig:heatmaps}
\end{figure}

For each scenario $s$, the oracle workgroup size $\Omega(s)$ is the
workgroup size which resulted in the lowest mean runtime. If the
performance of stencils were independent of workgroup size, we would
expect that the oracle workgroup size would remain constant across all
scenarios $s \in S$. Instead, we find that there are
\input{gen/num_oracle_params} unique oracle workgroup sizes, with
\input{gen/oracle_params_per_scenario_perc}\% of scenarios having a
unique workgroup size. This indicates that tuning for optimal
workgroup size is a non-trivial task. Figure~\ref{fig:oracle-wgsizes}
shows the distribution of oracle workgroup sizes, demonstrating that
there is clearly no ``silver bullet'' workgroup size which works for
all scenarios, and that the space of oracle workgroup sizes is non
linear and complex. As Figure~\ref{fig:oracle-accuracy} shows,
\input{gen/num_wgsizes_50_accuracy} unique workgroup sizes are
required in order to achieve oracle performance just 50\% of the time.

The workgroup size which is most frequently optimal is
\input{gen/max_oracle_param}, which is optimal for
\input{gen/max_oracle_param_frequency} of scenarios. Note that this is
not adequate to use as a baseline for comparing relative performance,
as it does not respect legality constraints that, that is
$\input{gen/max_oracle_param_w} \not\in W_{safe}$.
Figure~\ref{fig:performance-legality} illustrates this point by
plotting the legality of different workgroup sizes and their mean
performance relative to the oracle.


\begin{figure}
\centering
\includegraphics{gen/img/num_params_oracle.pdf}
\caption{%
  Accuracy compared to the oracle as a function of the number of
  workgroup sizes used. The best accuracy that is achievable using a
  single statically chosen value is
  \protect\input{gen/max_oracle_param_frequency}.%
}
\label{fig:oracle-accuracy}
\end{figure}

\begin{figure}
\centering
\includegraphics{gen/img/params_summary.pdf}
\caption{%
  The dashed line shows the \emph{legality} of workgroup sizes, i.e.\
  the ratio of scenarios for which that workgroup size is legal. The
  solid line shows the geometric mean of performance relative to the
  oracle for all scenarios for which the workgroup size is legal.%
}
\label{fig:performance-legality}
\end{figure}


% To begin to get a feel for the
%
% Figure~\ref{fig:performance-wgsizes}. Furthermore, the performance of
% workgroup sizes is shown in Figure~\ref{fig:performances} to be vary
% greatly across architectures, kernels, and datasets.

\TODO{What is the distribution of relative performance across
  architectures, programs, and datasets?}

\begin{table}
  \parbox{.45\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_params_coverage}
    \caption{The 25 workgroup sizes with the greatest legality.}
  }
  \hfill
  \parbox{.45\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_params_perf}
    \caption{The 25 workgroup sizes with the greatest performance.}
  }
\end{table}


\subsection{Performance Upper Bounds}

\begin{figure}
\includegraphics{gen/img/max_speedups}
\caption{%
  Speedup of oracle workgroup size over the worst workgroup size for
  each scenario, and the statically chosen workgroup size that
  provides the best overall performance.%
}
\label{fig:speedups}
\end{figure}

For a given scenario $s$, the ratio of the workgroups sizes from
$W_{legal}(s)$ which provide the longest and shortest mean runtimes is
used to calculate an upper bound for the possible performance
influence of workgroup size:

\begin{equation}
r_{max}(s) = r(s, \argmax_{w \in W_{legal}(s)} t(s,w), \Omega(s))
\end{equation}

When applied to each scenario $s \in S$ from the experimental results,
the average performance upper bound is found to be
$\input{gen/avg_possible_speedup}\times$ (range:
$\input{gen/min_possible_speedup}\times$ -
$\input{gen/max_possible_speedup}\times$).

\TODO{T-test to demonstrate confidence in differences between best and
  worst params.}


\cleardoublepage
\begin{figure}
\input{fig/performance-wgsizes}
\caption{%
  Comparing workgroup performance relative to the oracle as function
  of: (\subref{fig:performance-max-wgsize})~maximum legal size,
  (\subref{fig:performance-wg-c})~number of columns, and
  (\subref{fig:performance-wg-r})~ number of rows.%
}
\label{fig:performance-wgsizes}
\end{figure}
\newpage
\begin{figure}
\input{fig/performances}
\caption{%
  Performance relative to the oracle of workgroup sizes across all
  test cases, grouped by: (\subref{fig:performance-kernels})~kernels,
  (\subref{fig:performance-devices})~devices, and
  (\subref{fig:performance-datasets})~datasets.%
}
\label{fig:performances}
\end{figure}


\subsection{Baseline Parameters}

From the experimental results, the baseline workgroup size $\bar{w}$
is found to be \input{gen/one_r}, with a geometric mean performance
$\bar{p}(\bar{w})$ of
\input{gen/one_r_perf}. Figure~\ref{fig:speedups} shows the speedup of
the oracle over this baseline parameter for all scenarios. If we
assume that a pragmatic developer with enough time would eventually
find this static optimal, then this provides a reasonable upper bound
on the attainable improvements of an autotuner for workgroup size over
that of a static choice.


\subsection{Human Expert}


The stencil workgroup size as chosen by a human expert is
$32 \times 4$.

\TODO{How often does this parameter fail?}

\TODO{When it is legal, how much of the available performance does
  this attain?}


\subsection{Heuristics}

\subsubsection{Per-device workgroup sizes}

\TODO{%
  What is the performance of picking one value per: architecture,
  dataset, and program?%
}

\subsubsection{Using maximum legal size}

A common approach taken by application developers is to set the
workgroup size for a kernel based on the maximum legal workgroup size,
for example to use the square root of the maximum to set the number of
columns and rows.

\TODO{What is the perforamnce of always picking the max legal value?}

\subsubsection{Scaling with maximum legal size}

% \TODO{Partial baseline. How much performance could we get if we lock
%   one of the parameters and twiddle the other?}

% While the reasoning for this approach is perfectly intuitive, these
% results indicate that there no useful correlation between the ratio
% of a particular workgroup size to the maximum legal workgroup and
% the observed performance.

\subsection{Summary}

By comparing the relative performance of an average of
\input{gen/avg_num_params} workgroup sizes for each workload, the
following conclusions can be drawn:

\begin{enumerate}
\item The performance of a workgroup size for a particular workload
  depends properties of the hardware, software, and dataset. The
  performance gap between different workgroup sizes for a single
  workload is up to $\input{gen/max_possible_speedup}\times$.
\item Not all workgroup sizes are legal, and we can only test if a
  value \emph{is} legal at runtime.
\item Differing workloads have wildly different optimal workgroup
  sizes, and the best performance can be achieved using static tuning
  is optimal for only \input{gen/max_oracle_param_frequency} of
  workloads.
\end{enumerate}

% I believe this presents a compelling case for the development of an
% autotuner which can select the optimal workgroup size at runtime.


\section{Predicting Optimal Workgroup Sizes}


To evaluate the performance of a classifier, a subset of scenarios
$S_{training} \subset S$ are labelled with the workgroup size which
gave the best performance for each. The classifier is trained on this
labelled training data, and tested using a set of unseen scenarios
$S_{testing} = S - S_{training}$. The performance of each predicted
oracle workgroup size is compared against the true oracles for that
seTODO{What is the average speedup achieved using different
  classifiers?}

\TODO{What percentage of the maximum performance do the classifiers
  achieve?}

\begin{figure}
\centering
\includegraphics{gen/img/classification-xval.pdf}
\caption{%
  Classification performance using cross-validation.%
}
\end{figure}

\begin{figure}
\centering
\includegraphics{gen/img/classification-xval-random.pdf}
\caption{%
  Per-instance performances of classification using cross-validation
  and the ``random'' error handler.%
}
\end{figure}


\TODO{Rank eigenvectors of PCA on features.}

\TODO{Evaluate the effectiveness of training with synthetic
  benchmarks.}

\subsection{Selection of Features}

\TODO{How does the selection of features affect performance? Show a
  plot of performance vs number of features.}


\subsection{Effectiveness of Synthetic Benchmarks}

\TODO{How well does classification perform when trained solely on
  synthetic benchmarks, and tested soley on real benchmarks?}

\TODO{Repeat the above, but with incremtally fewere benchmarks.}


\subsection{Autotuning Overheads}

\TODO{What is the costs of auto-tuning?}

\TODO{How many iterations do we need to do before we ``break even''?}

\section{Predicting Stencil Runtime}

\begin{enumerate}
\item How accurately does it predict the runtime of a stencil?
\item What is the relationship between \emph{classification} accuracy,
  and the accuracy of predicted runtimes? Does it really matter if the
  predicted runtime is inaccurate?
\end{enumerate}

\begin{figure}
\centering
\includegraphics{gen/img/runtime-regression-xval.pdf}
\caption{%
  Performance of regressors at predicting program runtime.%
}
\end{figure}

\subsection{Summary}


\section{Predicting Relative Performance}

\subsection{Summary}



% \section{Comparison with hand tuned implementations}

% \TODO{%
%   Compare performance against some hand-crafted implementation of one
%   of the real benchmarks.%
% }


\section{Summary}

% \begin{table}
% \scriptsize
% \input{\DataDir/tab/xval.tex}
% \caption{Results of 10 fold cross-validation.}
% \end{table}

% \begin{table}
% \scriptsize
% \input{\DataDir/tab/synthetic_real.tex}
% \caption{Results of training using synthetic benchmarks and testing on
%   real.}
% \end{table}
