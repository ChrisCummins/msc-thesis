\section{Introduction}

This chapter evaluates the performance of OmniTune when tasked with
selecting workgroup sizes for SkelCL stencil codes. First I discuss
measurement noise present in the experimental results, and the methods
used to accommodate for it. Then I examine the observed effect that
workgroup size has on the performance of SkelCL stencils. The
effectiveness of each of the autotuning techniques described in the
previous chapters are evaluated using multiple different machine
learning algorithms, and the prediction quality is scrutinised for
portability across programs, devices, and datasets.


\subsubsection{Overview of Experimental Results}

The experimental results consist of measured runtimes for a set of
\emph{test cases}, collected using the methodology explained int he
previous chapter. Each test case $\tau_i$ consists of a scenario,
workgroup size pair $\tau_i = (s_i,w_i)$, and is associated with a
\emph{sample} of observed runtimes from multiple runs of the
program. A total of \input{gen/num_runtime_stats} test cases were
evaluated, which represents an exhaustive enumeration of the workgroup
size optimisation space for \input{gen/num_scenarios} scenarios. For
each scenario, runtimes for an average of \input{gen/avg_num_params}
(max \input{gen/max_num_params}) unique workgroup sizes were
measured. The average sample size of runtimes for each test case is
\input{gen/avg_sample_count} (min \input{gen/min_sample_count}, total
\input{gen/num_samples}).


\section{Statistical Soundness}

\begin{figure}
\input{fig/runtime-histograms}
\caption{%
  Distribution of runtime samples for 9 test cases with average
  runtimes between 2--200ms. Each plot contains a 35-bin histogram of
  1000 samples, and a fitted kernel density estimate with bandwidth
  0.3. The sample mean is shown as a vertical dashed line. In some of
  the plots, the distribution of runtimes is bimodal, and skewed to
  the lower end of the runtimes range. The long tail in some
  distributions suggests delays in program execution caused by other
  processes on the test systems. The similarity between arithmetic
  mean runtimes and peak density estimates suggests that a
  sufficiently accurate estimation of the true mean can be found using
  the arithmetic mean of a large number of observations.%
}
\label{fig:runtime-histograms}
\end{figure}

The complex interaction between processes competing for the finite
resources of a system introduces many sources for noise in program
runtime measurements. Before making any judgements about the relative
performance of optimisation configurations, we must establish the
level of noise present in these measurements. To do this, we evaluate
the distribution of runtimes for a selection of test cases. A random
subset of 1000 test cases was chosen and 1000 runtimes observations
were recorded for each. We can then produce fine-grained histograms of
runtimes for individual test
cases. Figure~\ref{fig:runtime-histograms} shows an example nine of
these, for test cases with mean runtimes between 2--200ms.

The central limit theorem allows the assumption of an underlying
Gaussian distribution for samples of size $\ge 30$~\cite{Georges2007}.
Given our minimum sample size of \input{gen/min_sample_count}, we can
use 95\% confidence intervals to provide statistical confidence that
the arithmetic mean of observed runtimes with respect to the true
mean. Figure~\ref{fig:ci-trends} plots the average size of 95\%
confidence intervals across the 1000 test cases, normalised to their
respective means, as a function of sample size. It shows the
diminishing returns that increasing sample size provides. For example,
increasing the sample count from 10 to 30 results in an approximate
50\% reduction in confidence interval size. Increasing the sample size
from 30 to 50 results in only a 25\% reduction.

\begin{figure}
\centering
\includegraphics{gen/img/ci_trend}
\caption{%
  % FIXME: Check against \input{gen/max_ci} and \input{gen/mean_ci}
  Ratio of confidence interval to mean as a function of sample
  count. The two dashed lines indicate the confidence interval at the
  minimum sample count (\fixme{3.7}\%), and mean sample count
  (\fixme{2.5}\%).%
}
\label{fig:ci-trends}
\end{figure}

By comparing the average confidence interval at different sample
counts against the full experiment results of
\input{gen/num_runtime_stats} test cases, we can assert with 95\%
confidence that the true mean for each test case is within
\input{gen/mean_ci}\% of the sample mean (given the average number of
samples per test case), or \input{gen/max_ci}\% in the worst case (at
the minimum number of samples).

\TODO{Is the number of samples I have enough?}

\TODO{How does variance change with architecture, program, or
  dataset.}


\section{Optimising Workgroup Sizes}

In this section we explore the impact that workgroup size has on the
performance of stencil codes, and the effectiveness of simple
techniques to tune it.

\subsection{Oracle Workgroup Sizes}


\begin{figure}
\centering
\includegraphics{gen/img/max_wgsizes.pdf}
\vspace{-1.5em} % Shrink vertical padding
\caption{Foo bar}
\label{fig:max-wgsizes}
\end{figure}


\begin{figure}
% \begin{subfigure}[t]{0.45\textwidth}
% \centering
% \includegraphics{gen/img/1.pdf}
% \vspace{-1.5em} % Shrink vertical padding
% \caption{}
% \label{fig:max-wgsizes}
% \end{subfigure}
% ~%
% \begin{subfigure}[t]{0.45\textwidth}
% \centering
% \includegraphics{gen/img/2.pdf}
% \vspace{-1.5em} % Shrink vertical padding
% \caption{}
% \label{fig:coverage}
% \end{subfigure}
% \\
% \begin{subfigure}[t]{0.45\textwidth}
% \centering
% \includegraphics{gen/img/3.pdf}
% \vspace{-1.5em} % Shrink vertical padding
% \caption{}
% \label{fig:max-wgsizes}
% \end{subfigure}
% ~%
% \begin{subfigure}[t]{0.45\textwidth}
% \centering
% \includegraphics{gen/img/4.pdf}
% \vspace{-1.5em} % Shrink vertical padding
% \caption{}
% \label{fig:coverage}
% \end{subfigure}
%
% \begin{subfigure}[t]{0.98\textwidth}
% \centering
% \includegraphics{gen/img/max_wgsizes.pdf}
% \vspace{-1.5em} % Shrink vertical padding
% \caption{}
% \label{fig:max-wgsizes}
% \end{subfigure}
% \\
\begin{subfigure}[t]{0.98\textwidth}
\centering
\includegraphics{gen/img/coverage_space.pdf}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:coverage}
\end{subfigure}
\\
\begin{subfigure}[t]{0.98\textwidth}
\centering
\includegraphics{gen/img/oracle_param_space.pdf}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:oracle-wgsizes}
\end{subfigure}
\caption{%
  A subset of the workgroup size optimisation space,
  $w_c \le 100, w_r \le 100$.  In~(\subref{fig:max-wgsizes}), the
  ratio of scenarios for which workgroup sizes are less than
  $W_{max}(s)$. (\subref{fig:coverage}) and
  (\subref{fig:oracle-wgsizes}) show log transformations of the
  frequency at which parameters were \emph{legal}, or the
  \emph{oracle}.%
}
\label{fig:heatmaps}
\end{figure}

For each scenario $s$, the oracle workgroup size $\Omega(s)$ is the
workgroup size which resulted in the lowest mean runtime. If the
performance of stencils were independent of workgroup size, we would
expect that the oracle workgroup size would remain constant across
scenarios $s \in S$. Instead, we find that there are
\input{gen/num_oracle_params} unique oracle workgroup sizes, with
\input{gen/oracle_params_per_scenario_perc}\% of scenarios having a
unique workgroup size. This indicates that tuning for optimal
performance is a non-trivial task. As Figure~\ref{fig:oracle-accuracy}
shows, \input{gen/num_wgsizes_50_accuracy} unique workgroup sizes are
required in order to achieve oracle performance just 50\% of the time.

Figure~\ref{fig:max-wgsizes} shows the distribution of maximum
workgroup sizes across all scenarios. Figure~\ref{fig:oracle-wgsizes}
shows the distribution of oracle workgroup sizes, demonstrating that
there is clearly no ``silver bullet'' workgroup size which works for
all scenarios.

\begin{figure}
\centering
\includegraphics{gen/img/num_params_oracle.pdf}
\caption{%
  Accuracy compared to the oracle as a function of the number of
  workgroup sizes used. The best accuracy that is achievable using a
  single statically chosen value is
  \protect\input{gen/max_oracle_param_frequency}.%
}
\label{fig:oracle-accuracy}
\end{figure}

The mode of the oracle workgroup sizes is
\input{gen/max_oracle_param}, which is optimal for
\input{gen/max_oracle_param_frequency} of scenarios. Note that this is
not adequate to use as a baseline for comparing relative performance,
as it does not respect legality constraints that, that is
$\input{gen/max_oracle_param_w} \not\in W_{safe}$.
Figure~\ref{fig:performance-legality} illustrates this point by
plotting both the legality, and performance relative to the oracle, of
distinct workgroup sizes.


\begin{figure}
\centering
\includegraphics{gen/img/params_summary.pdf}
\caption{%
  The red line shows the ``legality'' of workgroup sizes, i.e.\ the
  ratio of scenarios for which that workgroup size is legal.  The
  green lines show the geometric mean of the performance relative to
  the oracle for all scenarios for which the workgroup size is legal.%
}
\label{fig:performance-legality}
\end{figure}


\subsection{Workgroup Size Legality}


\cleardoublepage
\begin{figure}
\input{fig/performance-wgsizes}
\caption{%
  Comparing workgroup performance relative to the oracle as function
  of: (\subref{fig:performance-max-wgsize})~maximum legal size,
  (\subref{fig:performance-wg-c})~number of columns, and
  (\subref{fig:performance-wg-r})~ number of rows.%
}
\label{fig:performance-wgsizes}
\end{figure}
\newpage
\begin{figure}
\input{fig/performances}
\caption{%
  Performance relative to the oracle of workgroup sizes across all
  test cases, grouped by: (\subref{fig:performance-kernels})~kernels,
  (\subref{fig:performance-devices})~devices, and
  (\subref{fig:performance-datasets})~datasets.%
}
\label{fig:performances}
\end{figure}

% To begin to get a feel for the
%
% Figure~\ref{fig:performance-wgsizes}. Furthermore, the performance of
% workgroup sizes is shown in Figure~\ref{fig:performances} to be vary
% greatly across architectures, kernels, and datasets.

\TODO{What is the distribution of relative performance across
  architectures, programs, and datasets?}

\begin{table}
  \parbox{.45\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_params_coverage}
    \caption{The 25 workgroup sizes with the greatest legality.}
  }
  \hfill
  \parbox{.45\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_params_perf}
    \caption{The 25 workgroup sizes with the greatest performance.}
  }
\end{table}


\subsection{Performance Upper Bounds}

\begin{figure}
\includegraphics{gen/img/max_speedups}
\caption{%
  Speedup of oracle workgroup size over the worst workgroup size for
  each scenario, and the statically chosen workgroup size that
  provides the best overall performance.%
}
\label{fig:speedups}
\end{figure}

For a given scenario $s$, the ratio of the workgroups sizes from
$W_{legal}(s)$ which provide the longest and shortest mean runtimes is
used to calculate an upper bound for the possible performance
influence of workgroup size:

\begin{equation}
r_{max}(s) = r(s, \argmax_{w \in W_{legal}(s)} t(s,w), \Omega(s))
\end{equation}

When applied to each scenario $s \in S$ from the experimental results,
the average performance upper bound is found to be
$\input{gen/avg_possible_speedup}\times$ (range:
$\input{gen/min_possible_speedup}\times$ -
$\input{gen/max_possible_speedup}\times$).

\TODO{T-test to demonstrate confidence in differences between best and
  worst params.}


\subsection{Refused Parameters}

Of the \input{gen/num_params} workgroup sizes tested,
\input{gen/ratio_refused_params}\% were refused in one or more test
cases, with an average \input{gen/ratio_refused_test_cases}\% of test
cases leading to refused parameters. For a workgroup size to be
refused, it must satisfy the architectural and program-specific
constraints which are exposed by OpenCL, but still lead to a
\texttt{CL\_OUT\_OF\_RESOURCES} error when the kernel is
enqueued. Table~\ref{tab:top-refused-params} lists the most frequently
refused parameters, and the percentage of test cases for which they
were refused. While uncommon, a refused parameters is an obvious
inconvenience to the user, as one would expect that any workgroup size
within the specified maximum should behave \emph{correctly} if not
performantly.

Figure~\ref{fig:refused-params-by-dev-vendor} suggests that the
problem is vendor --- or at least device --- specific. The
distribution of refused parameters is skewed towards the Intel CPU
devices, while no workgroup sizes were refused by the AMD
GPU. \TODO{Bug reports\ldots} For now, it is imperative that the
autotuning system be capable of adapting to these refused parameters
by suggesting alternatives when they occur.

\begin{table}
\parbox{.32\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_refused_params_1}
  }
  \hfill
  \parbox{.32\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_refused_params_2}
  }
  \hfill
  \parbox{.32\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_refused_params_3}
  }
  \caption{The thirty most refused parameters, ranked in descending
    order.}
  \label{tab:top-refused-params}
\end{table}

\begin{figure}
\centering
\begin{subfigure}[h]{.45\textwidth}
  \centering
  \includegraphics{gen/img/refused_params_by_device}
  \caption{}
  \label{fig:refused-params-by-device}
\end{subfigure}
\hfill
\begin{subfigure}[h]{.45\textwidth}
  \centering
  \includegraphics{gen/img/refused_params_by_vendor}
  \caption{}
  \label{fig:refused-params-by-vendor}
\end{subfigure}
\caption{%
  The ratio of test cases with refused workgroup sizes, grouped by:
  (\subref{fig:refused-params-by-device}) device;
  (\subref{fig:refused-params-by-vendor}) OpenCL vendor.%
}
\label{fig:refused-params-by-dev-vendor}
\end{figure}

% \begin{subfigure}[h]{.45\textwidth}
%   \centering
%   \includegraphics{gen/img/refused_param_space}
%   \caption{}
%   \label{fig:refused-params-space}
% \end{subfigure}


\subsection{Baseline Parameters}

From the experimental results, the baseline workgroup size $\bar{w}$
is found to be \input{gen/one_r}, with a geometric mean performance
$\bar{p}(\bar{w})$ of
\input{gen/one_r_perf}. Figure~\ref{fig:speedups} shows the speedup of
the oracle over this baseline parameter for all scenarios. If we
assume that a pragmatic developer with enough time would eventually
find this static optimal, then this provides a reasonable upper bound
on the attainable improvements of an autotuner for workgroup size over
that of a static choice.


\subsection{Human Expert}


The stencil workgroup size as chosen by a human expert is
$32 \times 4$.

\TODO{How often does this parameter fail?}

\TODO{When it is legal, how much of the available performance does
  this attain?}


\subsection{Heuristics}

\subsubsection{Per-device workgroup sizes}

\TODO{%
  What is the performance of picking one value per: architecture,
  dataset, and program?%
}

\subsubsection{Using maximum legal size}

A common approach taken by application developers is to set the
workgroup size for a kernel based on the maximum legal workgroup size,
for example to use the square root of the maximum to set the number of
columns and rows.

\TODO{What is the perforamnce of always picking the max legal value?}

\subsubsection{Scaling with maximum legal size}

% \TODO{Partial baseline. How much performance could we get if we lock
%   one of the parameters and twiddle the other?}

% While the reasoning for this approach is perfectly intuitive, these
% results indicate that there no useful correlation between the ratio
% of a particular workgroup size to the maximum legal workgroup and
% the observed performance.

\subsection{Summary}

By comparing the relative performance of an average of
\input{gen/avg_num_params} workgroup sizes for each workload, the
following conclusions can be drawn:

\begin{enumerate}
\item The performance of a workgroup size for a particular workload
  depends properties of the hardware, software, and dataset. The
  performance gap between different workgroup sizes for a single
  workload is up to $\input{gen/max_possible_speedup}\times$.
\item Not all workgroup sizes are legal, and we can only test if a
  value \emph{is} legal at runtime.
\item Differing workloads have wildly different optimal workgroup
  sizes, and the best performance can be achieved using static tuning
  is optimal for only \input{gen/max_oracle_param_frequency} of
  workloads.
\end{enumerate}

% I believe this presents a compelling case for the development of an
% autotuner which can select the optimal workgroup size at runtime.


\section{Classification Performance}


To evaluate the performance of a classifier, a subset of scenarios
$S_{training} \subset S$ are labelled with the workgroup size which
gave the best performance for each. The classifier is trained on this
labelled training data, and tested using a set of unseen scenarios
$S_{testing} = S - S_{training}$. The performance of each predicted
oracle workgroup size is compared against the true oracles for that
set. % Several selections of $S_{training}$ have been used:


\section{Optimising Workgroup Sizes}

% To begin to get a feel for the
%
% Figure~\ref{fig:performance-wgsizes}. Furthermore, the performance of
% workgroup sizes is shown in Figure~\ref{fig:performances} to be vary
% greatly across architectures, kernels, and datasets.

\TODO{What is the distribution of relative performance across
  architectures, programs, and datasets?}

\begin{table}
  \parbox{.45\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_params_coverage}
    \caption{The 25 workgroup sizes with the greatest legality.}
  }
  \hfill
  \parbox{.45\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_params_perf}
    \caption{The 25 workgroup sizes with the greatest performance.}
  }
\end{table}


\begin{figure}
\centering
\includegraphics{gen/img/params_summary.pdf}
\caption{%
  The red line shows the ``legality'' of workgroup sizes, i.e.\ the
  ratio of scenarios for which that workgroup size is legal.  The
  green lines show the geometric mean of the performance relative to
  the oracle for all scenarios for which the workgroup size is legal.%
}
\label{fig:performance-legality}
\end{figure}

\subsection{Refused Parameters}

Of the \input{gen/num_params} workgroup sizes tested,
\input{gen/ratio_refused_params}\% were refused in one or more test
cases, with an average \input{gen/ratio_refused_test_cases}\% of test
cases leading to refused parameters. For a workgroup size to be
refused, it must satisfy the architectural and program-specific
constraints which are exposed by OpenCL, but still lead to a
\texttt{CL\_OUT\_OF\_RESOURCES} error when the kernel is
enqueued. Table~\ref{tab:top-refused-params} lists the most frequently
refused parameters, and the percentage of test cases for which they
were refused. While uncommon, a refused parameters is an obvious
inconvenience to the user, as one would expect that any workgroup size
within the specified maximum should behave \emph{correctly} if not
performantly.

Figure~\ref{fig:refused-params-by-dev-vendor} suggests that the
problem is vendor --- or at least device --- specific. The
distribution of refused parameters is skewed towards the Intel CPU
devices, while no workgroup sizes were refused by the AMD
GPU. \TODO{Bug reports\ldots} For now, it is imperative that the
autotuning system be capable of adapting to these refused parameters
by suggesting alternatives when they occur.

\begin{table}
\parbox{.32\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_refused_params_1}
  }
  \hfill
  \parbox{.32\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_refused_params_2}
  }
  \hfill
  \parbox{.32\linewidth}{
    \centering
    \scriptsize
    \rowcolors{2}{white}{gray!25}
    \input{gen/tab/top_refused_params_3}
  }
  \caption{The thirty most refused parameters, ranked in descending
    order.}
  \label{tab:top-refused-params}
\end{table}

\begin{figure}
\centering
\begin{subfigure}[h]{.45\textwidth}
  \centering
  \includegraphics{gen/img/refused_params_by_device}
  \caption{}
  \label{fig:refused-params-by-device}
\end{subfigure}
\hfill
\begin{subfigure}[h]{.45\textwidth}
  \centering
  \includegraphics{gen/img/refused_params_by_vendor}
  \caption{}
  \label{fig:refused-params-by-vendor}
\end{subfigure}
\caption{%
  The ratio of test cases with refused workgroup sizes, grouped by:
  (\subref{fig:refused-params-by-device}) device;
  (\subref{fig:refused-params-by-vendor}) OpenCL vendor.%
}
\label{fig:refused-params-by-dev-vendor}
\end{figure}

% \begin{subfigure}[h]{.45\textwidth}
%   \centering
%   \includegraphics{gen/img/refused_param_space}
%   \caption{}
%   \label{fig:refused-params-space}
% \end{subfigure}


\cleardoublepage
\begin{figure}
\input{fig/performance-wgsizes}
\caption{%
  Comparing workgroup performance relative to the oracle as function
  of: (\subref{fig:performance-max-wgsize})~maximum legal size,
  (\subref{fig:performance-wg-c})~number of columns, and
  (\subref{fig:performance-wg-r})~ number of rows.%
}
\label{fig:performance-wgsizes}
\end{figure}

\begin{figure}
\input{fig/performances}
\caption{%
  Performance relative to the oracle of workgroup sizes across all
  test cases, grouped by: (\subref{fig:performance-kernels})~kernels,
  (\subref{fig:performance-devices})~devices, and
  (\subref{fig:performance-datasets})~datasets.%
}
\label{fig:performances}
\end{figure}


\subsection{Performance Upper Bounds}

\begin{figure}
\includegraphics{gen/img/max_speedups}
\caption{%
  Speedup of oracle workgroup size over the worst workgroup size for
  each scenario, and the statically chosen workgroup size that
  provides the best overall performance.%
}
\label{fig:speedups}
\end{figure}

For a given scenario $s$, the ratio of the workgroups sizes from
$W_{legal}(s)$ which provide the longest and shortest mean runtimes is
used to calculate an upper bound for the possible performance
influence of workgroup size:

\begin{equation}
r_{max}(s) = r(s, \argmax_{w \in W_{legal}(s)} t(s,w), \Omega(s))
\end{equation}

When applied to each scenario $s \in S$ from the experimental results,
the average performance upper bound is found to be
$\input{gen/avg_possible_speedup}\times$ (range:
$\input{gen/min_possible_speedup}\times$ -
$\input{gen/max_possible_speedup}\times$).

\TODO{T-test to demonstrate confidence in differences between best and
  worst params.}


\subsection{Baseline Parameters}

From the experimental results, the baseline workgroup size $\bar{w}$
is found to be \input{gen/one_r}, with a geometric mean performance
$\bar{p}(\bar{w})$ of
\input{gen/one_r_perf}. Figure~\ref{fig:speedups} shows the speedup of
the oracle over this baseline parameter for all scenarios. If we
assume that a pragmatic developer with enough time would eventually
find this static optimal, then this provides a reasonable upper bound
on the attainable improvements of an autotuner for workgroup size over
that of a static choice.


\subsection{Oracle Workgroup Sizes}

Figure~\ref{fig:max-wgsizes} shows the distribution of maximum
workgroup sizes across all scenarios. Figure~\ref{fig:oracle-wgsizes}
shows the distribution of oracle workgroup sizes, demonstrating that
there is clearly no ``silver bullet'' workgroup size which works for
all scenarios. As Figure~\ref{fig:oracle-accuracy} shows,
\input{gen/num_wgsizes_50_accuracy} unique workgroup sizes are
required in order to achieve oracle performance just 50\% of the
time.

\begin{figure}
\centering
\includegraphics{gen/img/num_params_oracle.pdf}
\caption{%
  Accuracy compared to the oracle as a function of the number of
  workgroup sizes used. The best accuracy that is achievable using a
  single statically chosen value is
  \protect\input{gen/max_oracle_param_frequency}.%
}
\label{fig:oracle-accuracy}
\end{figure}

The mode of the oracle workgroup sizes is
\input{gen/max_oracle_param}, which is optimal for
\input{gen/max_oracle_param_frequency} of scenarios. Note that this is
a different value from the \emph{baseline} identified previously,
which identifies the single choice which provides the best average
case performance. This is because the value is not legal for all
scenarios; that is, $\input{gen/max_oracle_param_w} \not\in W_{safe}$.
Figure~\ref{fig:performance-legality} illustrates this point by
plotting the geometric mean performance across workgroup sizes for:
all scenarios, and only the scenarios for which the workgroup size is
legal.

\TODO{How many distinct oracle workgroup sizes are there?}

\TODO{What does the distribution of oracle workgroup sizes look like?}


\subsection{Human Expert}


The stencil workgroup size as chosen by a human expert is
$32 \times 4$.

\TODO{How often does this parameter fail?}

\TODO{When it is legal, how much of the available performance does
  this attain?}


\subsection{Heuristics}

\subsubsection{Per-device workgroup sizes}

\TODO{%
  What is the performance of picking one value per: architecture,
  dataset, and program?%
}

\subsubsection{Using maximum legal size}

A common approach taken by application developers is to set the
workgroup size for a kernel based on the maximum legal workgroup size,
for example to use the square root of the maximum to set the number of
columns and rows.

\TODO{What is the perforamnce of always picking the max legal value?}

\subsubsection{Scaling with maximum legal size}

% \TODO{Partial baseline. How much performance could we get if we lock
%   one of the parameters and twiddle the other?}

% While the reasoning for this approach is perfectly intuitive, these
% results indicate that there no useful correlation between the ratio
% of a particular workgroup size to the maximum legal workgroup and
% the observed performance.

\subsection{Summary}

By comparing the relative performance of an average of
\input{gen/avg_num_params} workgroup sizes for each workload, the
following conclusions can be drawn:

\begin{enumerate}
\item The performance of a workgroup size for a particular workload
  depends properties of the hardware, software, and dataset. The
  performance gap between different workgroup sizes for a single
  workload is up to $\input{gen/max_possible_speedup}\times$.
\item Not all workgroup sizes are legal, and we can only test if a
  value \emph{is} legal at runtime.
\item Differing workloads have wildly different optimal workgroup
  sizes, and the best performance can be achieved using static tuning
  is optimal for only \input{gen/max_oracle_param_frequency} of
  workloads.
\end{enumerate}

% I believe this presents a compelling case for the development of an
% autotuner which can select the optimal workgroup size at runtime.


\section{Classification Performance}


To evaluate the performance of a classifier, a subset of scenarios
$S_{training} \subset S$ are labelled with the workgroup size which
gave the best performance for each. The classifier is trained on this
labelled training data, and tested using a set of unseen scenarios
$S_{testing} = S - S_{training}$. The performance of each predicted
oracle workgroup size is compared against the true oracles for that
seTODO{What is the average speedup achieved using different
  classifiers?}

\TODO{What percentage of the maximum performance do the classifiers
  achieve?}

% \begin{figure}
% \centering
% \includegraphics{gen/img/classification-xval.pdf}
% \caption{%
%   Classification performance using cross-validation.%
% }
% \end{figure}

% \begin{figure}
% \centering
% \includegraphics{gen/img/classification-xval-random.pdf}
% \caption{%
%   Per-instance performances of classification using cross-validation
%   and the ``random'' error handler.%
% }
% \end{figure}


% \TODO{Rank eigenvectors of PCA on features.}

% \TODO{Evaluate the effectiveness of training with synthetic
%   benchmarks.}

\subsection{Selection of Features}

\TODO{How does the selection of features affect performance? Show a
  plot of performance vs number of features.}


\subsection{Effectiveness of Synthetic Benchmarks}

\TODO{How well does classification perform when trained solely on
  synthetic benchmarks, and tested soley on real benchmarks?}

\TODO{Repeat the above, but with incremtally fewere benchmarks.}


\subsection{Autotuning Overheads}

\TODO{What is the costs of auto-tuning?}

\TODO{How many iterations do we need to do before we ``break even''?}

% \section{Predicting Stencil Runtime}

% \begin{enumerate}
% \item How accurately does it predict the runtime of a stencil?
% \item What is the relationship between \emph{classification} accuracy,
%   and the accuracy of predicted runtimes? Does it really matter if the
%   predicted runtime is inaccurate?
% \end{enumerate}

% \begin{figure}
% \centering
% \includegraphics{gen/img/runtime-regression-xval.pdf}
% \caption{%
%   Performance of regressors at predicting program runtime.%
% }
% \end{figure}


% \section{Predicting Relative Performance}

\section{Comparison with hand tuned implementations}

\TODO{%
  Compare performance against some hand-crafted implementation of one
  of the real benchmarks.%
}


\section{Summary}

% \begin{table}
% \scriptsize
% \input{\DataDir/tab/xval.tex}
% \caption{Results of 10 fold cross-validation.}
% \end{table}

% \begin{table}
% \scriptsize
% \input{\DataDir/tab/synthetic_real.tex}
% \caption{Results of training using synthetic benchmarks and testing on
%   real.}
% \end{table}
