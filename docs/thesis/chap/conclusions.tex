As the trend towards higher core counts and increasing parallelism
continues, the need for high level, accessible abstractions to manage
such parallelism will continue to go. Autotuning proves a valuable aid
for achieving these goals, providing the benefits of low level
performance tuning while maintaining ease of use, without burdening
developers with optimisation concerns. As the need for autotuned
parallelism rises, the desire for collaborative techniques for sharing
performance data must be met with systems capable of supporting this
cross-platform learning.

In this thesis, I have presented my attempt at providing such a
system, by designing a novel framework which has the benefits of fast,
``always-on'' autotuning, while being able to synchronise data with
global repositories of knowledge which others may contribute to. The
framework provides an interface for autotuning which is sufficiently
generic to be easily re-purposed to target a range of optimisation
parameters.

To demonstrate the utility of this framework, I implemented a frontend
for predicting the workgroup size of OpenCL kernels for SkelCL stencil
codes. This optimisation space is complex, non linear, and critical
for the performance of stencil kernels, with up to a $207.72\times$
slowdown if an improper value is picked. Selecting the correct
workgroup size is difficult --- requiring a knowledge of the kernel,
dataset, and underlying architecture. The challenge is increased even
more so by inconsistencies in the underlying system which cause some
workgroup sizes to fail completely. Autotuning in this space requires
a system which is resilient this challenging in the space, and several
techniques were proposed to address this.


33\% over that of a human expert, while .

\section{Critical Analysis}

% TODO: Review against project plan.

% Achievements:
%
% Understanding a large code base (SkelCL)

% Things I could have done
%
% How many features are required?
% How many synthetic benchmarks are required?
% Compare against hand-crafted stencils


\section{Future Work}

% \TODO{Write an experiment for which static instruction counts fall
% down. For example, two programs with similar instruction counts, one
% with a huge loop, the other with straight line code.}

\begin{algorithm}
\input{alg/autotune-hybrid}
\caption{%
  Selecting workgroup size using a combination of classifiers and
  regressors.%
}
\label{alg:autotune-hybrid}
\end{algorithm}

% TODO: Auto-discovery of errors & bug reports

% TODO: Extensions to SkelCL: Cluster parallelism, multi-dimensional
% stencils, stencil specialisations for specific memory access
% patterns.

% TODO: Additional optimisation parameters, e.g. which device to
% execute on.

% TODO: Apply to additional skeleton libraries.

% TODO: energy as an optimisation cotarget.


% Leather, H., O’Boyle, M., & Worton, B. (2009). Raced Profiles:
% Efficient Selection of Competing Compiler Optimizations. In LCTES
% ’09: Proceedings of the ACM SIGPLAN/SIGBED 2009 Conference on
% Languages, Compilers, and Tools for Embedded Systems
% (pp. 1–10). Dublin.
\TODO{%
  Consider using adaptive sampling plans and some sort of global
  mechanism for performing cooperative exploration across multiple
  devices, while reducing the number of samples required to
  distinguish good from bad workgroup sizes~\cite{Leather2009}.%
}

\TODO{%
  Make the remote active. Rather than simply fetching from remote SQL
  tables, the remote could be a webserver with a REST api for HTTP
  GET/PUT of training data, and could analyse the data and build
  models asynchronously.%
}
