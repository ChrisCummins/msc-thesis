\section{Introduction}

In this chapter I apply the OmniTune framework to SkelCL. The publicly
available implementation
\footnote{\url{https://github.com/ChrisCummins/omnitune}} predicts
workgroup sizes for OpenCL stencil skeleton kernels in order to
minimise their runtime on CPUs and multi-GPU systems. The optimisation
space presented by the workgroup size of OpenCL kernels is large,
complex, and non-linear. Successfully applying machine learning to
such a space requires plentiful training data, and the careful
selection of features, and classification approach. The following
sections address these challenges.


\section{Understanding the Space}

SkelCL stencil kernels are parameterised by a workgroup size $w$,
which consists of two integer values to denote the number of rows and
columns (where we need to distinguish the individual components, we
will use symbols $w_r$ and $w_c$ to denote rows and columns,
respectively).


\subsection{Constraints}

Unlike in many autotuning applications, the space of possible
parameter values $W$ is subject to hard constraints, and these
constraints cannot conviently be statically determined.

\subsubsection{Architectural constraints}

Each OpenCL device imposes a maximum workgroup size which can be
statically checked by querying the \texttt{clGetDeviceInfo()}
API. These are defined by the limits of the hardware, typically the
number of execution units which have access to a shared memory pool.


\subsubsection{Kernel constraints}

At runtime, once an OpenCL program has been compiled to a kernel,
users can query the maximum workgroup size supported by that kernel
using the \texttt{clGetKernelInfo()} API. Crucially, this value cannot
easily be obtained statically, as there is no mechanism to determine
the maximum workgroup size for a given source code and device without
first compiling it, which in OpenCL does not occur until
runtime. Factors which affect a kernel's maximum workgroup size
include the number registers required for a kernel, and the available
number of SIMD execution units for each type of instructions in a
kernel.


\subsubsection{Refused parameters}

$w < W_{max}(s), w \not\in W_{refused}(s)$


The subject to satisfying two hard constraints: the maximum workgroup
size imposed by the architecture, and the maximum workgroup size
imposed by a kernel. Exceeding either of these constraints will cause
an \texttt{CL\_OUT\_OF\_RESOURCES} error when the OpenCL kernel is
enqueued.

For a given \emph{scenario} $s$ (a combination of program, device, and
dataset), the subset of workgroup sizes $W_{legal}(s) \subsetq W$
(where $W$ is the set of all possible workgroup sizes) that are legal
is defined as:

\begin{equation}
  W_{legal}(s) = \left\{w | w \in W, w < W_{max}(s) \right\} - W_{refused}(s)
\end{equation}




From the set of all workgroup sizes $W$, the subset of workgroup sizes
$W_{legal}(s) \subseteq W$ that are legal for a given scenario $s$ can
be found using:

\begin{equation}
  W_{legal}(s) = \left\{w | w \in W, w < W_{max}(s) \right\} - W_{refused}(s)
\end{equation}




% Anyone downloading a copy of OmniTune will instantly have access to
% the global database of training data, including the
% \input{gen/num_samples} runtimes which were collected to write this
% thesis.

% \texttt{cec.chlox1mra3iz.us-west-2.rds.amazonaws.com:3306}

\section{Training}

One challenge of performing empirical performance evaluations is
gathering enough applications to ensure meaningful
comparisons. Synthetic benchmarks are a popular method for
circumventing this problem. The automatic generation of such
benchmarks has clear benefits for reducing evaluation costs; however,
creating meaningful benchmark programs is a difficult problem if we
are to avoid the problems of redundant computation and produce
provable halting benchmarks.

In practise, stencil codes exhibit many common traits: they have a
tightly constrained interface, predictable memory access patterns, and
well defined numerical input and output data types. This can be used
to create a confined space of possible stencil codes by enforcing
upper and lower bounds on properties of the codes which can not
normally be guaranteed for general-purpose programs, e.g.\ the number
of floating point operations. In doing so, it is possible to
programatically generate stencil workloads which share similar
properties to those which we intend to target.

Based on observations of real world stencil codes from the fields of
cellular automata, image processing, and PDE solvers, I implemented a
stencil generator which uses parameterised kernel templates to produce
source codes for collecting training data. The stencil codes are
parameterised by stencil shape (one parameter for each of the four
directions), input and output data types (either integers, or single
or double precision floating points), and \emph{complexity} --- a
simple boolean metric for indicating the desired number of memory
accesses and instructions per iteration, reflecting the relatively
bi-modal nature of the reference stencil codes, either compute
intensive (e.g. FDTD simulation), or lightweight (e.g. Game of Life).

A total of 68 synthetic benchmarks were generated for collecting
performance training data. Using a large number of synthetic
benchmarks helps adress the ``small $n$, large $P$'' problem, which
describes the difficulty of statistical inference in spaces for which
the set of possible hypotheses $P$ is significantly larger than the
number of observations $n$\CitationNeeded{}. By creating
parameterised, synthetic benchmarks, it is possible to explore a much
larger set of the space of possible stencil codes than if relying
solely on reference applications, reducing the risk of overfitting to
particular program features.


\section{Stencil Features}


\begin{table}
\input{tab/features}
\caption{Stencil features and their types, describing the dataset, kernel,
  and device.}
\label{tab:features}
\end{table}


Properties of the architecture, program, and dataset all contribute to
the performance of a workgroup size. The success of a machine learning
system depends on the ability to translate these properties into
meaningful explanatory variables --- \emph{features}. To capture this
in OmniTune, requests for paramters are packed with a copy of the
OpenCL kernel and attributes of the dataset and device to extract
vectors of 102 features describing hte architecture, kernel, and
dataset. Table~\ref{tab:features} includes a full list of features and
types.


\begin{itemize}
\item \textbf{Device} --- OmniTune uses the OpenCL
  \texttt{clGetDeviceInfo()} API to query a number of properties about
  the target execution device. Examples include the size of local
  memory, maximum work group size, number of compute units, etc.
\item \textbf{Kernel} --- The user code for a stencil is passed to the
  OmniTune server, which compiles the OpenCL kernel to LLVM IR
  bitcode. The \texttt{opt} \texttt{InstCount} statistics pass is used
  to obtain static counts for each type of instruction present in the
  kernel, as well as the total number of instructions. The instruction
  counts for each type are divided by the total number of instructions
  to produce a \emph{density} of instruction for that type. Examples
  include total static instruction count, ratio of instructions per
  type, ratio of basic blocks per instruction, etc.
\item \textbf{Dataset} --- The SkelCL container type is used to
  extract the input and output data types, and the 2D grid size.
\end{itemize}


\subsection{Reducing Feature Extraction Overhead}


Feature extraction (particularlly compilation to LLVM IR) introduces a
runtime overhead to the classification process (\TODO{How much?}). To
minimise this, lookup tables for device and dataset features are used,
and cached locally in the OmniTune server and pushed to the remote
data store. The device ID is used to index the devices table, and the
checksum of an OpenCL source is used to index the kernel features
table. Before feature extraction occurs for either, a lookup is
performed in the relevant table, meaning that the cost of feature
extraction is amortised over time.


\section{Predicting Workgroup Sizes}

The OmniTune server supports three methods of predicting workgroup
size. The first approach to selecting workgroup sizes is to convert
the set of oracle workgroup sizes into a hypothesis space and then use
a classifier to predict for a given set of features the workgroup size
which will provide the best performance.


\subsubsection{Satisfying Constraints}

The set of workgroup sizes which a classifier may predict is defined
by the oracle workgroup sizes of the training data:

\begin{equation}
W_{training} = \{ \Omega(s) | s \in S_{training} \}
\end{equation}

This does not guarantee that the set of workgroup sizes which may be
predicted is within the set of legal workgroup sizes for each tested
scenario:

\begin{equation}
W_{training} \nsubseteq \cup \{ W_{legal}(s) | s \in S_{testing} \}
\end{equation}

\TODO{2 solutions: satisfy constraint at classification time
  (i.e. error handlers), or at training time (i.e. one classifier per
  max-wg-size)\ldots}

As a result, it is possible that a classifier will predict a workgroup
size that is invalid for a given scenario, $w \not\in W_{legal}(s)$.
For these cases, I evaluate the effective of three fallback strategies
to select a legal workgroup size:

\begin{enumerate}
\item \emph{Baseline} --- select the workgroup size which is known to
  be safe and provides the highest average case performance on
  training data.
\item \emph{Random} --- select a random workgroup size which is known
  to be legal $w \in W_{legal}(s)$.
\item \emph{Reshape} --- scale the predicted workgroup size
  proportionally so that it fits within the space of legal workgroup
  sizes, $w < W_{max}(s)$. This attempts to preserve ``shape'' of the
  predicted workgroup size.
\end{enumerate}

See Algorithm~\ref{alg:autotune-classification} for definitions. The
evaluation compares the average performance achieved using each
fallback strategy, along with the percentage of cases for which these
fallback strategies were required.


\begin{algorithm}
\input{alg/autotune-classification}
\caption{Select optimal workgroup size using classification}
\label{alg:autotune-classification}
\end{algorithm}


\begin{algorithm}
\input{alg/autotune-classification2}
\caption{Select optimal workgroup size using multiple classifiers}
\label{alg:autotune-classification2}
\end{algorithm}


\TODO{Implement and evaluate classification using one classifier for
  each unique $w \in \cup \{ W_{max}(s) | s \in S_{training} \}$. This
  would be an alternative method of overcoming the invalid prediction
  problem.}

\subsection{Predicting Stencil Code Runtime}

\TODO{The first approach requires very expensive training and isn't
  amenable to online tuning\ldots} A second approach to selecting the
workgroup size is to attempt to directly predict the runtime of a
given workload. Given a regressor $f(a,k,d,w)$ which predicts the
runtime of a program given a set of architecture, kernel, and dataset
features $a,k,d$, and workgroup size $w$, the predicted optimal
workgroup size is the $w$ value which minimises the output of the
regressor:

\begin{equation}
  \underset{w \in W_{legal}(s)}{\argmin} f(a,k,d,w)
\end{equation}

By predicting the runtimes of only the workgroup sizes from within
$W_{legal}(s)$, we overcome the issue of predicting invalid workgroup
sizes. Training a regressor to predict runtimes requires a set of
training data which contains workgroup sizes as an additional feature,
and the measured mean runtime as the label.

% \begin{algorithm}
% \input{alg/autotune-runtime-regression}
% \caption{Selecting workgroup size using runtime regression}
% \label{alg:autotune-runtime-regression}
% \end{algorithm}

\subsection{Predicting Relative Performance}

Accurately predicting the runtime of an arbitrary program is a
difficult problem due to the impacts of flow control. It may be
effective to instead predict the \emph{relative} performance of two
different workgroup sizes for the same program. To do this, we select
a baseline workgroup size $w_b \in W_{safe}$, and train a regressor
$g(a,k,d,w)$ with training data labelled with the relative performance
over the baseline $r(w, w_b)$. Predicting the optimal workgroup
requires maximising the output of the regressor:

\begin{equation}
  \underset{w \in W_{legal}(s)}{\argmax} g(a,k,d,w)
\end{equation}

% \begin{algorithm}
% \input{alg/autotune-speedup-regression}
% \caption{Selecting workgroup size using speedup regression}
% \label{alg:autotune-speedup-regression}
% \end{algorithm}


\subsection{Meta-tuning: a Hybrid Approach}

\TODO{Implement and test. See Algorithm~\ref{alg:autotune-hybrid}.}

\begin{algorithm}
\input{alg/autotune-hybrid}
\caption{Selecting workgroup size using a hybrid approach}
\label{alg:autotune-hybrid}
\end{algorithm}


\section{Implementation}

By design, the client-server model allows for enabling autotuning with
a low impact on the target applications. As such, the modifications
required to enable OmniTune support within SkelCL were minimal. The
hard-coded workgroup size used in the stencil skeleton is replaced by
a call to an OmniTune server, which uses classifiers
Figure~\ref{fig:omnitune-system-flow} shows the behaviour of the
OmniTune server.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{img/omnitune-data-schema.pdf}
\caption{%
  OmniTune SkelCL schema.%
}
\label{fig:omnitune-system-flow}
\end{figure}


\section{Summary}
