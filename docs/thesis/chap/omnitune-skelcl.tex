\section{Introduction}

In this chapter I apply the OmniTune framework to SkelCL. The publicly
available implementation
\footnote{\url{https://github.com/ChrisCummins/omnitune}} predicts
workgroup sizes for OpenCL stencil skeleton kernels in order to
minimise their runtime on CPUs and multi-GPU systems. The optimisation
space presented by the workgroup size of OpenCL kernels is large,
complex, and non-linear. Successfully applying machine learning to
such a space requires plentiful training data, and the careful
selection of features, model. The following sections address these
challenges.

% Anyone downloading a copy of OmniTune will instantly have access to
% the global database of training data, including the
% \input{gen/num_samples} runtimes which were collected to write this
% thesis.

% \texttt{cec.chlox1mra3iz.us-west-2.rds.amazonaws.com:3306}

\section{Training}

One challenge of performing empirical performance evaluations is
gathering enough applications to ensure meaningful
comparisons. Synthetic benchmarks are a popular method for
circumventing this problem. The automatic generation of such
benchmarks has clear benefits for reducing evaluation costs; however,
creating meaningful benchmark programs is a difficult problem if we
are to avoid the problems of redundant computation and produce
provable halting benchmarks.

In practise, stencil codes exhibit many common traits: they have a
tightly constrained interface, predictable memory access patterns, and
well defined numerical input and output data types. This can be used
to create a confined space of possible stencil codes by enforcing
upper and lower bounds on properties of the codes which can not
normally be guaranteed for general-purpose programs, e.g.\ the number
of floating point operations. In doing so, it is possible to
programatically generate stencil workloads which share similar
properties to those which we intend to target.

Based on observations of real world stencil codes from the fields of
cellular automata, image processing, and PDE solvers, I implemented a
stencil generator which uses parameterised kernel templates to produce
source codes for collecting training data. The stencil codes are
parameterised by stencil shape (one parameter for each of the four
directions), input and output data types (either integers, or single
or double precision floating points), and \emph{complexity} --- a
simple boolean metric for indicating the desired number of memory
accesses and instructions per iteration, reflecting the relatively
bi-modal nature of the reference stencil codes, either compute
intensive (e.g. FDTD simulation), or lightweight (e.g. Game of Life).

A total of 68 synthetic benchmarks were generated for collecting
performance training data. Using a large number of synthetic
benchmarks helps adress the ``small $n$, large $P$'' problem, which
describes the difficulty of statistical inference in spaces for which
the set of possible hypotheses $P$ is significantly larger than the
number of observations $n$\CitationNeeded{}. By creating
parameterised, synthetic benchmarks, it is possible to explore a much
larger set of the space of possible stencil codes than if relying
solely on reference applications, reducing the risk of overfitting to
particular program features.


\section{Feature Extraction}

Properties of the architecture, program, and dataset all contribute to
the performance of a workgroup size. The success of a machine learning
system depends on the ability to translate these properties into
meaningful explanatory variables --- \emph{features}. For each
scenario, a vector of 102 features is extracted to capture properties
of the architecture, device, and dataset.


\subsubsection{Architectural features}

OmniTune use the OpenCL \texttt{clGetDeviceInfo()} API to query a
number of properties about the target execution device. Examples
include the size of local memory, maximum work group size, number of
compute units, etc.


\subsubsection{Kernel features}

To extract features of the kernel, the user code for the stencil is
passed to the OmniTune server, which compiles the OpenCL kernel to
LLVM IR bitcode. The \texttt{opt} \texttt{InstCount} statistics pass
is used to obtain instruction counts for each type present in the
kernel, and the total number of instructions. The instruction counts
for each type are divided by the total number of instructions to
produce a \emph{ratio} of instruction for that type. Examples include
total static instruction count, ratio of instructions per type, ratio
of basic blocks per instruction, etc.

\TODO{Write an experiment for which static instruction counts fall
  down. For example, two programs with similar instruction counts, one
  with a huge loop, the other with straight line code.}


\subsubsection{Dataset features}

Dataset features are extracted from the SkelCL container type. The
extracted features are the input and output data types, and the 2D
grid size.

See Table~\ref{tab:features} for a full list of features and
types.

\subsubsection{Cost of Feature Extraction}

\TODO{Report feature extraction cost. Note that feature vectors are
  cached so this cost is a one-off, subsequent iterations are just a
  table lookup.}

\begin{table}
\input{tab/features}
\caption{Feature names and types, describing the dataset, kernel,
  and device.}
\label{tab:features}
\end{table}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{img/omnitune-data-schema.pdf}
\caption{%
  OmniTune SkelCL schema.%
}
\label{fig:omnitune-system-flow}
\end{figure}


\section{Predicting Workgroup Size}




\subsection{Predicting Optimal Workgroup Sizes}

The first approach to selecting workgroup sizes is to convert the set
of oracle workgroup sizes into a hypothesis space and then use a
classifier to predict the oracle workgroup size for a given set of
features. To evaluate this approach, a subset of scenarios
$S_{training} \subset S$ are labelled with their oracle workgroup
size. The classifier is trained on this labelled training data, and
tested using a set of unseen scenarios
$S_{testing} = S - S_{training}$. The performance of each predicted
oracle workgroup size is compared against the true oracles for that
set.


\subsubsection{Satisfying Constraints}

The set of workgroup sizes which a classifier may predict is defined
by the oracle workgroup sizes of the training data:

\begin{equation}
W_{training} = \{ \Omega(s) | s \in S_{training} \}
\end{equation}

This does not guarantee that the set of workgroup sizes which may be
predicted is within the set of legal workgroup sizes for each tested
scenario:

\begin{equation}
W_{training} \nsubseteq \cup \{ W_{legal}(s) | s \in S_{testing} \}
\end{equation}

\TODO{2 solutions: satisfy constraint at classification time
  (i.e. error handlers), or at training time (i.e. one classifier per
  max-wg-size)\ldots}

As a result, it is possible that a classifier will predict a workgroup
size that is invalid for a given scenario, $w \not\in W_{legal}(s)$.
For these cases, I evaluate the effective of three fallback strategies
to select a legal workgroup size:

\begin{enumerate}
\item \emph{Baseline} --- select the workgroup size which is known to
  be safe and provides the highest average case performance on
  training data.
\item \emph{Random} --- select a random workgroup size which is known
  to be legal $w \in W_{legal}(s)$.
\item \emph{Reshape} --- scale the predicted workgroup size
  proportionally so that it fits within the space of legal workgroup
  sizes, $w < W_{max}(s)$. This attempts to preserve ``shape'' of the
  predicted workgroup size.
\end{enumerate}

See Algorithm~\ref{alg:autotune-classification} for definitions. The
evaluation compares the average performance achieved using each
fallback strategy, along with the percentage of cases for which these
fallback strategies were required.


\begin{algorithm}
\input{alg/autotune-classification}
\caption{Select optimal workgroup size using classification}
\label{alg:autotune-classification}
\end{algorithm}


\begin{algorithm}
\input{alg/autotune-classification2}
\caption{Select optimal workgroup size using multiple classifiers}
\label{alg:autotune-classification2}
\end{algorithm}


\TODO{Implement and evaluate classification using one classifier for
  each unique $w \in \cup \{ W_{max}(s) | s \in S_{training} \}$. This
  would be an alternative method of overcoming the invalid prediction
  problem.}

\subsection{Predicting Stencil Code Runtime}

\TODO{The first approach requires very expensive training and isn't
  amenable to online tuning\ldots} A second approach to selecting the
workgroup size is to attempt to directly predict the runtime of a
given workload. Given a regressor $f(a,k,d,w)$ which predicts the
runtime of a program given a set of architecture, kernel, and dataset
features $a,k,d$, and workgroup size $w$, the predicted optimal
workgroup size is the $w$ value which minimises the output of the
regressor:

\begin{equation}
  \underset{w \in W_{legal}(s)}{\argmin} f(a,k,d,w)
\end{equation}

By predicting the runtimes of only the workgroup sizes from within
$W_{legal}(s)$, we overcome the issue of predicting invalid workgroup
sizes. Training a regressor to predict runtimes requires a set of
training data which contains workgroup sizes as an additional feature,
and the measured mean runtime as the label.

% \begin{algorithm}
% \input{alg/autotune-runtime-regression}
% \caption{Selecting workgroup size using runtime regression}
% \label{alg:autotune-runtime-regression}
% \end{algorithm}

\subsection{Predicting Relative Performance}

Accurately predicting the runtime of an arbitrary program is a
difficult problem due to the impacts of flow control. It may be
effective to instead predict the \emph{relative} performance of two
different workgroup sizes for the same program. To do this, we select
a baseline workgroup size $w_b \in W_{safe}$, and train a regressor
$g(a,k,d,w)$ with training data labelled with the relative performance
over the baseline $r(w, w_b)$. Predicting the optimal workgroup
requires maximising the output of the regressor:

\begin{equation}
  \underset{w \in W_{legal}(s)}{\argmax} g(a,k,d,w)
\end{equation}

% \begin{algorithm}
% \input{alg/autotune-speedup-regression}
% \caption{Selecting workgroup size using speedup regression}
% \label{alg:autotune-speedup-regression}
% \end{algorithm}


\subsection{Meta-tuning: a Hybrid Approach}

\TODO{Implement and test. See Algorithm~\ref{alg:autotune-hybrid}.}

\begin{algorithm}
\input{alg/autotune-hybrid}
\caption{Selecting workgroup size using a hybrid approach}
\label{alg:autotune-hybrid}
\end{algorithm}


\section{Implementation}

By design, the client-server model allows for enabling autotuning with
a low impact on the target applications. As such, the modifications
required to enable OmniTune support within SkelCL were minimal. The
hard-coded workgroup size used in the stencil skeleton is replaced by
a call to an OmniTune server, which uses classifiers
Figure~\ref{fig:omnitune-system-flow} shows the behaviour of the
OmniTune server.


\section{Summary}
