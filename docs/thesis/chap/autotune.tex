\section{Introduction}

In this chapter we describe the design and implementation of OmniTune,
an extensible, dynamic autotuner. We evaluate the performance of
OmniTune when tasked with tuning workgroup size of stencil codes.


\section{Machine Learning and Performance Tuning}

\TODO{We're always dealing with a sparsity in data.}


\subsection{Feature Extraction}

\TODO{The first step to developing the autotuner is feature
  extraction. That means mining the dataset to begin to correlate the
  measured dependent variable (in this case, some measure of
  performance) with explanatory variables, or features. There are
  three sets of features we are interested in: the hardware, software,
  and dataset.}

\todo{For hardware features, it's simply a case of querying the OpenCL
  API to fetch relevant device information, such as the number of
  cores available, and size of local memory. Since SkelCL supports
  multi-GPU parallelism, we'll make a note of how many devices were
  used, too.}

\todo{The software features are a little more tricky. We're looking
  for a way to capture some description of the *computation* of a
  given source code. For this, I first compile a kernel to LLVM
  bitcode, and then use the static instruction counts generated by
  LLVM's InstCount pass to generate a feature vector of the total
  instruction count and the density of different kinds of
  instructions, e.g. the number of floating point additions per
  instruction. This is sufficient for my needs but it is worth noting
  that such a crude metric may likely fall down in the presence of
  sufficiently diverging control flow, since the static instruction
  counts would no longer resemble the number of instructions actually
  executed.}

\todo{Dataset features are simple by comparison - I merely record the
  width and height of the grid, and use C++ template functions to
  stringify the input and output data types.}

For each scenario, a feature vector is extracted to capture properties
of the architecture, device, and dataset:

\begin{itemize}
\item \emph{Architectural features} --- size of local memory, maximum
  work group size, number of compute units, etc. Accessed using the
  OpenCL \texttt{clGetDeviceInfo()} API.
\item \emph{Kernel features} --- total static instruction count, ratio
  of instructions per type, ratio of basic blocks per instruction,
  etc. Accessed by compiling the OpenCL kernel to LLVM IR bitcode, and
  using the \texttt{opt} \texttt{InstCount} statistics pass.
\item \emph{Dataset features} --- size and type of the
  dataset. Accessed from the SkelCL Matrix container type.
\end{itemize}

See \FIXME{No appendix!} for a full list of features and types. For
training, feature vectors are labelled with the oracle workgroup size,
and a classifier is trained on a subset of this labelled training
data. The performance of the classifier is evaluated by comparing the
performance of the workgroup size predicted for an unseen feature
vector against the oracle workgroup size for that feature.

A full list of the feature names and types used to train machine
learning models. For training data, each feature vector was labelled
with the oracle workgroup size.

\begin{multicols}{2}
\begin{Verbatim}[fontsize=\footnotesize]
data_width                         numeric
data_height                        numeric
data_tin                           nominal
data_tout                          nominal
kern_north                         numeric
kern_south                         numeric
kern_east                          numeric
kern_west                          numeric
kern_max_wg_size                   numeric
kern_instruction_count             numeric
kern_ratio_AShr_insts              numeric
kern_ratio_Add_insts               numeric
kern_ratio_Alloca_insts            numeric
kern_ratio_And_insts               numeric
kern_ratio_Br_insts                numeric
kern_ratio_Call_insts              numeric
kern_ratio_FAdd_insts              numeric
kern_ratio_FCmp_insts              numeric
kern_ratio_FDiv_insts              numeric
kern_ratio_FMul_insts              numeric
kern_ratio_FPExt_insts             numeric
kern_ratio_FPToSI_insts            numeric
kern_ratio_FSub_insts              numeric
kern_ratio_GetElementPtr_insts     numeric
kern_ratio_ICmp_insts              numeric
kern_ratio_InsertValue_insts       numeric
kern_ratio_Load_insts              numeric
kern_ratio_Mul_insts               numeric
kern_ratio_Or_insts                numeric
kern_ratio_PHI_insts               numeric
kern_ratio_Ret_insts               numeric
kern_ratio_SDiv_insts              numeric
kern_ratio_SExt_insts              numeric
kern_ratio_SIToFP_insts            numeric
kern_ratio_SRem_insts              numeric
kern_ratio_Select_insts            numeric
kern_ratio_Shl_insts               numeric
kern_ratio_Store_insts             numeric
kern_ratio_Sub_insts               numeric
kern_ratio_Trunc_insts             numeric
kern_ratio_UDiv_insts              numeric
kern_ratio_Xor_insts               numeric
kern_ratio_ZExt_insts              numeric
kern_ratio_basic_blocks            numeric
kern_ratio_memory_instructions     numeric
kern_ratio_non_external_functions  numeric
dev_count                          numeric
dev_address_bits                   numeric
dev_double_fp_config               numeric
dev_endian_little                  numeric
dev_execution_capabilities         numeric
dev_extensions                     nominal
dev_global_mem_cache_size          numeric
dev_global_mem_cache_type          numeric
dev_global_mem_cacheline_size      numeric
dev_global_mem_size                numeric
dev_host_unified_memory            numeric
dev_image2d_max_height             numeric
dev_image2d_max_width              numeric
dev_image3d_max_depth              numeric
dev_image3d_max_height             numeric
dev_image3d_max_width              numeric
dev_image_support                  numeric
dev_local_mem_size                 numeric
dev_local_mem_type                 numeric
dev_max_clock_frequency            numeric
dev_max_compute_units              numeric
dev_max_constant_args              numeric
dev_max_constant_buffer_size       numeric
dev_max_mem_alloc_size             numeric
dev_max_parameter_size             numeric
dev_max_read_image_args            numeric
dev_max_samplers                   numeric
dev_max_work_group_size            numeric
dev_max_work_item_dimensions       numeric
dev_max_work_item_sizes_0          numeric
dev_max_work_item_sizes_1          numeric
dev_max_work_item_sizes_2          numeric
dev_max_write_image_args           numeric
dev_mem_base_addr_align            numeric
dev_min_data_type_align_size       numeric
dev_native_vector_width_char       numeric
dev_native_vector_width_double     numeric
dev_native_vector_width_float      numeric
dev_native_vector_width_half       numeric
dev_native_vector_width_int        numeric
dev_native_vector_width_long       numeric
dev_native_vector_width_short      numeric
dev_preferred_vector_width_char    numeric
dev_preferred_vector_width_double  numeric
dev_preferred_vector_width_float   numeric
dev_preferred_vector_width_half    numeric
dev_preferred_vector_width_int     numeric
dev_preferred_vector_width_long    numeric
dev_preferred_vector_width_short   numeric
dev_queue_properties               numeric
dev_single_fp_config               numeric
dev_type                           numeric
dev_vendor                         nominal
dev_vendor_id                      nominal
dev_version                        nominal
\end{Verbatim}
\end{multicols}


\section{Collective Tuning and the Client-Server Model}

\TODO{Three-tier client, proxy, server model. Include UML sequence
  diagrams and system diagram.}

\TODO{The implementation of this autotuner uses a three-tier
  client-server model. A master server stores the labelled training
  data in a common location. Then, for each SkelCL-capable machine, a
  system-level daemon hosts a DBus session bus which SkelCL processes
  can communicate with to request workgroup sizes. On launch, this
  daemon requests the latest training data from the master
  server. When a SkelCL stencil is invoked, it synchronously calls the
  \texttt{RequestWgsize()} method of the autotuner daemon, passing as
  arguments the required data in order to assemble a feature
  vector. Feature extraction then occurs within the daemon, which
  classifies the datapoint and returns the suggested workgroup size to
  the SkelCL process. This is a very low latency operation, and the
  system daemon can handle multiple connections from separate SkelCL
  processes simultaneously (although this is an admittedly unlikely
  use-case given that most GPGPU programs expect to be run in
  isolation).}

\subsection{Decoupling Autotuner and Client Programs}

\begin{lstlisting}[
  language=C++,
  label=lst:omnitune-client,
  caption={
    Implementation of client interface to OmniTune proxy.
  }
]
void requestWgSize(const std::string &deviceName,
                   const int deviceCount,
                   const skelcl::StencilShape &shape,
                   const int dataWidth,
                   const int dataHeight,
                   const std::string &Tin,
                   const std::string &Tout,
                   const std::string &source,
                   const size_t maxWgSize,
                   cl_uint *const local) {
  init(); // Initialise DBus connection and proxy objects.

  std::vector<Glib::VariantBase> _args;
  _args.push_back(Glib::Variant<std::string>::create(deviceName));
  _args.push_back(Glib::Variant<int>::create(deviceCount));
  ... // pack additional input arguments
  Glib::VariantContainerBase args
      = Glib::VariantContainerBase::create_tuple(_args);

  // Synchronously get parameter values.
  Glib::VariantContainerBase response;
  if (training)
      response = proxy->call_sync("RequestTrainingWgsize", args);
  else
      response = proxy->call_sync("RequestWgsize", args);
  Glib::VariantIter iterator(response.get_child(0));
  Glib::Variant<int16_t> var;

  // Set workgroup size.
  iterator.next_value(var);
  local[0] = var.get();
  iterator.next_value(var);
  local[1] = var.get();
}
\end{lstlisting}


\begin{lstlisting}[
  language=Python,
  label=lst:omnitune-proxy,
  caption={
    Implementation of proxy interface for receiving requests.
  }
]
class SkelCLProxy(omnitune.Proxy):
    ...

    @dbus.service.method(INTERFACE_NAME, in_signature='siiiiiiiisss',
                         out_signature='(nn)')
    def RequestWgsize(self, device_name, device_count,
                                     north, south, east, west, data_width,
                                     data_height, type_in, type_out, source,
                                     max_wgsize):
        ...

    @dbus.service.method(INTERFACE_NAME, in_signature='siiiiiiiisss',
                         out_signature='(nn)')
    def RequestTrainingWgsize(self, device_name, device_count,
                              north, south, east, west, data_width,
                              data_height, type_in, type_out, source,
                              max_wgsize):
        ...
\end{lstlisting}

\section{Autotuning using Classification}

\TODO{Once we have features, we can create a dataset from which will
  can train a machine learning classifier. We label each feature
  vector with the size of the workgroup that gave the best
  performance.}

\todo{We now insert an autotuner into SkelCL which performs runtime
  feature extraction and classification before every stencil
  invocation.}

The simplest autotuner is one which selects the workgroup size which
is most commonly optimal (i.e. the mode of all optimal workgroup sizes
$\left\{ \Omega(s) | s \in S \right\}$):

\begin{equation}
\text{ZeroR} = \text{mode}( \left\{ \Omega(s) | s \in S \right\} )
\end{equation}

However, this is not satisfactory for the purpose of selecting a
workgroup size to use, due to the constraint $W_{max}(s)$ enforced for
each scenario. As a result, we propose a so called ``OneR''
classifier, which first defines $W_{safe} \in W$ as the intersection
of legal workgroup sizes across all scenarios, then selects the
workgroup size $w$ which maximises the average performance
$\bar{p}(w)$ across this reduced parameter space.

\begin{align}
W_{safe} &= \cap \left\{ f(s) | s \in S \right\} \\
\text{OneR} &= \argmax_{w \in W_{safe}} \bar{p}(w)
\end{align}

This provides a baseline for comparing against a more sophisticated
autotuning approaches using machine learning.

\subsection{Satisfying Constraints}

Since classifiers are probabilistic systems, it is possible that a
classifier will predict a workgroup size that is invalid for the given
scenario, $w \not\in W_{legal}(s)$. In these cases, one of three
fallback strategies is used to select a safe workgroup size:

\begin{enumerate}
\item \emph{OneR} --- select the workgroup size which is known to be
  safe and provides the highest average case performance.
\item \emph{Random} --- select a random workgroup size uniformly from
  the set of legal values $w \in W_{legal}$.
\item \emph{Reshape} --- attempt to scale predicted the predicted
  workgroup size proportionally so that it fits within the space of
  legal workgroup sizes.
\end{enumerate}

The evaluation compares the average performance achieved using each
fallback strategy, along with the percentage of cases for which these
fallback strategies were required.

\begin{algorithm}
\caption{Select optimal workgroup size using classification}
\label{alg:autotune-classification}
\begin{algorithmic}[1]
\Require kernel features $k$, hardware features $h$, dataset features
$d$, error handler \textit{err\_fn}.
\Ensure workgroup size $w$

\State $w \leftarrow \text{classify}(k, h, d)$

\If{$\text{size}(w) \le k.\text{max\_wgsize}$}
    \State \textbf{return} $w$
\Else
    \State \textbf{return} \textit{err\_fn}$(w, k, h, d)$
\EndIf
\item[]

\Procedure{Random}{w, k, h, d}
  \State $C \leftarrow \{c_1, c_2, \ldots, c_n \}$
  \Comment Set of all workgroup sizes $\le k.\text{max\_wgsize}$
  \State \textbf{return} random choice $w \in C$
\EndProcedure
\item[]

\Procedure{Default}{w, k, h, d}
  \State $C \leftarrow \{c_1, c_2, \ldots, c_n \}$
  \Comment Set of all workgroup sizes $\le k.\text{max\_wgsize}$
  \State \textbf{return} $\underset{c \in C}{\argmax} f(k,h,d,c)$
\EndProcedure
\item[]

\Procedure{Reshape}{w, k, h, d}
  \State \textbf{return} $w / |k.\text{max\_wgsize}|$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Autotuning using Regression}

\TODO{If we could use regression to accurately predict the runtime or
  relative performance of different workgroup sizes, then we could
  overcome the expensive training cost of autotuning by
  classification.}

\subsection{Classification Using Regression}

Algorithms~\ref{alg:autotune-classification} and~\ref{alg:autotune-regression}.

\begin{algorithm}
\caption{Select optimal workgroup size using runtime regression}
\label{alg:autotune-classification}
\begin{algorithmic}[1]
\Require kernel features $k$, hardware features $h$, dataset features
$d$.
\Ensure workgroup size $w$
\State $C \leftarrow \{c_1, c_2, \ldots, c_n \}$
\Comment Set of all workgroup sizes $\le k.\text{max\_wgsize}$
\State \textbf{return} $\underset{c \in C}{\argmin} f(k,h,d,c)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Select optimal workgroup size using speedup regression}
\label{alg:autotune-regression}
\begin{algorithmic}[1]
\Require kernel features $k$, hardware features $h$, dataset features
$d$.
\Ensure workgroup size $w$
\State $C \leftarrow \{c_1, c_2, \ldots, c_n \}$
\Comment Set of all workgroup sizes $\le k.\text{max\_wgsize}$
\State \textbf{return} $\underset{c \in C}{\argmax} f(k,h,d,c)$
\end{algorithmic}
\end{algorithm}


\section{Meta-tuning: a Hybrid Approach}

\begin{algorithm}
\caption{Select optimal workgroup size using hybrid classification and regression}
\label{alg:autotune-hybrid}
\begin{algorithmic}[1]
\Require kernel features $k$, hardware features $h$, dataset features $d$.
\Ensure workgroup size $c$

\State $C \leftarrow \{ c_1, c_2,\ldots, c_n \}$
\Comment Set of all workgroup sizes $\le k.\text{max\_wgsize}$

\If{no control flow in kernel}
    \State \textbf{return} $\underset{c}{\argmin} f(k,h,d,c) = r$
\Else
   \State converged $\leftarrow$ false
   \State $c_b \leftarrow$ baseline values
   \State $r_b \leftarrow$ measure runtime of runtime of program with $c_b$
   \While{not converged}
     \State return $\underset{c}{\argmax} g(k,h,d,c) = s$
     \State $r \leftarrow$ evaluate $c$
     \If{measured speedup $\frac{r_b}{r} \approx$ predicted speedup $s$}
       \State converged = true
     \Else
       \State $C = C - \{c\}$
     \EndIf
   \EndWhile
   \State \textbf{return} $c$
\EndIf
\end{algorithmic}
\end{algorithm}

\section{Summary}
