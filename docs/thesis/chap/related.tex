This chapter begins with a brief survey of the broad field of
literature that is relevant to Algorithmic Skeletons. This is followed
by a review of the current state of the art in autotuning research,
focusing on heterogeneous parallelism, Algorithmic Skeletons, and
stencil codes. It presents the context and rationale for the research
undertaken for this thesis.


\section{Automating Parallelism}

It is widely accepted that parallel programming is difficult, and the
continued repetition of this claim has become something of a trite
mantra for the parallelism research community. An interesting
digression is to discuss some of the ways in which researchers have
attempted to tackle this difficult problem, and why, despite years of
research, it remains an ongoing challenge.

The most ambitious and perhaps daring field of parallelism research is
that of automatic parallelisation, where the goal is to develop
methods and systems to transform arbitrary sequential code into
parallelised code. This is a well studied subject, with the typical
approach being to perform these code transformations at the
compilation stage. In \citeauthor{Banerjee1993}'s thorough
review~\cite{Banerjee1993} on the subject, they outline the key
challenges of automatic parallelisation:
%
\begin{itemize}
\item determining whether sequential code can be legally transformed
  for parallel execution; and
\item identifying the transformation which will provide the highest
  performance improvement for a given piece of code.
\end{itemize}
%
Both of these challenges are extremely hard to tackle. For the former,
the difficulties lie in performing accurate analysis of code
behaviour. Obtaining accurate dynamic dependency analysis at compile
time is an unsolved problem, as is resolving pointers and points-to
analysis~\cite{Atkin-granville2013, Hind2001,Ghiya2001}.

The result of these challenges is that reliably performant, automatic
parallelisation of arbitrary programs remains a far from reached goal;
however, there are many note worthy variations on the theme which have
been able to achieve some measure of success.

One such example is speculative parallelism, which circumvents the
issue of having incomplete dependency information by speculatively
executing code regions in parallel while performing dependency tests
at runtime, with the possibility to fall back to ``safe'' sequential
execution if correctness guarantees are not
met~\cite{Prabhu2010,Trachsel2010}.  In~\cite{Jimborean2014},
\citeauthor{Jimborean2014} present a system which combines polyhedral
transformations of user code with binary algorithmic skeleton
implementations for speculative parallelisation, reporting speedups
over sequential code of up to $15.62\times$ on a 24 core processor.

Another example is PetaBricks, which is a language and compiler
enabling parallelism through ``algorithmic choice''~\cite{Ansel2009,
  Ansel2010}. With PetaBricks, users provide multiple implementations
of algorithms, optimised for different parameters or use cases. This
creates a search space of possible execution paths for a given
program. This has been combined with autotuning techniques for
enabling optimised multigrid programs~\cite{Chan2009}, with the wider
ambition that these autotuning techniques may be applied to all
algorithmic choice programs~\cite{Ansel2014}. While this helps produce
efficient parallel programs, it places a great burden on the
developer, requiring them to provide enough contrasting
implementations to make a search of the optimisation space fruitful.

Annotation-driven parallelism takes a similar approach. The user
annotates their code to provide ``hints'' to the compiler, which can
then perform parallelising transformations. A popular example of this
is OpenMP, which uses compiler pragmas to mark code sections for
parallel or vectorised execution~\cite{Dagum1998}. Previous work has
demonstrated code generators for translating OpenMP to
OpenCL~\cite{Grewe2013} and CUDA~\cite{Lee2009}. Again,
annotation-driven parallelism suffers from placing a burden on the
developer to identify the potential areas for parallelism, and lacks
the structure that algorithmic skeletons provide.

Algorithmic skeletons contrast the goals of automatic parallelisation
by removing the challenge of identifying potential parallelism from
compilers or users, instead allowing users to frame their problems in
terms of well defined patterns of computation. This places the
responsibility of providing performant, optimised implementations for
these patterns on the skeleton author.


\section{Iterative Compilation}\label{sec:iterative-compilation}

Iterative compilation is the method of performance tuning in which a
program is compiled and profiled iteratively using different
configurations of optimisations in order to find the configuration
which maximises performance. First presented by \citeauthor{Bodin1998}
in \citeyear{Bodin1998}, iterative compilation has been demonstrated
to be a highly effective form of empirical performance tuning for
selecting compiler optimisations~\cite{Bodin1998}.

% SUPEROPTIMISERS:
%
% H. Massalin, “Superoptimizer -- A Look at the Smallest Program,” ACM
% SIGPLAN Not., vol. 22, no. 10, pp. 122–126, 1987.
%
% R. Joshi, G. Nelson, and K. Randall, “Denali: a goal-directed
% superoptimizer,” ACM SIGPLAN Not., vol. 37, no. 5, p. 304, 2002.
\TODO{%
  Super-optimisers: demonstrably~\cite{Massalin1987} or
  provably~\cite{Joshi2002} optimal, but unbelievably costly. Can ML
  help?%
}

Given the huge number of possible compiler optimisations (as great as
$10^{200}$ in modern optimising compilers), it is often unfeasible to
perform an exhaustive search of the entire optimisation space, leading
to the development methods for reducing the cost of evaluating
configurations. These methods reduce evaluation costs either by
shrinking the size or dimensionality of the optimisation space, or by
using a directed search to traverse a subset of the space.

Machine learning has been successful applied to this problem by
predicting

Meta Optimisation~\cite{Stephenson2003}

% G. Fursin, Y. Kashnikov, A. W. Memon, Z. Chamski, O. Temam,
% M. Namolaru, E. Yom-Tov, B. Mendelson, A. Zaks, E. Courtois,
% F. Bodin, P. Barnard, E. Ashton, E. Bonilla, J. Thomson,
% C. K. I. Williams, and M. O’Boyle, “Milepost GCC: Machine Learning
% Enabled Self-tuning Compiler,” Int. J. Parallel Program., vol. 39,
% no. 3, pp. 296–327, Jan. 2011.
Milepost GCC is a machine learning enabled self-tuning compiler which
uses ~\cite{Fursin2011}.

Static analysis combined with iterative compilation~\cite{Runciman2014}.

A recent survey by \citeauthor{Burke2013} surveys the state of the
start in said hyper-heuristics, concluding that the generation of
heuristics XXX generalisation~\cite{Burke2013}.

%
% TOWARDS ONLINE TUNING:
%

% Ansel, J., & Reilly, U. O. (2012). SiblingRivalry: Online Autotuning
% Through Local Competitions. In Proceedings of the 2012 International
% Conference on Compilers, Architectures and Synthesis for Embedded
% Systems (pp. 91–100). ACM. doi:10.1145/2380403.2380425
\TODO{\cite{Ansel2012}}

% Eastep, J., Wingate, D., & Agarwal, A. (2011). Smart Data
% Structures: An Online Machine Learning Approach to Multicore Data
% Structures. In Proceedings of the 8th ACM International Conference
% on Autonomic Computing (pp. 11–20). New York, NY, USA:
% ACM. doi:10.1145/1998582.1998587
\TODO{Online reinforcement learning for optimising data structures
  online, \cite{Tesauro2005}}

% Tesauro, G. (2005). Online Resource Allocation Using Decompositional
% Reinforcement Learning. In AAAI (pp. 886–891).
\TODO{Reinforcement learning for resource allocation~\cite{Eastep2011}}

% W. F. Ogilvie, P. Petoumenos, Z. Wang, and H. Leather, “Intelligent
% Heuristic Construction with Active Learning,” in 18th International
% Workshop on Compilers for Parallel Computing, 2015.
\TODO{Using Active Learning to speed up the learning of compiler
  heuristics~\cite{Ogilvie2015}. Towards online auto-tuning, albeit
  only with binary optimisation parameter.}

%
% SOME EXAMPLES OF ML IN THE WILD:
%

% R. Bitirgen, E. Ipek, and J. F. Martinez, “Coordinated Management of
% Multiple Interacting Resources in Chip Multiprocessors: A Machine
% Learning Approach,” in 2008 41st IEEE/ACM International Symposium on
% Microarchitecture, 2008, pp. 318–329.
\TODO{Artificial Neural Networks for resource allocation of CMPS:
\cite{Bitirgen2008}}

% Z. Wang and M. F. P. O. Boyle, “Partitioning Streaming Parallelism
% for Multi-cores: A Machine Learning Based Approach,” in Proceedings
% of the 19th international conference on Parallel architectures and
% compilation techniques, 2010, pp. 307–318.
\TODO{Offline ML for partitioning streaming applications:
\cite{Wang2010}}


\section{Performance Tuning for Heterogeneous Parallelism}

As briefly discussed in Section~\ref{sec:gpgpu}, the complex
interactions between optimisations and heterogeneous hardware makes
performance tuning a difficult task. Performant GPGPU programs require
careful attention from the developer to properly manage data layout in
DRAM, caching, diverging control flow, and thread communication. The
performance of programs depends heavily on fully utilising
zero-overhead thread scheduling, memory bandwidth, and thread
grouping. \citeauthor{Ryoo2008a} illustrate the importance of these
factors by demonstrating speedups of up to $432\times$ for matrix
multiplication in CUDA by appropriate use of tiling and loop
unrolling~\cite{Ryoo2008a}. The importance of proper exploitation of
local shared memory and synchronisation costs is explored
in~\cite{Lee2010}.

In~\cite{Chen2014}, data locality optimisations are automated using a
description of the hardware and a memory-placement-agnostic
compiler. The authors demonstrate impressive speedups of up to
$2.08\times$, although at the cost of requiring accurate memory
hierarchy descriptor files for all targeted hardware. The descriptor
files must be hand generated, requiring expert knowledge of the
underlying hardware in order to properly exploit memory locality.

Data locality for nested parallel patterns is explored in~\cite{Lee}.
The authors use an automatic mapping strategy for nested parallel
skeletons on GPUs, which uses a custom intermediate representation and
a CUDA code generator, achieving $1.24\times$ speedup over hand
optimised code on 7 of 8 Rodinia benchmarks.

Reduction of the xGPGPU optimisation space is demonstrated
in~\cite{Ryoo2008}, using the common subset of optimal configurations
across a set of training examples. This technique reduces the search
space by 98\%, although it does not guarantee that for a new program,
the search space will include the optimal configuration.

\citeauthor{Magni2014} demonstrated that thread coarsening of OpenCL
kernels can lead to speedups in program performance between
$1.11\times$ and $1.33\times$ in~\cite{Magni2014}. The authors achieve
this using a machine learning model to predict optimal thread
coarsening factors based on the static features of kernels, and an
LLVM function pass to perform the required code transformations.

Automatic generation of OpenCL kernels from high-level programming
concepts is explored in~\cite{Steuwer2015}. A set of rewrite rules is
used to transform high-level expressions to OpenCL code, creating a
space of possible implementations. The authors report performance on a
par with that of hand written OpenCL kernels. The optimisation of low
level implementations of high-level expressions presents similar
challenges to that of autotuning algorithmic skeletons.


\section{Autotuning Algorithmic Skeletons}

An enumeration of the optimisation space of Intel Thread Building
Blocks in~\cite{Contreras2008} shows that runtime knowledge of the
available parallel hardware can have a significant impact on program
performance. \citeauthor{Collins2012} exploit this
in~\cite{Collins2012}, first using Principle Components Analysis to
reduce the dimensionality of the space of possible optimisation
parameters, followed by a search of parameter values to optimise
program performance by a factor of $1.6\times$ over values chosen by a
human expert. In~\cite{Collins2013}, they extend this using static
feature extraction and nearest neighbour classification to further
prune the search space, achieving an average 89\% of the oracle
performance after evaluating 45 parameters.

\citeauthor{Dastgeer2011} developed a machine learning based autotuner
for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data
is used to predict the optimal execution device (i.e.\ CPU, GPU) for a
given program by predicting execution time and memory copy overhead
based on problem size. The autotuner only supports vector operations,
and there is limited cross-architecture
evaluation. In~\cite{Dastgeer2015a}, the authors extend SkePU to
improve the data consistency and transfer overhead of container types,
reporting up to a $33.4\times$ speedup over the previous
implementation.


\section{Code Generation and Autotuning for Stencils}

Stencil codes have a variety of computationally expensive uses from
fluid dynamics to quantum mechanics. Efficient, tuned stencil kernels
are highly sought after, with early work in \citeyear{Bolz2003} by
\citeauthor{Bolz2003} demonstrating the capability of GPUs for
massively parallel stencil operations~\cite{Bolz2003}. In the
resulting years, stencil codes have received much attention from the
performance tuning research community.

\citeauthor{Ganapathi2009} demonstrated early attempts at autotuning
multicore stencil codes in~\cite{Ganapathi2009}, drawing upon the
successes of statistical machine learning techniques in the compiler
community, as discussed in
Section~\ref{sec:iterative-compilation}. They present an autotuner
which can achieve performance within 1\% or up to 18\% better than
that of a human expert. From a space of 10 million configurations,
they evaluate the performance of a randomly selected 1500
combinations, using Kernel Canonical Correlation Analysis to build
correlations between tunable parameter values and measured performance
targets. Performance targets are L1 cache misses, TLB misses, cycles
per thread, and power consumption. The use of KCAA restricts the
scalability of their system as the complexity of model building grows
exponentially with the number of features. In their evaluation, the
system requires two hours of compute time to build the KCAA model for
only 400 seconds of benchmark data. They present a compelling argument
for the use of energy efficiency as an optimisation target in addition
to runtime, citing that it was the power wall that lead to the
multicore revolution in the first place. Their choice of only 2
benchmarks and 2 platforms makes the evaluation of their autotuner
somewhat limited.

\citeauthor{Berkeley2009} targeted 3D stencils code performance
in~\cite{Berkeley2009}. Stencils are decomposed into core blocks,
sufficiently small to avoid last level cache capacity misses. These
are then further decomposed to thread blocks, designed to exploit
common locality threads may have within a shared cache or local
memory. Thread blocks are divided into register blocks in order to
take advantage of data level parallelism provided by the available
registers. Data allocation is optimised on NUMA systems. The
performance evaluation considers speedups of various optimisations
with and without consideration for host/device transfer overhead.

\citeauthor{Kamil2010} presents an autotuning framework
in~\cite{Kamil2010} which accepts as input a Fortran 95 stencil
expression and generates tuned shared-memory parallel implementations
in Fortan, C, or CUDA. The system uses an IR to explore autotuning
transformations, enumerating a subset of the optimisation space and
recording only a single execution time for each configuration,
reporting the fastest. They demonstrate their system on 4
architectures using 3 benchmarks, with speedups of up to $22\times$
compared to serial implementations. The CUDA code generator does not
optimise for the GPU memory hierarchy, using only global memory. As
demonstrated in this thesis, improper utilisation of local memory can
hinder program performance by two orders of magnitude. There is no
directed search or cross-program learning.

In~\cite{Zhang2013a}, \citeauthor{Zhang2013a} present a code generator
and autotuner for 3D Jacobi stencil codes. Using a DSL to express
kernel functions, the code generator performs substitution from one of
two CUDA templates to create programs for execution on GPUs. GPU
programs are parameterised and tuned for block size, block dimensions,
and whether input data is stored in read only texture memory. This
creates an optimisation space of up to 200 configurations. In an
evaluation of 4 benchmarks, the authors report impressive performance
that is comparable with previous implementations of iterative Jacobi
stencils on GPUs~\cite{Holewinski2012, Phillips2010}. The dominating
parameter is shown to be block dimensions, followed by block size,
then read only memory. The DSL presented in the paper is limited to
expressing only Jacobi Stencils applications. Critically, their
autotuner requires a full enumeration of the parameter space for each
program. Since there is no indication of the compute time required to
gather this data, it gives the impression that the system would be
impractical for the needs of general purpose stencil computing. The
autotuner presented in this thesis overcomes this drawback by learning
parameter values which transfer to unseen stencils, without the need
for an expensive tuning phase for each program and architecture.
% TODO: Depending on results of cross-architecture validation, this
% last claim may not hold up.
%
% The majority of applications tested are memory bound. Does this
% transfer to computer bound?

In~\cite{Christen2011}, \citeauthor{Christen2011} presents a DSL for
expressing stencil codes, a C code generator, and an autotuner for
exploring the optimisation space of blocking and vectorisation
strategies. \TODO{Pro: Supports arbitrarily high dimensional
  grids. They introduce *2* new DSLs without comment on why they were
  needed. This is a huge price of entry for anyone who actually wants
  to *use* PATUS to solve problems, and a decision that I think they
  could have justified better. They provide nice and concise
  explanations of stencil codes and the types of optimisation methods
  used by other autotuners (ATLAS, FLAME, FFTW, SPIRAL). However, they
  *barely* explain their own, saying only that they perform *either*
  an exhaustive, multi-run Powell, Nelder Mead, or evolutionary
  algorithms.  Their evaluation uses 6 benchmarks, and 3
  architectures. Only 1 of those is a GPU. It would have been nice to
  shown performance across GPU architectures. From the perspective of
  autotuning, the paper comes as across quite weak: They do not
  present an "oracle" performance, so we can't compare the quality of
  their autotuner compared to it. They don't show how the optimal
  tunable parameter values vary across results, so they don't
  demonstrate how autotuning is *necessary* (maybe there's one single
  value which works well for all results?). They do not give any
  indication of convergence time of their autotuner. They do not
  report the number of different combinations that their autotuner
  tries.}

A stencil grid can be decomposed into smaller subsections so that
multiple GPUs can operate on each subsection independently. This
requires a small overlapping region where each subsection meets ---
the halo region --- to be shared between devices. For iterative
stencils, values in the halo region must be synchronised between
devices after each iteration, leading to costly communication
overheads. One possible optimisation is to deliberately increase the
size of the halo region, allowing each device to compute updated
values for the halo region, instead of requiring a synchronisation of
shared state. This reduces the communication costs between GPUs, at
the expense of introducing redundant computation. Tuning the size of
this halo region is the goal of PARTANS~\cite{Lutz2013}, an autotuning
framework for multi-GPU stencil computations. \citeauthor{Lutz2013}
explore the effect of varying the size of the halo regions using six
benchmark applications, finding that the optimal halo size depends on
the size of the grid, the number of partitions, and the connection
mechanism (i.e.\ PCI express). The authors present an autotuner which
determines problem decomposition and swapping strategy offline, and
performs an online search for the optimal halo size. The selection of
overlapping halo region size compliments the selection of workgroup
size which is the subject of this thesis. However, PARTANS uses a
custom DSL rather than a the generic interface provided by SkelCL, and
PARTANS does not learn the results of tuning across similar programs,
or across multiple runs of the same program.


\section{Summary}

There is already a wealth of research literature on the topic
autotuning which begs the question, why isn't the majority of software
autotuned? This chapter has attempted to answer this question by
reviewing the state of the art in the autotuning literature, with
specific reference to auotuning for GPUs and stencil codes. The bulk
of this research falls prey of one of two shortcomings. Either they
identify and develop a methodology for tuning a particular
optimisation space but then fail to develop a system which can
properly exploit this (for example, by using machine learning to
predict optimal values across programs), or they present an autotuner
which targets too specific of a class of optimisations to be widely
applicable. This projects attempts to address both of those
shortcomings by expending great effort to deliver a working
implementation which users can download and use without any setup
costs, and by providing a modular and extensible framework which
allows rapid targeting of new autotuning platforms, enabled by a
shared autotuning logic and distributed training data. The following
chapter outlines the design of this system.
