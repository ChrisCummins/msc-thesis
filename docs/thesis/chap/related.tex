% COMPETITORS
\section{Performance Tuning for Heterogeneous Parallelism}

% Ryoo, S., Rodrigues, C. I., Stone, S. S., Baghsorkhi, S. S., Ueng,
% S.-Z., Stratton, J. a., & Hwu, W. W. (2008). Program optimization
% space pruning for a multithreaded GPU. In Proceedings of the 6th
% annual IEEE/ACM international symposium on Code generation and
% optimization (pp. 195–204). New York, New York, USA: ACM
% Press. doi:10.1145/1356058.1356084
\TODO{\cite{Ryoo2008}}

% G. Chen and B. Wu, “PORPLE: An Extensible Optimizer for Portable
% Data Placement on GPU,” in Microarchitecture (MICRO), 2014 47th
% Annual IEEE/ACM International Symposium on, 2014, pp. 88–100.
\TODO{Optimising data placement on GPUs using a description of the
hardware and a memory-placement-agnostic compiler. Shows impressive
speedups (2.08x max, 1.59x avg) from just optimising data
placement. \cite{Chen2014}}

% Lee, H., Brown, K. J., Sujeeth, A. K., Rompf, T., & Olukotun,
% K. (2014). Locality-Aware Mapping of Nested Parallel Patterns on
% GPUs. In Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM
% International Symposium on
% (pp. 63–74). IEEE. doi:10.1109/MICRO.2014.23
\TODO{GPU benchmarking: \cite{Lee}}

% A. Magni, C. Dubach, and M. O’Boyle, “Automatic optimization of
% thread-coarsening for graphics processors,” in International
% Conference on Parallel Architectures and Compilation, 2014,
% pp. 455–466.
\citeauthor{Magni2014} explored the effect of thread coarsening
in~\cite{Magni2014}, achieving average speedups between 1.11$\times$
and 1.33$\times$ using a machine-learning model based on the static
features of kernels.

% Steuwer, M., Fensch, C., & Dubach, C. (2015). Patterns and Rewrite
% Rules for Systematic Code Generation From High-Level Functional
% Patterns to High-Performance OpenCL Code. arXiv Preprint
% arXiv:1502.02389.
\TODO{Using re-write rules to translate high-level programs to OpenCL:
\cite{Steuwer2015}}


\section{Autotuning of Algorithmic Skeletons}

% Contreras, G., & Martonosi, M. (2008). Characterizing and improving
% the performance of Intel Threading Building Blocks. In Workload
% Characterization, 2008. IISWC 2008. IEEE International Symposium on
% (pp. 57–66). IEEE. doi:10.1109/IISWC.2008.4636091
\TODO{INTEL TBB \cite{Contreras2008}}

% Collins, A., Fensch, C., & Leather, H. (2012). Auto-Tuning Parallel
% Skeletons. Parallel Processing Letters, 22(02),
% 1240005. doi:10.1142/S0129626412400051
%
% Collins, A., Fensch, C., Leather, H., & Cole, M. (2013). MaSiF:
% Machine Learning Guided Auto-tuning of Parallel Skeletons. 20th
% Annual International Conference on High Performance Computing -
% HiPC, 186–195. doi:10.1109/HiPC.2013.6799098
\paragraph{MaSiF} \TODO{\cite{Collins2012}, \cite{Collins2013}}


% Dastgeer, U., Enmyren, J., & Kessler, C. W. (2011). Auto-tuning
% SkePU: a multi-backend skeleton programming framework for multi-GPU
% systems. In Proceedings of the 4th International Workshop on
% Multicore Software Engineering (pp. 25–32). ACM. Retrieved from
% http://dl.acm.org/citation.cfm?id=1984697
\citeauthor{Dastgeer2011} developed a machine learning based autotuner
for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data
is used to select the optimal backend (i.e.\ CPU, GPU) for a given
program by estimating execution time and memory copy overhead based on
problem size. There is limited cross-architecture evaluation, and the
autotuner only supports vector operations.

% Dastgeer, U., & Kessler, C. (2015). Smart Containers and Skeleton
% Programming for GPU-Based Systems. International Journal of Parallel
% Programming, 1–25. doi:10.1007/s10766-015-0357-6
\TODO{Smart containers for SkePU:~\cite{Dastgeer2015a}}


\section{Code Generation and Autotuning of Stencil Codes}

% Y. Zhang and F. Mueller, “Auto-generation and Auto-tuning of 3D
% Stencil Codes on GPU clusters,” in Proceedings of the Tenth
% International Symposium on Code Generation and Optimization, 2012,
% pp. 155–164.
In~\cite{Zhang2013a}, \citeauthor{Zhang2013a} present a code generator
and autotuner for 3D Jacobi stencil codes. Using a DSL to express
kernel functions, the code generator performs substitution from one of
two CUDA templates to create GPU programs. These programs are
parameterised and tuned for block size, block dimensions, and whether
input data is stored in read only texture memory. This creates a
search space of up to 200 configurations.

The authors report impressive performance, comparable with previous
implementations of iterative Jacobi stencils on
GPUs~\cite{Holewinski2012,Phillips2010}. The dominating parameter is
shown to be block dimensions, followed by block size, then read only
memory. An unfortunate limitation of their system is that they do not
support multi-GPU execution within a single node, instead using
message passing in a cluster. This rules out the possible benefits of
performing loop compaction for iterative stencils, an optimisation
which has shown to be beneficial for multi-GPU performance
in~\cite{Lutz2013}.

The DSL presented in the paper is limited to expressing only Jacobi
Stencils applications. Critically, their autotuner requires a full
enumeration of the parameter space for each program, with no element
of search space reduction or cross-program learning. Since there is no
evidence of the compute time required to perform this enumeration, it
gives the impression that the system would be impractical for the
needs of general purpose Stencil
computing.
% The majority of applications tested are memory bound. Does this
% transfer to computer bound?

In~\cite{Ganapathi2009}, \citeauthor{Ganapathi2009} argues for
applying statistical machine learning (SML) techniques to develop
autotuners for multicore software. They present an autotuner for
Stencil codes which can achieve performance within 1\% or up to 18\%
better than that of a human expert after 2 hours of running. They
evaluate the performance of a randomly selected 1500 configurations
(from a posible 10 million configs), and use Kernel Canonical
Correlation Analysis (KCCA) to build correlations between tunable
parameter values and measured performance values. Performance is
measured using hardware counters (L1 cache misses, TLB misses, cycles
per thread) and power consumtion in Watts/sec. KCCA seems like a
strange choice: it scales exponentially with the feature vector sizes,
and it takes 2 hours (!!!) to build the ML model for 400 sec worth of
benchmark data. They present an interesting argument that enegy
efficiency should be used as an autotuning target as well as just run
time, since it was the power wall that lead to the multicore
revolution in the first place. They explain the motivation and results
well. I like that they compare their results with human expert and
hardware upper bound. It is a solid paper which makes a compelling
argument, but their choice of only 2 benchmarks and 2 platforms makes
the evaluation of their autotuner a little limited.

\citeauthor{Kamil2010} presents an auto-tuning framework which accepts as input a
Fortran 95 stencil expression, and generates tuned parallel
implementations in Fortan, C, or CUDA. The system uses an IR to
explore auto-tuning transformations, and has an SMP backend code
generator. They demonstrate their system on 4 architectures using 3
benchmarks, with speedups of up to x22 over serial. The CUDA code
generator only uses global memory (!!). Also, there's no real search
engine. They randomly enumerate a subset of the optimisation space,
and then record only a single execution time (!!!), reporting the
fastest. Cited by 127.~\cite{Kamil2010}

\paragraph{PATUS} In~\cite{Christen2011}, \citeauthor{Christen2011}
presents a DSL for expressing stencil codes, a C code generator, and
an autotuner for exploring the optimisation space, using blocking and
vectorisation strategies. Pro: Supports arbitrarily high dimensional
grids. They introduce *2* new DSLs without comment on why they were
needed. This is a huge price of entry for anyone who actually wants to
*use* PATUS to solve problems, and a decision that I think they could
have justified better. They provide nice and concise explanations of
stencil codes and the types of optimisation methods used by other
autotuners (ATLAS, FLAME, FFTW, SPIRAL). However, they *barely*
explain their own, saying only that they perform *either* an
exhaustive, multi-run Powell, Nelder Mead, or evolutionary algorithms.
Their evaluation uses 6 benchmarks, and 3 architectures. Only 1 of
those is a GPU. It would have been nice to shown performance across
GPU architectures. From the perspective of autotuning, the paper comes
as across quite weak: They do not present an "oracle" performance, so
we can't compare the quality of their autotuner compared to it. They
don't show how the optimal tunable parameter values vary across
results, so they don't demonstrate how autotuning is *necessary*
(maybe there's one single value which works well for all
results?). They do not give any indication of convergence time of
their autotuner. They do not report the number of different
combinations that their autotuner tries.

% T. Lutz, C. Fensch, and M. Cole, “PARTANS: An Autotuning Framework
% for Stencil Computation on Multi-GPU Systems,” ACM
% Trans. Archit. Code Optim., vol. 9, no. 4, pp. 1–24, 2013.
\paragraph{PARTANS} \citeauthor{Lutz2013} explored the effect of
varying the size of the halo regions for multi-GPU stencil
computations in~\cite{Lutz2013}. A large halo increases redundant
computation across GPUs but decreases communication costs. They found
that the optimal halo size depends on the problem size, number of
partitions, and the connection mechanism (i.e.\ PCI express). They
developed an autotuner which determines problem decomposition and
swapping strategy offline, and performs an online search for the
optimal halo size. The study is limited to only three optimisation
parameters of a single class of program, and the results of tuning are
not shared across programs or across multiple runs of the same
program.
