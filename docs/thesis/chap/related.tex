This chapter begins with a brief survey of the broad field of
literature that is relevant to Algorithmic Skeletons. This is followed
by a review of the current state of the art in autotuning research,
focusing on heterogeneous parallelism, Algorithmic Skeletons, and
stencil codes. It presents the context and rationale for the research
undertaken for this thesis.


\section{Automating Parallelism}

It is widely accepted that parallel programming is difficult, and the
continued repetition of this claim has become something of a trite
mantra for the parallelism research community. An interesting
digression is to discuss some of the ways in which researchers have
attempted to tackle this difficult problem, and why, despite years of
research, it remains an ongoing challenge.


% \subsubsection{Automatically Parallelising Compilers}

The most ambitious and perhaps daring field of parallelism research is
that of automatic parallelisation, where the goal is to develop
methods and systems to transform arbitrary sequential code into
parallelised code. This is a well studied subject, with the typical
approach being to perform these code transformations at the
compilation stage. In \citeauthor{Banerjee1993}'s thorough
review~\cite{Banerjee1993} on the subject, they outline the key
challenges of automatic parallelisation:
%
\begin{itemize}
\item determining whether sequential code can be legally transformed
  for parallel execution; and
\item identifying the transformation which will provide the highest
  performance improvement for a given piece of code.
\end{itemize}
%
Both of these challenges are extremely hard to tackle. For the former,
the difficulties lie in performing accurate analysis of code
behaviour. Obtaining accurate dynamic dependency analysis at compile
time is an unsolved problem, as is resolving pointers and points-to
analysis~\cite{Atkin-granville2013, Hind2001,Ghiya2001}.

The result of these challenges is that reliably performant, automatic
parallelisation of arbitrary programs remains a far from reached goal;
however, there are many note worthy variations on the theme which have
been able to achieve some measure of success.

One such example is speculative parallelism, which circumvents the
issue of having incomplete dependency information by speculatively
executing code regions in parallel while performing dependency tests
at runtime, with the possibility to fall back to ``safe'' sequential
execution if correctness guarantees are not
met~\cite{Prabhu2010,Trachsel2010}.  In~\cite{Jimborean2014},
\citeauthor{Jimborean2014} present a system which combines polyhedral
transformations of user code with binary algorithmic skeleton
implementations for speculative parallelisation, reporting speedups
over sequential code of up to $15.62\times$ on a 24 core processor.

Another example is PetaBricks, which is a language and compiler
enabling parallelism through ``algorithmic choice''~\cite{Ansel2009,
  Ansel2010}. With PetaBricks, users provide multiple implementations
of algorithms, optimised for different parameters or use cases. This
creates a search space of possible execution paths for a given
program. This has been combined with autotuning techniques for
enabling optimised multigrid programs~\cite{Chan2009}, with the wider
ambition that these autotuning techniques may be applied to all
algorithmic choice programs~\cite{Ansel2014}. While this helps produce
efficient parallel programs, it places a great burden on the
developer, requiring them to provide enough contrasting
implementations to make a search of the optimisation space fruitful.

Annotation-driven parallelism takes a similar approach. The user
annotates their code to provide ``hints'' to the compiler, which can
then perform parallelising transformations. A popular example of this
is OpenMP, which uses compiler pragmas to mark code sections for
parallel or vectorised execution~\cite{Dagum1998}. Previous work has
demonstrated code generators for translating OpenMP to
OpenCL~\cite{Grewe2013} and CUDA~\cite{Lee2009}. Again,
annotation-driven parallelism suffers from placing a burden on the
developer to identify the potential areas for parallelism, and lacks
the structure that algorithmic skeletons provide.

Algorithmic skeletons contrast the goals of automatic parallelisation
by removing the challenge of identifying potential parallelism from
compilers or users, instead allowing users to frame their problems in
terms of well defined patterns of computation. This places the
responsibility of providing performant, optimised implementations for
these patterns on the skeleton author.


\section{Iterative Compilation}

% SUPEROPTIMISERS:
%
% H. Massalin, “Superoptimizer -- A Look at the Smallest Program,” ACM
% SIGPLAN Not., vol. 22, no. 10, pp. 122–126, 1987.
%
% R. Joshi, G. Nelson, and K. Randall, “Denali: a goal-directed
% superoptimizer,” ACM SIGPLAN Not., vol. 37, no. 5, p. 304, 2002.
\TODO{%
  Super-optimisers: demonstrably~\cite{Massalin1987} or
  provably~\cite{Joshi2002} optimal, but unbelievably costly. Can ML
  help?%
}

% MACHINE LEARNING

% M. Stephenson, M. Martin, and U. O. Reilly, “Meta Optimization:
% Improving Compiler Heuristics with Machine Learning,” ACM SIGPLAN
% Not., vol. 38, no. 5, pp. 77–90, 2003.
\TODO{Using ML to improve compiler heuristics~\cite{Stephenson2003}}

% J. M. C. Trilla and C. Runciman, “An Iterative Compiler for Implicit
% Parallelism,” in IFL, 2014, pp. 1–5.  IFL, 2014.
\cite{Runciman2014}

% E. K. Burke, M. Gendreau, M. Hyde, G. Kendall, G. Ochoa, E. Özcan,
% and R. Qu, “Hyper-heuristics: a survey of the state of the art,”
% J. Oper. Res. Soc., vol. 64, pp. 1695–1724, 2013.
\TODO{Hyper-heuristics survey paper: \cite{Burke2013}}

%
% SOME EXAMPLES OF ML IN THE WILD:
%

% R. Bitirgen, E. Ipek, and J. F. Martinez, “Coordinated Management of
% Multiple Interacting Resources in Chip Multiprocessors: A Machine
% Learning Approach,” in 2008 41st IEEE/ACM International Symposium on
% Microarchitecture, 2008, pp. 318–329.
\TODO{Artificial Neural Networks for resource allocation of CMPS:
\cite{Bitirgen2008}}

% G. Fursin, Y. Kashnikov, A. W. Memon, Z. Chamski, O. Temam,
% M. Namolaru, E. Yom-Tov, B. Mendelson, A. Zaks, E. Courtois,
% F. Bodin, P. Barnard, E. Ashton, E. Bonilla, J. Thomson,
% C. K. I. Williams, and M. O’Boyle, “Milepost GCC: Machine Learning
% Enabled Self-tuning Compiler,” Int. J. Parallel Program., vol. 39,
% no. 3, pp. 296–327, Jan. 2011.
\TODO{Milepost GCC is a machine learning enabled self-tuning
  compiler~\cite{Fursin2011}.}

% Z. Wang and M. F. P. O. Boyle, “Partitioning Streaming Parallelism
% for Multi-cores: A Machine Learning Based Approach,” in Proceedings
% of the 19th international conference on Parallel architectures and
% compilation techniques, 2010, pp. 307–318.
\TODO{Offline ML for partitioning streaming applications:
\cite{Wang2010}}

%
% TOWARDS ONLINE TUNING:
%

% Ansel, J., & Reilly, U. O. (2012). SiblingRivalry: Online Autotuning
% Through Local Competitions. In Proceedings of the 2012 International
% Conference on Compilers, Architectures and Synthesis for Embedded
% Systems (pp. 91–100). ACM. doi:10.1145/2380403.2380425
\TODO{\cite{Ansel2012}}

% Eastep, J., Wingate, D., & Agarwal, A. (2011). Smart Data
% Structures: An Online Machine Learning Approach to Multicore Data
% Structures. In Proceedings of the 8th ACM International Conference
% on Autonomic Computing (pp. 11–20). New York, NY, USA:
% ACM. doi:10.1145/1998582.1998587
\TODO{Online reinforcement learning for optimising data structures
  online, \cite{Tesauro2005}}

% Tesauro, G. (2005). Online Resource Allocation Using Decompositional
% Reinforcement Learning. In AAAI (pp. 886–891).
\TODO{Reinforcement learning for resource allocation~\cite{Eastep2011}}

% W. F. Ogilvie, P. Petoumenos, Z. Wang, and H. Leather, “Intelligent
% Heuristic Construction with Active Learning,” in 18th International
% Workshop on Compilers for Parallel Computing, 2015.
\TODO{Using Active Learning to speed up the learning of compiler
  heuristics~\cite{Ogilvie2015}. Towards online auto-tuning, albeit
  only with binary optimisation parameter.}


\section{Performance Tuning for Heterogeneous Parallelism}

Performance tuning of GPGPU programming is a nascent field,
\todo{\ldots}

\cite{Owens2006, Owens2008}

% S. Ryoo, C. I. Rodrigues, S. S. Baghsorkhi, S. S. Stone, D. B. Kirk,
% and W. W. Hwu, “Optimization principles and application performance
% evaluation of a multithreaded GPU using CUDA,” Proc. 13th ACM
% SIGPLAN Symp. Princ. Pract. parallel Program. - PPoPP ’08, p. 73,
% 2008.
\TODO{%
  The challenges and approaches to optimising GPU programs. CUDA
  programming requires the developer to explicitly manage data layout
  in DRAM, caching, thread communication and other
  resources. Performance of such programs depends heavily on fully
  utilizing zero-overhead thread scheduling, memory bandwidth, thread
  grouping, shared control flow, and intra-block thread
  communication. The paper gives an example of optimising matrix
  multiplication by utilising shared memory through tiling, and loop
  unrolling~\cite{Ryoo2008a}/%
}

Prior research has shown that the performance of a GPGPU program is
heavily influenced by proper exploitation of local shared memory and
synchronisation costs~\cite{Ryoo2008a, Lee2010} \TODO{``Where's the
  data?'' paper}

% Ryoo, S., Rodrigues, C. I., Stone, S. S., Baghsorkhi, S. S., Ueng,
% S.-Z., Stratton, J. a., & Hwu, W. W. (2008). Program optimization
% space pruning for a multithreaded GPU. In Proceedings of the 6th
% annual IEEE/ACM international symposium on Code generation and
% optimization (pp. 195–204). New York, New York, USA: ACM
% Press. doi:10.1145/1356058.1356084
\TODO{\cite{Ryoo2008}}

% G. Chen and B. Wu, “PORPLE: An Extensible Optimizer for Portable
% Data Placement on GPU,” in Microarchitecture (MICRO), 2014 47th
% Annual IEEE/ACM International Symposium on, 2014, pp. 88–100.
\TODO{Optimising data placement on GPUs using a description of the
hardware and a memory-placement-agnostic compiler. Shows impressive
speedups (2.08x max, 1.59x avg) from just optimising data
placement. \cite{Chen2014}}

% Lee, H., Brown, K. J., Sujeeth, A. K., Rompf, T., & Olukotun,
% K. (2014). Locality-Aware Mapping of Nested Parallel Patterns on
% GPUs. In Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM
% International Symposium on
% (pp. 63–74). IEEE. doi:10.1109/MICRO.2014.23
\TODO{GPU benchmarking: \cite{Lee}}

% A. Magni, C. Dubach, and M. O’Boyle, “Automatic optimization of
% thread-coarsening for graphics processors,” in International
% Conference on Parallel Architectures and Compilation, 2014,
% pp. 455–466.
\citeauthor{Magni2014} explored the effect of thread coarsening
in~\cite{Magni2014}, achieving average speedups between 1.11$\times$
and 1.33$\times$ using a machine-learning model based on the static
features of kernels.

% Steuwer, M., Fensch, C., & Dubach, C. (2015). Patterns and Rewrite
% Rules for Systematic Code Generation From High-Level Functional
% Patterns to High-Performance OpenCL Code. arXiv Preprint
% arXiv:1502.02389.
\TODO{Using re-write rules to translate high-level programs to OpenCL:
\cite{Steuwer2015}}


\section{Autotuning Algorithmic Skeletons}


% Gonz, H., & Leyton, M. (2010). A survey of algorithmic skeleton
% frameworks: high-level structured parallel programming enablers,
% 1135–1160. http://doi.org/10.1002/spe
Since their inception, Algorithmic Skeletons have captured a wide
range of research interests, and Algorithmic Skeleton Frameworks
(ASkFs) abound~\cite{Gonz2010}.


% Contreras, G., & Martonosi, M. (2008). Characterizing and improving
% the performance of Intel Threading Building Blocks. In Workload
% Characterization, 2008. IISWC 2008. IEEE International Symposium on
% (pp. 57–66). IEEE. doi:10.1109/IISWC.2008.4636091
\TODO{INTEL TBB \cite{Contreras2008}}

% Collins, A., Fensch, C., & Leather, H. (2012). Auto-Tuning Parallel
% Skeletons. Parallel Processing Letters, 22(02),
% 1240005. doi:10.1142/S0129626412400051
%
% Collins, A., Fensch, C., Leather, H., & Cole, M. (2013). MaSiF:
% Machine Learning Guided Auto-tuning of Parallel Skeletons. 20th
% Annual International Conference on High Performance Computing -
% HiPC, 186–195. doi:10.1109/HiPC.2013.6799098
\paragraph{MaSiF} \TODO{\cite{Collins2012}, \cite{Collins2013}}


% Dastgeer, U., Enmyren, J., & Kessler, C. W. (2011). Auto-tuning
% SkePU: a multi-backend skeleton programming framework for multi-GPU
% systems. In Proceedings of the 4th International Workshop on
% Multicore Software Engineering (pp. 25–32). ACM. Retrieved from
% http://dl.acm.org/citation.cfm?id=1984697
\citeauthor{Dastgeer2011} developed a machine learning based autotuner
for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data
is used to select the optimal backend (i.e.\ CPU, GPU) for a given
program by estimating execution time and memory copy overhead based on
problem size. There is limited cross-architecture evaluation, and the
autotuner only supports vector operations.

% Dastgeer, U., & Kessler, C. (2015). Smart Containers and Skeleton
% Programming for GPU-Based Systems. International Journal of Parallel
% Programming, 1–25. doi:10.1007/s10766-015-0357-6
\TODO{Smart containers for SkePU:~\cite{Dastgeer2015a}}


\section{Code Generation and Autotuning for Stencils}

Stencil codes have a variety of computationally expensive uses from
fluid dynamics to quantum mechanics. Efficient, tuned stencil kernels
are highly sought after, with early work in \citeyear{Bolz2003} by
\citeauthor{Bolz2003} demonstrating the capability of GPUs for
massively parallel stencil operations~\cite{Bolz2003}. In the
resulting years, stencil codes have received much attention from the
performance tuning research community.


% A. Ganapathi, K. Datta, A. Fox, and D. Patterson, “A Case for
% Machine Learning to Optimize Multicore Performance,” in First USENIX
% Workshop on Hot Topics in Parallelism (HotPar’09), 2009.
\citeauthor{Ganapathi2009} demonstrated early attempts at autotuning
multicore stencil codes in~\cite{Ganapathi2009}, drawing from the
success of statistical machine learning techniques in the compiler
community. They present an autotuner which can achieve performance
within 1\% or up to 18\% better than that of a human expert. From a
space of 10 million configurations, they evaluate the performance of a
randomly selected 1500 combinations, using Kernel Canonical
Correlation Analysis (KCAA) to build correlations between tunable
parameter values and measured performance values. Performance targets
are L1 cache misses, TLB misses, cycles per thread, and power
consumption. The use of KCAA restricts the scalality of their system
as the complexity of model building grows exponentially with the
number of features. In their evaluation, the system requires two hours
to build the KCAA model for only 400 seconds of benchmark data.

They present a compelling argument for the use of energy efficiency as
an optimisation target in addition to runtime, citing that it was the
power wall that lead to the multicore revolution in the first
place. Their choice of only 2 benchmarks and 2 platforms makes the
evaluation of their autotuner a little limited.

\citeauthor{Berkeley2009} targeted 3D stencils code performance
in~\cite{Berkeley2009}. Stencils are decomposed into core blocks,
sufficiently small to avoid last level cache capacity misses. These
are then decomposed to thread blocks, designed to exploit common
locality threads may have within a shared cache or local
memory. Thread blocks are then decomposed to register blocks, designed
to take advantage of data level parallelism provided by available
registers. Data allocation is optimised on NUMA systems. The
performance evaluation considers speedups of various optimisations
with and without consideration for host/device transfer overhead.


% Y. Zhang and F. Mueller, “Auto-generation and Auto-tuning of 3D
% Stencil Codes on GPU clusters,” in Proceedings of the Tenth
% International Symposium on Code Generation and Optimization, 2012,
% pp. 155–164.
In~\cite{Zhang2013a}, \citeauthor{Zhang2013a} present a code generator
and autotuner for 3D Jacobi stencil codes. Using a DSL to express
kernel functions, the code generator performs substitution from one of
two CUDA templates to create programs for execution on GPUs. GPU
programs are parameterised and tuned for block size, block dimensions,
and whether input data is stored in read only texture memory. This
creates an optimisation space of up to 200 configurations.

In an evaluation of 4 benchmarks, the authors report impressive
performance that is comparable with previous implementations of
iterative Jacobi stencils on
GPUs~\cite{Holewinski2012,Phillips2010}. The dominating parameter is
shown to be block dimensions, followed by block size, then read only
memory.

% THIS PARAGRAPH ISN'T REALLY RELEVANT:
%
% An unfortunate limitation of their system is that they do not support
% multi-GPU execution within a single node, instead using message
% passing in a cluster. This rules out the possible benefits of
% performing loop compaction across stencil iterations, an optimisation
% which has shown to be beneficial for multi-GPU performance
% in~\cite{Lutz2013}.

The DSL presented in the paper is limited to expressing only Jacobi
Stencils applications. Critically, their autotuner requires a full
enumeration of the parameter space for each program. Since there is no
indication of the compute time required to gather this data, it gives
the impression that the system would be impractical for the needs of
general purpose stencil computing. The autotuner presented in this
thesis overcomes this drawback by learning parameter values which
transfer to unseen stencils, without the need for an expensive tuning
phase for each program and architecture.
% TODO: Depending on results of cross-architecture validation, this
% last claim may not hold up.

% The majority of applications tested are memory bound. Does this
% transfer to computer bound?


\citeauthor{Kamil2010} presents an autotuning framework which accepts
as input a Fortran 95 stencil expression and generates tuned
shared-memory parallel implementations in Fortan, C, or CUDA. The
system uses an IR to explore autotuning transformations, enumerating a
subset of the optimisation space and recording only a single execution
time for each configuration, reporting the fastest. They demonstrate
their system on 4 architectures using 3 benchmarks, with speedups of
up to x22 over serial. The CUDA code generator does not optimise for
GPU memory hierarchy, using only global memory. There is no directed
search or cross-program learning.

\paragraph{PATUS} \TODO{In~\cite{Christen2011},
  \citeauthor{Christen2011} presents a DSL for expressing stencil
  codes, a C code generator, and an autotuner for exploring the
  optimisation space, using blocking and vectorisation
  strategies. Pro: Supports arbitrarily high dimensional grids. They
  introduce *2* new DSLs without comment on why they were needed. This
  is a huge price of entry for anyone who actually wants to *use*
  PATUS to solve problems, and a decision that I think they could have
  justified better. They provide nice and concise explanations of
  stencil codes and the types of optimisation methods used by other
  autotuners (ATLAS, FLAME, FFTW, SPIRAL). However, they *barely*
  explain their own, saying only that they perform *either* an
  exhaustive, multi-run Powell, Nelder Mead, or evolutionary
  algorithms.  Their evaluation uses 6 benchmarks, and 3
  architectures. Only 1 of those is a GPU. It would have been nice to
  shown performance across GPU architectures. From the perspective of
  autotuning, the paper comes as across quite weak: They do not
  present an "oracle" performance, so we can't compare the quality of
  their autotuner compared to it. They don't show how the optimal
  tunable parameter values vary across results, so they don't
  demonstrate how autotuning is *necessary* (maybe there's one single
  value which works well for all results?). They do not give any
  indication of convergence time of their autotuner. They do not
  report the number of different combinations that their autotuner
  tries.}

% T. Lutz, C. Fensch, and M. Cole, “PARTANS: An Autotuning Framework
% for Stencil Computation on Multi-GPU Systems,” ACM
% Trans. Archit. Code Optim., vol. 9, no. 4, pp. 1–24, 2013.
\paragraph{PARTANS} A stencil grid can be decomposed into smaller
subsections so that multiple GPUs can operate on each subsection
independently. This requires a small overlapping region where each
subsection meets --- the halo region --- to be shared between
devices. For iterative stencils, values in the halo region must be
synchronised between devices after each iteration, leading to costly
communication overheads. One possible optimisation is to deliberately
increase the size of the halo region, allowing each device to compute
updated values for the halo region, instead of requiring a
synchronisation of shared state. This reduces the communication costs
between GPUs, at the expense of introducing redundant computation.

Tuning the size of this halo region is the subject of PARTANS
\cite{Lutz2013}, an autotuning framework for multi-GPU stencil
computations. \citeauthor{Lutz2013} explored the effect of varying the
size of the halo regions using 6 benchmark applications, finding that
the optimal halo size depends on the size of the grid, the number of
partitions, and the connection mechanism (i.e.\ PCI express). They
developed an autotuner which determines problem decomposition and
swapping strategy offline, and performs an online search for the
optimal halo size. \TODO{Extra details about autotuner} The results of
tuning are not shared across programs or across multiple runs of the
same program.

\note{There is already a wealth of research literature on the topic
  autotuning which begs the question, why isn't the majority of
  software autotuned? The bulk of autotuning research projects falls
  prey of one of two shortcomings. Either they identify and develop a
  methodology for tuning a particular optimisation space but then fail
  to deliver a usable product, or they deliver an autotuner which
  targets too specific of a niche to gather mainstream traction. This
  projects attempts to address both of those shortcomings by expending
  great effort to deliver a working implementation which users can
  download and use without any setup costs, and by providing a modular
  and extensible framework which allows rapid targeting of new
  autotuning platforms, enabled by a shared autotuning logic and
  distributed training data.}


\section{Summary}
