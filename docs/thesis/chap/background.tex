% BACKGROUND
% ==========
%
% Background to the project, previous work, exposition of relevant
% literature, setting of the work in the proper context. This should
% contain sufficient information to allow the reader to appreciate the
% contribution you have made.

% Lit Review:

\section{Heterogeneous Parallelism}

\note{OPTIMISING PROGRAMS FOR GPUs}

Prior research has shown that performance of GPGPU programs are
heavily influenced by proper exploitation of local shared memory and
synchronisation costs~\cite{Ryoo2008a, Lee2010}

% S. Ryoo, C. I. Rodrigues, S. S. Baghsorkhi, S. S. Stone, D. B. Kirk,
% and W. W. Hwu, “Optimization principles and application performance
% evaluation of a multithreaded GPU using CUDA,” Proc. 13th ACM
% SIGPLAN Symp. Princ. Pract. parallel Program. - PPoPP ’08, p. 73,
% 2008.
\TODO{The challenges and approaches to optimising GPU programs. CUDA
programming requires the developer to explicitly manage data layout in
DRAM, caching, thread communication and other resources. Performance
of such programs depends heavily on fully utilizing zero-overhead
thread scheduling, memory bandwidth, thread grouping, shared control
flow, and intra-block thread communication. The paper gives an example
of optimising matrix multiplication by utilising shared memory through
tiling, and loop unrolling. \cite{Ryoo2008a}}

% V. W. Lee, P. Hammarlund, R. Singhal, P. Dubey, C. Kim, J. Chhugani,
% M. Deisher, D. Kim, A. D. Nguyen, N. Satish, M. Smelyanskiy, and
% S. Chennupaty, “Debunking the 100X GPU vs. CPU myth,” ACM SIGARCH
% Comput. Archit. News, vol. 38, p. 451, 2010.
In~\cite{Lee2010}, \citeauthor{Lee2010} present a performance analysis
of optimised throughput computing applications for GPUs and CPUs. Of
the 14 applications tested, they found GPU performance to be
$0.7\times$-$14.9\times$ that of multi-threaded CPU code, with an
average of only 2.5$\times$. This is much lower than the
$100\times$-$1000\times$ values reported by other studies,
% TODO: ^^ Citation needed!
a fact that they attribute to uneven comparison of optimised GPU code
to unoptimised CPU code, or vice versa. \citeauthor{Lee2010} found
that multithreading, cache blocking, reordering of memory accesses and
use of SIMD instructions to contribute most to CPU performance. For
GPUs, the most effective optimisations are reducing synchronization
costs, and exploiting local shared memory. In all cases, the programs
were optimised and hand-tuned by programmers with expert knowledge of
the target architectures. It is unclear whether their performance
results still hold for subsequent generations of devices.

% S. Ryoo, C. I. Rodrigues, S. S. Stone, S. S. Baghsorkhi, S.-Z. Ueng,
% J. a. Stratton, and W. W. Hwu, “Program optimization space pruning
% for a multithreaded GPU,” in Proceedings of the 6th annual IEEE/ACM
% international symposium on Code generation and optimization, 2008,
% pp. 195–204.
\TODO{Background info about optimizing for GPUs, including a checklist
of optimization metrics: \cite{Ryoo2008}}

\note{SUPEROPTIMIZERS}

% H. Massalin, “Superoptimizer -- A Look at the Smallest Program,” ACM
% SIGPLAN Not., vol. 22, no. 10, pp. 122–126, 1987.
\TODO{Searching for *actual* optimal programs by exhaustively
enumerating instruction set search space: \cite{Massalin1987}}

% Steuwer, M., Fensch, C., & Dubach, C. (2015). Patterns and Rewrite
% Rules for Systematic Code Generation From High-Level Functional
% Patterns to High-Performance OpenCL Code. arXiv Preprint
% arXiv:1502.02389.
\TODO{Using re-write rules to translate high-level programs to OpenCL:
\cite{Steuwer2015}}

% R. Joshi, G. Nelson, and K. Randall, “Denali: a goal-directed
% superoptimizer,” ACM SIGPLAN Not., vol. 37, no. 5, p. 304, 2002.
\TODO{Creating *actual* optimal programs through logical proofs:
\cite{Joshi2002}}


\note{APPLYING ML TO OPTIMISATION PROBLEMS}

% R. Bitirgen, E. Ipek, and J. F. Martinez, “Coordinated Management of
% Multiple Interacting Resources in Chip Multiprocessors: A Machine
% Learning Approach,” in 2008 41st IEEE/ACM International Symposium on
% Microarchitecture, 2008, pp. 318–329.
\TODO{Artificial Neural Networks for resource allocation of CMPS:
\cite{Bitirgen2008}}


\section{Algorithmic Skeletons}

% Cole, M. I. (1989). Algorithmic Skeletons: Structured Management of
% Parallel Computation. Pitman London. Retrieved from
% http://homepages.inf.ed.ac.uk/mic/Pubs/skeletonbook.pdf
\TODO{\cite{Cole1989}}

% Cole, M. I. (2004). Bringing skeletons out of the closet: a
% pragmatic manifesto for skeletal parallel programming. Parallel
% Computing, 30(3), 389–406. doi:10.1016/j.parco.2003.12.002
\TODO{\cite{Cole2004}}

% Striegnitz, J. (2000). Making C ++ Ready for Algorithmic Skeletons
% (Vol. 2000). Retrieved from http://www.fz-juelich.de/zam/FACT
\TODO{\cite{Striegnitz2000}}

\subsubsection{SkePU}

% Enmyren, J., & Kessler, C. (2010). SkePU: a multi-backend skeleton
% programming library for multi-GPU systems. In Proceedings of the
% fourth international workshop on High-level parallel programming and
% applications (pp. 5–14). ACM. Retrieved from
% http://dl.acm.org/citation.cfm?id=1863487
\TODO{\cite{Enmyren2010}}

\section{Autotuning}

% O. Jeffrey and M. David, “The Vision of Autonomic Computing,”
%Computer (Long. Beach. Calif)., vol. 36, no. 1, pp. 41–50, 2003.
\TODO{The ideal vision of self-governing agents in complex
environments. We can draw a parallel to the use of self-optimizing
agents: \cite{Jeffrey2003}}

% Ryoo, S., Rodrigues, C. I., Stone, S. S., Baghsorkhi, S. S., Ueng,
% S.-Z., Stratton, J. a., & Hwu, W. W. (2008). Program optimization
% space pruning for a multithreaded GPU. In Proceedings of the 6th
% annual IEEE/ACM international symposium on Code generation and
% optimization (pp. 195–204). New York, New York, USA: ACM
% Press. doi:10.1145/1356058.1356084
\TODO{\cite{Ryoo2008}}

% Z. Wang and M. F. P. O. Boyle, “Partitioning Streaming Parallelism
% for Multi-cores: A Machine Learning Based Approach,” in Proceedings
% of the 19th international conference on Parallel architectures and
% compilation techniques, 2010, pp. 307–318.
\TODO{Offline ML for partitioning streaming applications:
\cite{Wang2010}}

% J. Bilmes, K. Asanovic, C.-W. Chin, and J. Demmel, “Optimizing
% Matrix Multiply Using PHiPAC: A Portable, High-performance, ANSI C
% Coding Methodology,” in Proceedings of the 11th International
% Conference on Supercomputing, 1997, pp. 340–347.
\TODO{Guidelines for optimising, using an automatic code generator and
a search engine to optimise parameters: \cite{Bilmes1997}}

\note{ONLINE AUTOTUNING}

% Ansel, J., & Reilly, U. O. (2012). SiblingRivalry: Online Autotuning
% Through Local Competitions. In Proceedings of the 2012 International
% Conference on Compilers, Architectures and Synthesis for Embedded
% Systems (pp. 91–100). ACM. doi:10.1145/2380403.2380425
\TODO{\cite{Ansel2012}}

\subsubsection{PETABRICKS}
\note{PETABRICKS}

% Ansel, J. (2009). PetaBricks: a language and compiler for
% algorithmic choice. MIT.
\TODO{\cite{Ansel2009}}

% Ansel, J., & Chan, C. (2010). PetaBricks. XRDS: Crossroads, The ACM
% Magazine for Students, 17(1), 32. doi:10.1145/1836543.1836554
\TODO{\cite{Ansel2010}}

% Chan, C., Ansel, J., Wong, Y. L., Amarasinghe, S., & Edelman,
% A. (2009). Autotuning multigrid with PetaBricks. In ACM/IEEE
% Conference on Supercomputing. New York, New York, USA: ACM
% Press. doi:10.1145/1654059.1654065
\TODO{\cite{Chan2009}}

% Ansel, J. (2014). Autotuning Programs with Algorithmic
% Choice. Massachusetts Institute of Technology.
\TODO{\cite{Ansel2014}}

\subsection{Autotuning Algorithmic Skeletons}

% Contreras, G., & Martonosi, M. (2008). Characterizing and improving
% the performance of Intel Threading Building Blocks. In Workload
% Characterization, 2008. IISWC 2008. IEEE International Symposium on
% (pp. 57–66). IEEE. doi:10.1109/IISWC.2008.4636091
\TODO{INTEL TBB \cite{Contreras2008}}

\subsubsection{SkePU}

% Dastgeer, U., Enmyren, J., & Kessler, C. W. (2011). Auto-tuning
% SkePU: a multi-backend skeleton programming framework for multi-GPU
% systems. In Proceedings of the 4th International Workshop on
% Multicore Software Engineering (pp. 25–32). ACM. Retrieved from
% http://dl.acm.org/citation.cfm?id=1984697
\citeauthor{Dastgeer2011} developed a machine learning based autotuner
for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data
is used to select the optimal backend (i.e.\ CPU, GPU) for a given
program by estimating execution time and memory copy overhead based on
problem size. There is limited cross-architecture evaluation, and the
autotuner only supports vector operations.

\subsubsection{MaSiF}

% Collins, A., Fensch, C., & Leather, H. (2012). Auto-Tuning Parallel
% Skeletons. Parallel Processing Letters, 22(02),
% 1240005. doi:10.1142/S0129626412400051
\TODO{\cite{Collins2012}}

% Collins, A., Fensch, C., Leather, H., & Cole, M. (2013). MaSiF:
% Machine Learning Guided Auto-tuning of Parallel Skeletons. 20th
% Annual International Conference on High Performance Computing -
% HiPC, 186–195. doi:10.1109/HiPC.2013.6799098
\TODO{\cite{Collins2013}}


\subsection{Autotuning Stencil Codes}

\subsubsection{PATUS}

% Christen, M., Schenk, O., & Burkhart, H. (2011). PATUS: A Code
% Generation and Autotuning Framework for Parallel Iterative Stencil
% Computations on Modern Microarchitectures. In Parallel & Distributed
% Processing Symposium (IPDPS), 2011 IEEE International
% (pp. 676–687). IEEE. doi:10.1109/IPDPS.2011.70
\cite{Christen2011}

\subsubsection{PARTANS}

% T. Lutz, C. Fensch, and M. Cole, “PARTANS: An Autotuning Framework
% for Stencil Computation on Multi-GPU Systems,” ACM
% Trans. Archit. Code Optim., vol. 9, no. 4, pp. 1–24, 2013.
\citeauthor{Lutz2013} explored the effect of varying the size of the
halo regions for multi-GPU stencil computations in~\cite{Lutz2013}. A
large halo increases redundant computation across GPUs but decreases
communication costs. They found that the optimal halo size depends on
the problem size, number of partitions, and the connection mechanism
(i.e.\ PCI express). They developed an autotuner which determines
problem decomposition and swapping strategy offline, and performs an
online search for the optimal halo size. The study is limited to only
three optimisation parameters of a single class of program, and the
results of tuning are not shared across programs or across multiple
runs of the same program.

% Rul, S., Vandierendonck, H., Haene, J. D., & Bosschere,
% K. De. (2010). An Experimental Study on Performance Portability of
% OpenCL Kernels. In 2010 Symposium on Application Accelerators in
% High Performance Computing (SAAHPC’10) (pp. 4–6).
\TODO{\cite{Rul2010}}

% G. Chen and B. Wu, “PORPLE: An Extensible Optimizer for Portable
% Data Placement on GPU,” in Microarchitecture (MICRO), 2014 47th
% Annual IEEE/ACM International Symposium on, 2014, pp. 88–100.
\TODO{Optimising data placement on GPUs using a description of the
hardware and a memory-placement-agnostic compiler. Shows impressive
speedups (2.08x max, 1.59x avg) from just optimising data
placement. \cite{Chen2014}}

\citeauthor{Magni2014} explored the effect of thread coarsening
in~\cite{Magni2014}, achieving average speedups between 1.11$\times$
and 1.33$\times$ using a machine-learning model based on the static
features of kernels.


\note{ONLINE RL}

% Eastep, J., Wingate, D., & Agarwal, A. (2011). Smart Data
% Structures: An Online Machine Learning Approach to Multicore Data
% Structures. In Proceedings of the 8th ACM International Conference
% on Autonomic Computing (pp. 11–20). New York, NY, USA:
% ACM. doi:10.1145/1998582.1998587
\TODO{\cite{Eastep2011}}

% Tesauro, G. (2005). Online Resource Allocation Using Decompositional
% Reinforcement Learning. In AAAI (pp. 886–891).
\TODO{\cite{Tesauro2005}}


% Lee, H., Brown, K. J., Sujeeth, A. K., Rompf, T., & Olukotun,
% K. (2014). Locality-Aware Mapping of Nested Parallel Patterns on
% GPUs. In Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM
% International Symposium on
% (pp. 63–74). IEEE. doi:10.1109/MICRO.2014.23
\TODO{GPU benchmarking: \cite{Lee}}


\note{SKELCL}

% Steuwer, M., Kegel, P., & Gorlatch, S. (2011). SkelCL - A Portable
% Skeleton Library for High-Level GPU Programming. In Parallel and
% Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE
% International Symposium on
% (pp. 1176–1182). IEEE. doi:10.1109/IPDPS.2011.269
\TODO{The introductory SkelCL paper. Most highly cited:
\cite{Steuwer2011}}

% Steuwer, M., & Gorlatch, S. (2013). SkelCL: Enhancing OpenCL for
% High-Level Programming of Multi-GPU Systems. Parallel Computing
% Technologies, 7979, 258–272. doi:10.1007/978-3-642-39958-9_24
\TODO{Support for multi-GPU systems: \cite{Steuwer2013a}}

% S. Breuer, M. Steuwer, and S. Gorlatch, “Extending the SkelCL
% Skeleton Library for Stencil Computations on Multi-GPU Systems,”
% HiStencils 2014, pp. 23–30, 2014.
\TODO{Stencil computations: \cite{Breuer2014}}

% M. Steuwer, M. Friese, S. Albers, and S. Gorlatch, “Introducing and
% implementing the allpairs skeleton for programming multi-GPU
% Systems,” Int. J. Parallel Program., vol. 42, pp. 601–618, 2014.
\TODO{Allpairs skeleton: \cite{Steuwer2014}}


% Steuwer, M., & Gorlatch, S. (2013). High-level Programming for
% Medical Imaging on Multi-GPU Systems Using the SkelCL
% Library. Procedia Computer Science, 18,
% 749–758. doi:10.1016/j.procs.2013.05.239
\TODO{An example of reduced programmer effort for real world
application using SkelCL: \cite{Steuwer2013}}

% Steuwer, M., Kegel, P., & Gorlatch, S. (2012). Towards High-Level
% Programming of Multi-GPU Systems Using the SkelCL Library. In
% Parallel and Distributed Processing Symposium Workshops & PhD Forum
% (IPDPSW), 2012 IEEE 26th International
% (pp. 1858–1865). Ieee. doi:10.1109/IPDPSW.2012.229
\TODO{\cite{Steuwer2012}}
