\TODO{Challenges of parallel programming.}

\section{Parallel Programming with Algorithmic Skeletons}

\TODO{Intro to alg skel. Murray's thesis~\cite{Cole1989}. Algorithmic
  Skeletons in academia~\cite{Cole2004}.}

% Striegnitz, J. (2000). Making C ++ Ready for Algorithmic Skeletons
% (Vol. 2000). Retrieved from http://www.fz-juelich.de/zam/FACT
% \TODO{Early work towards C++ Alg. Skeletons\cite{Striegnitz2000}}

\subsection{Abstracting Task and Data Parallelism}

\TODO{%
  Short examples of task parallel skeletons. Describe the shortcomings
  of GPUs for task parallel operations. Data parallel operations will
  be described in detail in the SkelCL section.%
}

\subsection{Algorithmic Skeleton Frameworks}

\TODO{%
  Introduction to ASkFs. Give a couple of real world examples, both
  inside of academia and out (e.g. Intel Thread Building Blocks, and
  Google's MapReduce).%
}


\section{Heterogeneous parallelism and GPUs}

\TODO{%
  Start with a description of GPU hardware, i.e. SIMD EUs, many simple
  cores, lockstep execution. Describe the historical reasoning for
  this and how it influences the types of programs which are well
  suited for GPUs. GPUs offer huge throughput, but are even more
  challenging to program and debug. Give concrete examples.%
}

% S. Ryoo, C. I. Rodrigues, S. S. Stone, S. S. Baghsorkhi, S.-Z. Ueng,
% J. a. Stratton, and W. W. Hwu, “Program optimization space pruning
% for a multithreaded GPU,” in Proceedings of the 6th annual IEEE/ACM
% international symposium on Code generation and optimization, 2008,
% pp. 195–204.
\TODO{%
  Background info about optimizing for GPUs, including a checklist of
  optimization metrics~\cite{Ryoo2008}.%
}


\subsection{GPGPU Programming}

\TODO{Introduction to popular GPU programming models: OpenCL and
  CUDA.}

% K. Komatsu, K. Sato, Y. Arai, K. Koyama, H. Takizawa, and
% H. Kobayashi, “Evaluating performance and portability of OpenCL
% programs,” in The fifth international workshop on automatic
% performance tuning, 2010, p. 7.
\citeauthor{Komatsu2010} provide quantitative comparisons between the
performance of CUDA and OpenCL kernels
in~\cite{Komatsu2010}. \TODO{This paper is deserving of close
  scrutiny, as they perform an in-depth evaluation of the effect of
  workgroup size on the performance of kernels across devices and
  programs.}

% J. Fang, A. L. Varbanescu, and H. Sips, “A Comprehensive Performance
% Comparison of CUDA and OpenCL,” in Parallel Processing (ICPP), 2011
% International Conference on, 2011, pp. 216–225.
\TODO{Comparing OpenCL and CUDA performance: \cite{Karimi2010}}

% Enmyren, J., & Kessler, C. (2010). SkePU: a multi-backend skeleton
% programming library for multi-GPU systems. In Proceedings of the
% fourth international workshop on High-level parallel programming and
% applications (pp. 5–14). ACM. Retrieved from
% http://dl.acm.org/citation.cfm?id=1863487
\subsubsection{Skeletal Programming for GPUs}

\TODO{%
  SkePU Similar to SkelCL in scope and ambition, alternative
  implementation with C++ macros and CUDA~\cite{Enmyren2010}.%
}

\subsubsection{GPU vs CPU performance analysis}

% V. W. Lee, P. Hammarlund, R. Singhal, P. Dubey, C. Kim, J. Chhugani,
% M. Deisher, D. Kim, A. D. Nguyen, N. Satish, M. Smelyanskiy, and
% S. Chennupaty, “Debunking the 100X GPU vs. CPU myth,” ACM SIGARCH
% Comput. Archit. News, vol. 38, p. 451, 2010.
In~\cite{Lee2010}, \citeauthor{Lee2010} present a performance analysis
of optimised throughput computing applications for GPUs and CPUs. Of
the 14 applications tested, they found GPU performance to be
$0.7\times$-$14.9\times$ that of multi-threaded CPU code, with an
average of only 2.5$\times$. This is much lower than the
$100\times$-$1000\times$ values reported by other studies,
% TODO: ^^ Citation needed!
a fact that they attribute to uneven comparison of optimised GPU code
to unoptimised CPU code, or vice versa. \citeauthor{Lee2010} found
that multithreading, cache blocking, reordering of memory accesses and
use of SIMD instructions to contribute most to CPU performance. For
GPUs, the most effective optimisations are reducing synchronization
costs, and exploiting local shared memory. In all cases, the programs
were optimised and hand-tuned by programmers with expert knowledge of
the target architectures. It is unclear whether their performance
results still hold for subsequent generations of devices.


\subsection{The OpenCL Programming Model}

\TODO{Technical details of OpenCL model. Describe the memory model.}

% Rul, S., Vandierendonck, H., Haene, J. D., & Bosschere,
% K. De. (2010). An Experimental Study on Performance Portability of
% OpenCL Kernels. In 2010 Symposium on Application Accelerators in
% High Performance Computing (SAAHPC’10) (pp. 4–6).
\TODO{Portability of OpenCL kernels~\cite{Rul2010}}

\section{High-Level GPU Programming with SkelCL}

Introduced in~\cite{Steuwer2011}, SkelCL is an object oriented C++
library that provides OpenCL implementations of data parallel
algorithmic skeletons for heterogeneous parallelism using CPUs or
multi-GPUs. Skeletons are parameterised with muscle functions by the
user, which are compiled into OpenCL kernels for execution on device
hardware. The Vector and Matrix container types transparently handle
communication between the host and device memory, and support
partitioning for multi-GPU execution.

SkelCL is freely
available\footnote{\url{http://skelcl.uni-muenster.de}} and
distributed under dual GPL and academic licenses.

\TODO{%
  Towards high-level programming~\cite{Steuwer2012}. Support for
  multi-GPU systems~\cite{Steuwer2013a}. Stencil
  computations~\cite{Breuer2014}. Allpairs
  skeleton~\cite{Steuwer2014}. An example of reduced programmer effort
  for real world application using SkelCL: \cite{Steuwer2013}%
}


\subsection{Pattern definitions}

\TODO{Brief descriptions and example uses of patterns.}

\subsubsection{Map}

\begin{equation}
\map\left(f, [x_1,x_2,\ldots,x_n]\right) \to [f(x_1),f(x_2),\ldots,f(x_n)]
\end{equation}

When applied to an $n \times m$ matrix:

\begin{equation}
\map\left(f,
\begin{bmatrix}
  x_{11} & \cdots & x_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nm}
\end{bmatrix}\right)
\to
\begin{bmatrix}
  f(x_{11}) & \cdots & f(x_{1m}) \\
  \vdots & \ddots & \vdots \\
  f(x_{n1}) & \cdots & f(x_{nm})
\end{bmatrix}
\end{equation}

\subsubsection{Zip}

\begin{equation}
\zip\left( \oplus, [x_1,x_2,\ldots,x_n], [y_1,y_2,\ldots,y_n] \right)
\to
\left[ x_1 \oplus y_1, x_2 \oplus y_2, \ldots, x_n \oplus y_n \right]
\end{equation}

\begin{equation}
\begin{split}
\zip \left( \oplus,
\begin{bmatrix}
  x_{11} & \cdots & x_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nm}
\end{bmatrix},
\begin{bmatrix}
  y_{11} & \cdots & y_{1m} \\
  \vdots & \ddots & \vdots \\
  y_{n1} & \cdots & y_{nm}
\end{bmatrix} \right) \\
\to
\begin{bmatrix}
  x_{11} \oplus y_{11} & \cdots & x_{1m} \oplus y_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} \oplus y_{n1} & \cdots & x_{nm} \oplus y_{nm}
\end{bmatrix}
\end{split}
\end{equation}

\subsubsection{Reduce}

\begin{equation}
\reduce \left( \oplus, i, [x_1,x_2,\ldots,x_n] \right)
\to
x_1 \oplus x_2 \oplus \ldots \oplus x_n
\end{equation}

\begin{equation}
\reduce \left( \oplus, i,
\begin{bmatrix}
  x_{11} & \cdots & x_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nm}
\end{bmatrix} \right)
\to
x_{11} \oplus x_{12} \oplus \ldots \oplus x_{nm}
\end{equation}

\subsubsection{Scan}

\begin{equation}
\scan \left( \oplus, i, [x_1,x_2,\ldots,x_n] \right)
\to
\left[ i, x_1, x_1 \oplus x_2, \ldots, x_1 \oplus x_2 \oplus \ldots \oplus x_n \right]
\end{equation}

\subsubsection{AllPairs}

\begin{equation}
\allpairs \left( \oplus,
\begin{bmatrix}
  x_{11} & \cdots & x_{1d} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nd}
\end{bmatrix},
\begin{bmatrix}
  y_{11} & \cdots & y_{1m} \\
  \vdots & \ddots & \vdots \\
  y_{n1} & \cdots & y_{nm}
\end{bmatrix} \right)
\to
\begin{bmatrix}
  z_{11} & \cdots & z_{1m} \\
  \vdots & \ddots & \vdots \\
  z_{n1} & \cdots & z_{nm}
\end{bmatrix}
\end{equation}

where:

\begin{equation}
z_{ij} =
\left[ x_{i1}, x_{i2}, \ldots, x_{id} \right] \oplus
\left[ y_{j1}, y_{j2}, \ldots, y_{jd} \right]
\end{equation}

An additional implementation is provided for when the $\oplus$
operator is known to match that of a zip pattern:

\begin{equation}
z_{ij} =
\left[
  x_{i1}, \oplus y_{j1}, x_{i2} \oplus y_{j2}, \ldots, x_{id} \oplus y_{jd}
\right]
\end{equation}


\subsubsection{Stencil}

Stencils are patterns of computation which operate on uniform grids of
data, where the value of each cell is updated based on its current
value and the value of one or more neighbouring elements, which we'll
call the border region. In SkelCL, users provide the function which
updates a cell's value, and SkelCL orchestrates the parallel execution
of these functions. Each cell maps to a single work item; and this
collection of work items is then divided into \emph{workgroups} for
execution on the target hardware.

Given a customising function $f$, a \emph{stencil shape} $S$, and an
$n \times m$ matrix:

\begin{equation}
\stencil \left( f, S,
\begin{bmatrix}
  x_{11} & \cdots & x_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nm}
\end{bmatrix} \right)
\to
\begin{bmatrix}
  z_{11} & \cdots & z_{1m} \\
  \vdots & \ddots & \vdots \\
  z_{n1} & \cdots & z_{nm}
\end{bmatrix}
\end{equation}

where:

\begin{equation}
z_{ij} = f \left(
\begin{bmatrix}
  z_{i-S_n,j-S_w} & \cdots & z_{i-S_n,j+S_e} \\
  \vdots & \ddots & \vdots \\
  z_{i+S_s,j-S_w} & \cdots & z_{i+S_s,j+S_e}
\end{bmatrix} \right)
\end{equation}

A popular application of Stencil codes is for iterative problems, in
which \todo{\ldots} discrete time steps $0 <= t <= t_{max}$, and
$t \in \mathbb{Z}$

\begin{equation}
g(f, S, M, t) =
\begin{cases}
  \stencil \left( f, S, g(f, S, M, t-1) \right),& \text{if } t \geq 1\\
  M_{init}, & \text{otherwise}
\end{cases}
\end{equation}

A Stencil operation in which the size of the stencil shape $S$ is zero
in every direction is functionally equivalent to a Map operation.


\subsection{Container Types}

\TODO{Description of vector and matrices, supported data types, lazy
  data transfer \ldots}

\subsubsection{Data distributions}

\TODO{Description and diagrams for single, block, copy, and overlap
  distributions.}


\subsection{Implementation Details}

Each skeleton is represented by a template class, declared in a header
file detailing the public API. A private header file contains the
template definition. E.g. \texttt{SkelCL/Map.h} contains the Map
class, and \texttt{SkelCL/detail/MapDef.h} contains the
implementation. Non-trivial kernels are stored in separate source
files, e.g. \texttt{SkelCL/detail/MapKernel.cl}.

\TODO{Description of OpenCL skeleton templates, and the compilation
  process - i.e. substitution of user functions, handling additional
  arguments  \ldots}

\TODO{Algorithm listing for stencil}

\subsubsection{Stencil}

\lstinputlisting[
  language=C++,
  float,
  floatplacement=t,
  label=lst:skelcl-stencil-interface,
  caption={
    Interface for SkelCL stencil skeleton.
  }
]{dat/skelcl-stencil-interface.cpp}

\subsubsection{StencilSequence}

\TODO{Iterative stencils, and sequences of stencils}

\subsection{Example Applications}

% Three example programs

% ease of use
% skeleton composition
% stencils

\TODO{%
  Provide definition of three simple example programs, then code
  listings for SkelCL implementations and a comparison of runtimes
  using a decent GPU vs. sequential CPU. Preferably also hand coded
  OpenCL?%
}

\subsubsection{Application 1: Mandelbrot Set}

\TODO{Definition, and performance results.}

% \TODO{%
%   The easy way to do this is to write a program which will iterate
%   over the pixels in your image and calculate their value. This is
%   easy, intuitive, and it's around 50 lines of code. The downside is
%   it is slow.%
% }

% \todo{%
%   So in the name of performance you start browsing around for ways to
%   make this fast. You notice a lot of buzz about "heterogeneous
%   parallelism", so you decide to give it a shot and dabble in a bit of
%   GPGPU programming. After reading through some tutorials and the API
%   documentation you find that all you have to do is: add a few OpenCL
%   headers, select a platform, select a device, create a command queue,
%   compile a program, create a kernel, create a buffer, enqueue a
%   kernel, read the buffer, handle any errors... and you are
%   done. This blows your single program out to around 200 lines of
%   code, but it runs 20 times faster. In the age of multicores you're
%   going to need that extra performance, but as a developer you may
%   not be willing to pay the high price that this demands.%
% }

% \todo{%
%   Algorithmic Skeletons offer a solution; by abstracting common
%   patterns of communication, they allow libraries and language authors
%   to provide robust parallel implementations which allow users to
%   focus on solving problems, rather than having to micro-manage the
%   tricky coordination of parallel resources. That's all well and good,
%   but if you said to me "Chris, surely you could illustrate this point
%   better using a cartoon skeleton doing a jig", I'd be inclined to
%   agree with you.%
% }

% \todo{%
%   So you take your new-found knowledge of Algorithmic Skeletons and
%   apply it to your Mandelbrot problem. You realise that calculating
%   each pixel is just a Map operation, so, armed with a Map skeleton,
%   you go back to your sequential Mandelbrot code and make the
%   necessary adjustments to use Skeletons. Now you have a program which
%   looks just like your sequential version, but harnesses all the power
%   of the GPU to provide near-``hand tuned'' levels of performance.%
% }

% \todo{%
%   I say near hand tuned performance because clearly it's not quite as
%   fast as when you did all of that OpenCL programming yourself. Why is
%   that? The reason is that by their nature, Algorithmic Skeletons are
%   forced to forgo low-level tuning in order to generalise for all
%   cases. If you think how hard it is to select the effective
%   optimisations for a program, imagine trying to achieve that at the
%   level of a patterns library for all possible use-cases! For this
%   reason, I would argue that if we want to achieve both the of ease of
%   use of skeletons and the high performance of hand tuned code, we
%   need autotuning.%
% }

\subsubsection{Application 2: Dot Product}

\TODO{Definition, and performance results.}

\lstset{language=C++}
\begin{lstlisting}[
  float,
  floatplacement=t,
  basicstyle=\scriptsize,
  caption={Example program to calculate dot product using SkelCL.}
]
#include <SkelCL/SkelCL.h>
#include <SkelCL/Vector.h>
#include <SkelCL/Zip.h>
#include <SkelCL/Reduce.h>

int main(int argc, char* argv[]) {
  // Initialise SkelCL to use any device.
  skelcl::init(skelcl::nDevices(1).deviceType(skelcl::device_type::ANY));

  // Define the skeleton objects.
  skelcl::Zip<int(int, int)> mult("int func(int x, int y) { return x * y; }");
  skelcl::Reduce<int(int)> sum("int func(int x, int y) { return x + y; }", "0");

  // Create two vectors A and B of length "n".
  const int n = 1024; skelcl::Vector<int> A(n), B(n);
  skelcl::Vector<int>::iterator a = A.begin(), b = B.begin();
  while (a != A.end()) { *a = rand() % n; ++a; *b = rand() % n; ++b; }

  // Invoke skeleton: x = A . B
  int x = sum(mult(A, B)).first();

  return 0;
}
\end{lstlisting}

\subsubsection{Application 3: Gaussian Blur}

\TODO{Definition, listing, and performance results.}

\subsection{Grid Decomposition of Stencil Codes}

In OpenCL, kernels are mapped to work-items for execution on the
processing units of GPUs and CPUs. These work-items are then grouped
into one or more workgroups. The choice of workgroup size is left to
the developer, but with two sets of constraints. The first set of
constraints is the maximum workgroup size which an execution device
supports. This value cannot be exceeded, irrespective of the kernel
being executed. The second constraint is the maximum workgroup size a
kernel supports, and can only be queried at runtime once a kernel has
been compiled. The selection of workgroup size for stencil skeletons
is particularly relevant to the performance of the stencil as it
affects utilisation of fast local memory. In a stencil code, each
work-item reads the values of multiple neighbouring elements. To
facilitate this, the value of all elements within a workgroup are
stored in fast local memory, which greatly reduces the read latency
for GPUs. Changing the workgroup size affects the amount of local
memory required for each workgroup, which in turn affects the number
of workgroups which may be simultaneously active.

\TODO{%
  While the user is clearly in control of the type of work which is
  executed, the size of the grid, and the size of the border region,
  it is very much the responsibility of the skeleton implementation to
  select what workgroup size to use. As such I designed an experiment
  to explore just what effect changing workgroup sizes has on the
  performance of Stencil skeletons.%
}

\TODO{Add diagram showing workgroup decomposition.}
