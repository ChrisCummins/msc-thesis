Parallelism is increasingly seen as the only viable approach to
maintaining continued performance improvements in a multicore
world. Core counts in all types of computers from mobile to
supercomputers have been steadily increasing, with most machines
having a graphics card. Despite this shift in the hardware paradigm,
the adoption of parallel programming practises has been slow and
awkward due to the prohibitive complexity and low level of
abstractions available to programmers.

Programmers today who want to take advantage of the available
processing power of a machine must be intimiately knowledgable of the
operating system or language-specific threading model and
synchronisation primitives, and at least one system for programming
the GPU (which is almost certainly an entirely different programming
language). Coupled with the inherent cognitive overhead of reasoning
about and debugging parallel programs, this has led to the
understandable belief that parallel programming is only for the
elite. \fixme{We need some way of improving accessibility}

Introduced by \citeauthor{Cole1989} in \citeyear{Cole1989},
Algorithmic Skeletons offer to simplify the task of parallel
programming by abstracting common patterns of communication, providing
parallelised implementations of higher order
functions~\cite{Cole1989}. Algorithmic Skeletons capture patterns of
communication, and are parameterised with ``muscle functions'' by the
programmer that implement the problem specific logic.

\TODO{The problem with threads\ldots \cite{Lee2006}}

GPUs enable massive performance through heterogeneous
parallelism. However, developing software for these devices is
challenging, as the programming models provided by OpenCL and CUDA
require a low level knowledge of the underlying architecture to
properly exploit the potential performance. SkelCL addresses this
programmability challenge by providing high level patterns for common
data parallel operations. This thesis demonstrates that the parameters
which are necessarily abstracted by such skeletons can have a huge
impact on performance.

The ambition of this thesis is to demonstrate that, using machine
learning, we can develop self-tuning systems which closely approach
--- and in some cases, outperform --- the kinds of hand tuned code
which traditionally came at the cost of hundreds of man hours of work
from expert programmers to develop.

% B. Catanzaro and K. Keutzer, “Parallel computing with patterns and
% frameworks,” XRDS Crossroads, ACM Mag. Students, vol. 17, no. 5,
% p. 22, 2010.
% \todo{\cite{Catanzaro2010}}

\note{There is already a wealth of research literature on the topic
  autotuning which begs the question, why isn't the majority of
  software autotuned? The bulk of autotuning research projects falls
  prey of one of two shortcomings. Either they identify and develop a
  methodology for tuning a particular optimisation space but then fail
  to deliver a usable product, or they deliver an autotuner which
  targets too specific of a niche to gather mainstream traction. This
  projects attempts to address both of those shortcomings by expending
  great effort to deliver a working implementation which users can
  download and use without any setup costs, and by providing a modular
  and extensible framework which allows rapid targeting of new
  autotuning platforms, enabled by a shared autotuning logic and
  distributed training data.}

\note{The rationale for autotuning long running iterative skeleton
  applications is that the longer the program runs, the smaller the
  training data we can reasonably expect the user to gather. A typical
  long running scientific program workload will only be run once, as
  there is no need to repeat a computation for which you already have
  the answer.}

\section{The Problem}

\TODO{Why do we need autotuning? Describe necessary abstraction of low
  level tuning parameters for high-level patterns, and demonstrate
  their effect on application performance.}


\section{Motivating Example}

\TODO{Simple example showing the ``power'' of picking the right
  optimisation parameter values. Take a small handful of contrasting
  performance results, especially for cases where human intuition
  fails.}


\section{Contributions}

The key contributions of this thesis are:

\begin{itemize}
  % FIXME: Consider re-ordering these to bury the evaluation
  % part. It's not as interesting as the autotuner itself.
\item An empirical evaluation of the performance of workgroup size to
  parameterise high-level parallel patterns. We enumerate the
  optimisation space of workgroup size for SkelCL stencil kernels
  across \input{gen/num_runtime_stats} test instances, demonstrating
  an average performance loss of
  $\input{gen/avg_possible_speedup_perc}\%$ (max
  $\input{gen/max_possible_speedup_perc}\%$) if workgroup size is not
  correctly tuned. \TODO{Unexpected results. By exploring a much
    larger space than is traditionally used, we find that workgroup
    sizes outside the expected norm (i.e. not a multiple of 32) can
    bring huge performance bonuses.}
\item The development of \emph{OmniTune} --- an extensible,
  distributed autotuner. When tasked with selecting workgroup sizes of
  stencil codes in SkelCL, the presented autotuner achieves
  $\input{gen/best_avg_classification_performance}\%$ of the available
  performance, providing an average
  $\input{gen/best_avg_classification_speedup}\times$ speedup over the
  best possible statically chosen value.
\item The novel technique for the procedural generation of stencil
  skeleton programs, and its applications for machine learning-enabled
  autotuning. This has the advantages of reducing the cost of training
  while increasing the size of the space which can be explored. The
  effectiveness of this approach is demonstrated by testing an
  autotuner trained using synthetic benchmarks against
  \input{gen/num_real_kernels} real world stencil codes, achieving a
  worst case
  $\input{gen/biggest_synthetic_real_classification_performance_drop}\%$
  loss in achieved performance.
\item An evaluation of several approaches to performing machine
  learning classification in which the hypothesis space has hard
  constraints on legality.
\item A comparison of multiple approaches to minimise the cost of
  effective autotuning. \TODO{flesh out: compare cost of full
    enumeration with that of classification and regression.}
\end{itemize}

\section{Structure}

The remainder of the document is structured as follows:

\begin{itemize}
\item Chapter~\ref{chap:background} contains necessary background
  material, an introduction to the SkelCL framework, and a description
  of the techniques used throughout the thesis;
\item Chapter~\ref{chap:related} contains an exposition of relevant
  literature in the field of autotuning and heterogeneous parallelism,
  contrasting the related research with my own;
\item Chapter~\ref{chap:autotune} presents OmniTune, an extensible and
  distributed autotuner capable of predicting optimisation parameter
  values for unseen programs at runtime;
\item Chapter~\ref{chap:methodology} describes a comprehensive
  exploration of the optimisation space of workgroup size for stencil
  skeletons, including the methodology for obtaining performance data
  and experimental setup;
\item Chapter~\ref{chap:evaluation} contains an extensive evaluation
  of the effectiveness of OmniTune with respect to its accuracy,
  performance performance compared to human experts, and a cost
  benefit analysis of autotuning;
\item Chapter~\ref{chap:conclusions} contains concluding remarks, a
  critical evaluation of the presented work, and plans for future
  research.
\end{itemize}
