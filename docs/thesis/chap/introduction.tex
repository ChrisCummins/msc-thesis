GPUs enable massive performance through heterogeneous
parallelism. However, developing software for these devices is
challenging, as the programming models provided by OpenCL and CUDA
require a low level knowledge of the underlying architecture to
properly exploit the potential performance. SkelCL addresses this
programmability challenge by providing high level patterns for common
data parallel operations. This thesis demonstrates that the parameters
which are necessarily abstracted by such skeletons can have a huge
impact on performance.

The ambition of this thesis is demonstrate that, using machine
learning, we can develop self-tuning systems which closely approach
--- and in some cases, outperform --- the kinds of hand tuned code
which traditionally came at the cost of hundreds of man hours of work
from expert programmers to develop.

% B. Catanzaro and K. Keutzer, “Parallel computing with patterns and
% frameworks,” XRDS Crossroads, ACM Mag. Students, vol. 17, no. 5,
% p. 22, 2010.
% \todo{\cite{Catanzaro2010}}

\note{There is already a wealth of research literature on the topic
  autotuning which begs the question, why isn't the majority of
  software autotuned? The bulk of autotuning research projects falls
  prey of one of two shortcomings. Either they identify and develop a
  methodology for tuning a particular optimisation space but then fail
  to deliver a usable product, or they deliver an autotuner which
  targets too specific of a niche to gather mainstream traction. This
  projects attempts to address both of those shortcomings by expending
  great effort to deliver a working implementation which users can
  download and use without any setup costs, and by providing a modular
  and extensible framework which allows rapid targeting of new
  autotuning platforms, enabled by a shared autotuning logic and
  distributed training data.}

\note{The rationale for autotuning long running iterative skeleton
  applications is that the longer the program runs, the smaller the
  training data we can reasonably expect the user to gather. A typical
  long running scientific program workload will only be run once, as
  there is no need to repeat a computation for which you already have
  the answer.}

\section{The Problem}

\TODO{Why do we need autotuning? Describe necessary abstraction of low
  level tuning parameters for high-level patterns, and demonstrate
  their effect on application performance.}


\section{Motivating Example}

\TODO{Simple example showing the ``power'' of picking the right
  optimisation parameter values. Take a small handful of contrasting
  performance results, especially for cases where human intuition
  fails.}


\section{Contributions}

The key contributions of this thesis are:

\begin{itemize}
\item A broad review of the current state of the art in approaches to
  the challenge of programmability for heterogeneous systems. A
  focused literature review on iterative and machine-learning enabled
  compilation and autotuning for compute intensive parallel
  applications.
\item An empirical evaluation of the performance of workgroup size to
  parameterise high-level parallel patterns. We enumerate the
  optimisation space of workgroup size for SkelCL stencil kernels
  across \input{gen/num_runtime_stats} test instances, demonstrating
  an average performance loss of
  $\input{gen/avg_possible_speedup_perc}\%$ (max
  $\input{gen/max_possible_speedup_perc}\%$) if workgroup size is not
  correctly tuned.
\item The development of \emph{OmniTune} --- an extensible,
  distributed autotuner. When tasked with selecting workgroup sizes of
  stencil codes in SkelCL, the presented autotuner achieves
  $\input{gen/best_avg_classification_performance}\%$ of the available
  performance, providing an average
  $\input{gen/best_avg_classification_speedup}\times$ speedup over the
  best possible statically chosen value.
\item The novel application of synthetic \TODO{(programatically
    generated?)} stencil codes for training machine learning-enabled
  autotuning. This has the advantages of reducing the cost of training
  while increasing the size of the space which can be explored. The
  effectiveness of this approach is demonstrated by training an
  testing an autotuner trained using synthetic benchmarks against
  \input{gen/num_real_kernels} real world stencil codes, achieving
  $\input{gen/best_avg_synthetic_real_classification_performance}\%$
  of the available performance, providing an average
  $\input{gen/best_avg_synthetic_real_classification_speedup}\times$
  speedup over the best possible statically chosen value.
\item A comparison of multiple approaches to minimise the costs of
  effective autotuning. \TODO{flesh out: compare cost of full
    enumeration with that of classification and regression.}
  % TODO: Unexpected results. By exploring a much larger space than is
  % traditionally used, we find that workgroup sizes outside the
  % expected norm (i.e. not a multiple of 32) can bring huge
  % performance bonuses.
\end{itemize}

\section{Structure}

The remainder of the document is structured as follows:

\begin{itemize}
\item Chapter~\ref{chap:background} contains necessary background
  information, an introduction to the SkelCL framework, and a
  description of the techniques used throughout the thesis;
\item Chapter~\ref{chap:related} contains an exposition of the
  relevant literature in the field of autotuning and heterogeneous
  parallelism, contrasting the existing research with my
  own;
\item Chapter~\ref{chap:methodology} describes an exploration of the
  stencil workgroup size optimisation space, including methodology and
  observed performance results;
\item Chapter~\ref{chap:autotune} presents OmniTune, an extensible and
  distributed autotuner capable of machine-learning enabled runtime
  selection of optimisation parameter values;
\item Chapter~\ref{chap:evaluation} contains an extensive evaluation
  of the effectiveness of OmniTune for selecting workgroup sizes for
  SkelCL stencil codes;
\item Chapter~\ref{chap:conclusions} contains concluding remarks, a
  critical evaluation of the presented work, and plans for future
  research.
\end{itemize}
