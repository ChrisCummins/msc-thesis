GPUs enable massive performance through heterogeneous
parallelism. However, developing software for these devices is
challenging, as the programming models provided by OpenCL and CUDA
require a low level knowledge of the underlying architecture to
properly exploit the potential performance. SkelCL addresses this
programmability challenge by providing high level skeleton patterns
for common data parallel operations. This project demonstrates that
the parameters which are necessarily abstracted by such skeletons can
have a huge impact on performance. To demonstrate this, I present an
autotuner for selecting the \emph{workgroup size} of Stencil pattern
kernels.

% B. Catanzaro and K. Keutzer, “Parallel computing with patterns and
% frameworks,” XRDS Crossroads, ACM Mag. Students, vol. 17, no. 5,
% p. 22, 2010.
\todo{\cite{Catanzaro2010}}

\TODO{The rationale for autotuning long running iterative skeleton
  applications is that the longer the program runs, the smaller the
  training data we can reasonably expect the user to gather. A typical
  long running scientific program workload will only be run once, as
  there is no need to repeat a computation for which you already have
  the answer.}

\TODO{There is already a wealth of research literature on the topic
  autotuning which begs the question, why isn't the majority of
  software autotuned? The bulk of autotuning research projects falls
  prey of one of two shortcomings. Either they identify and develop a
  methodology for tuning a particular optimisation space but then fail
  to deliver on any usable product, or they deliver an autotuner which
  targets too specific of a niche to become mainstream. This projects
  attempts to address both of those shortcomings by expending great
  effort to deliver a working implementation which users can download
  and use without any setup costs, and by providing a modular and
  extensible framework which allows rapid targeting of new autotuning
  platforms, enabled by a shared autotuning logic and distributed
  training data.}


\section{Algorithmic Skeletons}

% Cole, M. I. (1989). Algorithmic Skeletons: Structured Management of
% Parallel Computation. Pitman London. Retrieved from
% http://homepages.inf.ed.ac.uk/mic/Pubs/skeletonbook.pdf
\TODO{Murray's thesis~\cite{Cole1989}.}

% Cole, M. I. (2004). Bringing skeletons out of the closet: a
% pragmatic manifesto for skeletal parallel programming. Parallel
% Computing, 30(3), 389–406. doi:10.1016/j.parco.2003.12.002
\TODO{Algorithmic Skeletons in academia~\cite{Cole2004}.}

% Striegnitz, J. (2000). Making C ++ Ready for Algorithmic Skeletons
% (Vol. 2000). Retrieved from http://www.fz-juelich.de/zam/FACT
\TODO{Early work towards C++ Alg. Skeletons\cite{Striegnitz2000}}


\section{The Problem}

\TODO{Why do we need autotuning? Describe necessary abstraction of low
  level tuning parameters, and demonstrate their effect on application
  performance.}


\section{Contributions}

The key contributions of this thesis are:

\begin{itemize}
\item A broad review of the current state of the art in approaches to
  the challenge of parallel programmability, namely automatic
  parallelisation and high-level parallel programming. A focused
  literature review on iterative and machine-learning enabled
  compilation and autotuning for compute intensive parallel
  applications.
\item An empirical evaluation of the performance impact of values used
  to parameterise high-level parallel patterns. We enumerate the
  optimisation space of workgroup size for SkelCL stencil kernels
  using \fixme{XXX} test instances, showing how choice of workgroup
  size has an average \fixme{XXX}$\times$ influence on runtime (max
  \fixme{XXX}$\times$).
\item The development of \emph{omnitune} --- an extensible,
  distributed autotuner. When tasked with selecting workgroup sizes of
  stencil codes in SkelCL, the presented autotuner achieves an average
  \fixme{XXX}$\times$ speedup over the best possible statically chosen
  value, achieving \fixme{XXX}\% of the oracle performance.
\item The novel application of synthetic \TODO{(programatically
    generated?)} benchmarks for training machine learning-enabled
  autotuning. This has the advantages of reducing the cost of training
  while increasing the size of the space which can be explored. The
  effectiveness of this approach is demonstrated by training an
  autotuner for 4 real world Stencil codes using synthetic benchmarks,
  showing that \FIXME{sexy result goes here}.
\item A comparison of multiple approaches to minimise the costs of
  effective autotuning. \TODO{flesh out: compare cost of full
    enumeration with that of classification and regression.}
\end{itemize}

\section{Structure}

The remainder of the document is structured as follows:
\fixme{\ldots}

\subsubsection{Notation and Terminology}

\TODO{Define mathematical notation used, and uncommon terms such as
  ``oracle''.}
