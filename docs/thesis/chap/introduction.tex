Parallelism is increasingly seen as the only viable approach to
maintaining continued performance improvements in a multicore world.
Despite this, the adoption of parallel programming practises has been
slow and awkward, leading to a growing disparity between the levels of
available performance and the ability for application developers to
exploit it.

The multicore processors of modern devices offer many opportunities
for parallelism, but fully harnessing this processing power requires
an intimate knowledge of both the parallel programming semantics of
the language and performance characteristics of the underlying
hardware. In recent years, general purpose programming with GPUs
promises even greater data parallel throughput, but is a significantly
greater challenge to tame, forcing developers to master an entirely
alien programming model (such as provided by CUDA or OpenCL) and
architecture (SIMD with a multi-level memory hierarchy). As such,
GPGPU programming is often considered beyond the realm of all but the
most power demanding of expert programmers. If steps are not taken to
increase the accessibility of such parallelism, this will only serve
to widen the gap between available and utilised performance as the
core counts of hardware continue to increase.

One possible solution for this \emph{programmability challenge} comes
in the form of Algorithmic Skeletons, which offer to simplify parallel
programming by raising the level of abstraction so that developers can
focus on solving problems, rather than coordinating parallel
resources. They achieve this by by providing robust parallel
implementations of common patterns of computation which developers
parameterise with their application-specific code. This greatly
reduces the challenge of parallel programming, allowing users to
structure their problem solving logic sequentially, while offloading
the cost of parallel coordination to the skeleton author.


\section{The Problem}

Unfortunately, when tuning parallel programs, one size \emph{cannot}
fit all, as the parameters which affect the performance of a parallel
program are architecture specific, program specific, and even
\emph{dataset} specific. This is especially problematic for
Algorithmic Skeletons, as skeleton author cannot tune the performance
of an implementation across the breadth of these three dimensions, and
this results in programs which forgo the performance advantages that
can be achieved with the low level tuning of hand written parallel
code.


% First prong: standardise autotuning

If the performance of Algorithmic Skeletons is to be competitive with
that of hand crafted parallel programs, then these skeletons must be
capable of adapting to their environments. The development of such
\emph{autotuning} software is an entire research field unto itself ---
and understandably so: there is an irresistible appeal to the idea of
software which is capable of maximising its own efficiency without the
need for human intervention. The unfortunate reality is that while
these autotuning systems share the unified goal of improving the
performance of their respective optimisation targets, the range of
competing approaches and implementations has resulted in a fragmented
state in which no one system has been able to achieve mainstream
traction. This is the first aim of this thesis: to tackle the issue of
providing a unified interface for autotuning which reduces the amount
of redundant and overlapping work needed to implement autotuning for
different optimisation targets.

% Second prong: machine learning to cut down cost

The second aim of this thesis is to explore the techniques for
reducing the \emph{cost} of autotuning for use in Algorithmic
Skeletons. Typically, autotuning using iterative compilation requires
enumerating some portion of the optimisation space for each program
being tuned; however, the cost of such an exploration is prohibitively
expensive when summed across the broad range of uses cases targeted by
Algorithmic Skeletons. As such, successfully autotuning Algorithmic
Skeletons will require a method for \emph{predicting} the values of
parameters which will maximise performance, without the need for trial
and error. To succeed, such a method of tuning does not need to
provide perfectly accurate predictions, but simply sufficiently
estimates so that, when combined with the vast accessibility
improvements offered by Algorithmic Skeletons, it provides a
convincing argument for Algorithmic Skeletons as the solution to the
parallel programmability crisis.


\section{Contributions}

The key contributions of this thesis are:

\begin{itemize}
\item The development of \emph{OmniTune} --- a novel and extensible
  framework for autotuning of optimisation parameters during a
  program's runtime.
\item The application of OmniTune for tuning of the SkelCL Algorithmic
  Skeleton library. When tasked with predicting the workgroup sizes of
  Stencil skeletons on both GPUs and CPUs, OmniTune achieves
  $\input{gen/best_avg_classification_performance}\%$ of the available
  performance, providing an average
  $\input{gen/best_avg_classification_speedup}\times$ speedup over the
  best possible statically chosen parameter.
\item The novel application of procedurally generated benchmark
  programs for collecting autotuning training data. This reduces the
  cost of training while increasing the size of the space which can be
  explored. The effectiveness of this approach is demonstrated by
  testing an autotuner trained using synthetic benchmarks against
  \input{gen/num_real_kernels} real world stencil codes, achieving a
  worst case
  $\input{gen/biggest_synthetic_real_classification_performance_drop}\%$
  loss in achieved performance.
\item An empirical evaluation of the performance of workgroup size to
  parameterise high-level parallel patterns. I enumerate the
  optimisation space of workgroup size for SkelCL stencil kernels
  across \input{gen/num_runtime_stats} test cases, demonstrating an
  average performance loss of up to
  $\input{gen/avg_possible_speedup_perc}\%$ if workgroup size is not
  correctly tuned. % \TODO{Unexpected results. By exploring a much
    % larger space than is traditionally used, we find that workgroup
    % sizes outside the expected norm (i.e. not a multiple of 32) can
    % bring huge performance bonuses.}
\item A comparison of multiple approaches to minimise the cost of
  effective autotuning. \TODO{flesh out: compare cost of full
    enumeration with that of classification and regression.}
\item An evaluation of several approaches to performing machine
  learning classification in which the hypothesis space has hard
  constraints on legality. \TODO{flesh out: why is this important?}
\end{itemize}


\section{Motivation}

\begin{figure}
\centering
\begin{subfigure}[h]{.49\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{gen/img/motivation_1}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-1}
\end{subfigure}
~%
\begin{subfigure}[h]{.49\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{gen/img/motivation_2}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-2}
\end{subfigure}
\caption{%
  Workgroup size optimisation space of a stencil benchmark across
  devices: (\subref{fig:motivation-1}) Intel CPU,
  (\subref{fig:motivation-2}) NVIDIA GPU. This shows that stencil
  performance is dependent on properties of the underlying
  architecture, with different optimal workgroup sizes ($56 \times 20$
  vs.\ $64 \times 4$) for the two devices shown.%
}
\label{fig:motivation-arch}
\end{figure}

\begin{figure}
\begin{subfigure}[h]{.49\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{gen/img/motivation_3}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-3}
\end{subfigure}
~%
\begin{subfigure}[h]{.49\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{gen/img/motivation_4}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-4}
\end{subfigure}
\caption{%
  Workgroup size optimisation space of two stencils on the same
  device. Despite both programs executing on the same device, the
  relative performance of workgroup sizes varies greatly between the
  two programs. The optimal workgroup sizes are $128\times2$ and
  $256\times4$ respectively.%
}
\label{fig:motivation-prog}
\end{figure}

In this section I present the case for autotuning the workgroup size
of SkelCL stencil skeletons. Stencil workgroup sizes presents a two
dimensional parameter space consisting of a number of rows and
columns. It is constrained by the underlying architecture and
properties of the stencil code. For a detailed discussion of the
parameter space and experimental methodology, see
Chapters~\ref{chap:background} and~\ref{chap:methodology}.

By comparing the mean runtime of a stencil program using different
workgroup sizes while keeping all other conditions constant, we can
assess the relative performance of different points in the
optimisation space. Plotting this two dimensional optimisation space
using a three dimensional bar chart provides a quick visual overview
of the optimisation space, in which the two horizontal axes represent
the number of rows and columns in a workgroup, and height of each bar
shows the performance of at that point in the space (higher is
better).

If the performance of workgroup sizes were not dependent on the
execution device, we would expect the relative performance of points
in the optimisation space to be consistent across devices. As shown in
Figure~\ref{fig:motivation-arch}, this is not the case, with the
optimisation space of the same benchmark on different devices being
radically different. Not only does the optimal workgroup size change
between devices, but the performance of suboptimal workgroup sizes is
equally dissimilar.

The optimisation space of~\ref{fig:motivation-1} has a grid-like
structure, with clear performance advantages of workgroup sizes at
multiples of 8 columns. A developer specifically targeting this device
would learn to select workgroup sizes which follow this pattern. This
domain specific knowledge does not transfer to the optimisation space
of~\ref{fig:motivation-2}, where the relatively simple optimisation
space is amenable to a stochastic hill climbing search.

The optimal workgroup size is different for each of the four examples,
and the difference between the maximum and minimum performance
workgroup sizes provides an average $37.0\times$ speedup. The existing
SkelCL stencil implementation uses a statically chosen workgroup size
of $32\times4$, and this provides an average of only 63\% of the
available performance when compared to the best workgroup size for
these four examples. Even for this small set of examples, static
values and simple heuristics cannot provide portable performance. The
workgroup size parameter is sensitive to factors outside the influence
of the developers control, such as the type of program, the data being
operated on, and the underlying hardware. This makes portable
performance tuning a difficult task, and it has traditional been the
responsibility to domain specialists to labourisiously hand tune
individual programs to match the target problem and underlying
hardware.

Given the important role that stencil codes play in many fields of
computer science and simulation, and their computationally intensive
nature, I believe that there is a compelling case for the development
of an autotuner which can accommodate for these differences of
workgroup size performance between devices, and programs.

The ambition of this thesis is to demonstrate that, using machine
learning, we can develop predictive tuning systems which closely
approach --- and in some cases, outperform --- the kinds of ad hoc
hand tuning which traditionally came at the cost of many man hours of
work from expert programmers to perform.


\section{Structure}

The remainder of the document is structured as follows:

\begin{itemize}
\item Chapter~\ref{chap:background} contains necessary background
  material, an introduction to the SkelCL framework, and a description
  of the techniques used throughout the thesis;
\item Chapter~\ref{chap:related} contains an exposition of relevant
  literature in the field of autotuning and heterogeneous parallelism,
  contrasting the related research with my own;
\item Chapter~\ref{chap:autotune} presents OmniTune, an extensible and
  distributed autotuner capable of predicting optimisation parameter
  values for unseen programs at runtime;
\item Chapter~\ref{chap:omnitune-skelcl} describes the application of
  OmniTune for selecting workgroup size of SkelCL stencils;
\item Chapter~\ref{chap:methodology} describes a comprehensive
  exploration of the optimisation space of workgroup size for stencil
  skeletons, including the methodology for obtaining performance data
  and experimental setup;
\item Chapter~\ref{chap:evaluation} evaluates the effectiveness of
  OmniTune with respect to its accuracy, performance performance
  compared to human experts, and a cost benefit analysis of
  autotuning;
\item Chapter~\ref{chap:conclusions} contains concluding remarks, a
  critical evaluation of the presented work, and plans for future
  research.
\end{itemize}


\section{Summary}

This introductory chapter has outlined the need for higher levels of
abstraction for parallel programming and the difficulty that this
provides for performance tuning. It advocates the use of machine
learning for auotuning, and describes the contributions of this thesis
towards this goal. In the next chapter, I provide an overview of the
techniques and methodology used in this thesis.
