Parallelism is increasingly seen as the only viable approach to
maintaining continued performance improvements in a multicore
world. Core counts are increasing in all types of devices from mobile
to supercomputers, and devices are becoming increasingly
heterogeneous. Despite this shift in the hardware paradigm, the
adoption of parallel programming practises has been slow and awkward
due to the prohibitive complexity and low level of abstractions
available to developers.

To fully harness the processing power of a modern machine, programmers
must be equipped with an intimiate knowledge of the operating system
or language-specific threading model and synchronisation primitives,
and at least one system for programming heterogeneous devices (almost
certainly using an entirely different programming language to the host
device). Coupled with the inherent cognitive overhead of reasoning
about and debugging parallel programs, this has led to the
understandable belief that parallel programming is only for the
elite. More than ever, tools are needed to make parallel programming
accessible, without sacrificing performance.

SkelCL addresses this programmability challenge by allowing users to
easily harness the power of GPUs and CPUs for data parallel
computing~\cite{Steuwer2011}. To achieve this, SkelCL separates the
concerns of problem solving and coordinating heterogeneous devices,
achieving this abstraction using Algorithmic Skeletons.

Introduced by \citeauthor{Cole1989} in \citeyear{Cole1989},
Algorithmic Skeletons simplify the task of parallel programming by
abstracting common patterns of communication, providing parallel
implementations of higher order functions~\cite{Cole1989}. Algorithmic
Skeletons capture patterns of communication, and are parameterised by
the programmer with ``muscle functions'' that implement problem
specific logic. In doing so, they allow users to focus on solving the
problem at hand, affording greater ease of use than requring the
developer to coordinate parallel resources.


\section{The Problem}

The cost of this ease of use is performance. With each increase in the
level of the abstraction, implementation details are hidden from the
user which can be significantly impactful on performance. In SkelCL,
one such implementation detail which is abstracted from the user is
the selection of workgroup size --- a parameter of OpenCL kernels
which controls the mapping of threads to parallel execution
devices. Hiding such low level details from the user is key to the
goal of separating problem solving logic from coordination logic, so
it is the responsiblity of SkelCL to set the value of this parameter
to ensure good performance automatically.

Static values and simple heuristics cannot reliably achieve this
goal. Many optimisation parameters are sensitive to factors outside
the influence of the developers control, such as the type of program,
the data being operated on, and the underlying hardware. This makes
portable performance tuning a difficult task, and it has tradtional
been the responsibility to domain specialists to labourisiously hand
tune individual programs to match the target hardware.

The ambition of this thesis is to demonstrate that, using machine
learning, we can develop predictive tuning systems which closely
approach --- and in some cases, outperform --- the kinds of hand tuned
code which traditionally came at the cost of hundreds of man hours of
work from expert programmers to develop.


\section{Motivating Example}

\TODO{3D trisurfs of performance of wgsizes across different programs,
  devices, and datasets.}


\section{Contributions}

The key contributions of this work are:

\begin{itemize}
  % FIXME: Consider re-ordering these to bury the evaluation
  % part. It's not as interesting as the autotuner itself.
\item The development of \emph{OmniTune} --- an extensible,
  distributed system for runtime adaptive tuning of optimisation
  parameters.
\item An empirical evaluation of the performance of workgroup size to
  parameterise high-level parallel patterns. We enumerate the
  optimisation space of workgroup size for SkelCL stencil kernels
  across \input{gen/num_runtime_stats} test instances, demonstrating
  an average performance loss of
  $\input{gen/avg_possible_speedup_perc}\%$ (max
  $\input{gen/max_possible_speedup_perc}\%$) if workgroup size is not
  correctly tuned. % \TODO{Unexpected results. By exploring a much
    % larger space than is traditionally used, we find that workgroup
    % sizes outside the expected norm (i.e. not a multiple of 32) can
    % bring huge performance bonuses.}
\item The application of OmniTune for predicting workgroup sizes of
  unseen stencil codes in SkelCL. OmniTune achieves
  $\input{gen/best_avg_classification_performance}\%$ of the available
  performance, providing an average
  $\input{gen/best_avg_classification_speedup}\times$ speedup over the
  best possible statically chosen value.
\item A novel technique for procedurally generating stencil codes, and
  its application for training machine learning models. This reduces
  the cost of training while increasing the size of the space which
  can be explored. The effectiveness of this approach is demonstrated
  by testing an autotuner trained using synthetic benchmarks against
  \input{gen/num_real_kernels} real world stencil codes, achieving a
  worst case
  $\input{gen/biggest_synthetic_real_classification_performance_drop}\%$
  loss in achieved performance.
\item A comparison of multiple approaches to minimise the cost of
  effective autotuning. \TODO{flesh out: compare cost of full
    enumeration with that of classification and regression.}
\item An evaluation of several approaches to performing machine
  learning classification in which the hypothesis space has hard
  constraints on legality.
\end{itemize}

\section{Structure}

The remainder of the document is structured as follows:

\begin{itemize}
\item Chapter~\ref{chap:background} contains necessary background
  material, an introduction to the SkelCL framework, and a description
  of the techniques used throughout the thesis;
\item Chapter~\ref{chap:related} contains an exposition of relevant
  literature in the field of autotuning and heterogeneous parallelism,
  contrasting the related research with my own;
\item Chapter~\ref{chap:autotune} presents OmniTune, an extensible and
  distributed autotuner capable of predicting optimisation parameter
  values for unseen programs at runtime;
\item Chapter~\ref{chap:omnitune-skelcl} describes the application of
  OmniTune for selecting workgroup size of SkelCL stencils;
\item Chapter~\ref{chap:methodology} describes a comprehensive
  exploration of the optimisation space of workgroup size for stencil
  skeletons, including the methodology for obtaining performance data
  and experimental setup;
\item Chapter~\ref{chap:evaluation} evaluates the effectiveness of
  OmniTune with respect to its accuracy, performance performance
  compared to human experts, and a cost benefit analysis of
  autotuning;
\item Chapter~\ref{chap:conclusions} contains concluding remarks, a
  critical evaluation of the presented work, and plans for future
  research.
\end{itemize}
