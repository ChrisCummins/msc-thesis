% METHODOLOGY
% ===========
%
% Description of the work undertaken: this may be divided into
% chapters describing the conceptual design work and the actual
% implementation separately. Any problems or difficulties and the
% suggested solutions should be mentioned. Alternative solutions and
% their evaluation should also be included.

% Leather, H., O’Boyle, M., & Worton, B. (2009). Raced Profiles:
% Efficient Selection of Competing Compiler Optimizations. In LCTES
% ’09: Proceedings of the ACM SIGPLAN/SIGBED 2009 Conference on
% Languages, Compilers, and Tools for Embedded Systems
% (pp. 1–10). Dublin.
\TODO{%
  Using adapting sampling plans to reduce the number of samples
  required to distinguish good from bad compiler
  configurations~\cite{Leather2009}.%
}

\begin{table}
\footnotesize
\centering
\begin{tabular}{| l | l |}
  \hline
  & \textbf{Descriptionr}\\
  \hline
  $\bm{c}$ & Kernel compilation times \\
  $\bm{p}$ & Skeleton prepare times \\
  $\bm{u}$ & Host $\rightarrow$ Device transfers \\
  $\bm{k}$ & Kernel execution times \\
  $\bm{d}$ & Device $\rightarrow$ Host transfers \\
  $\bm{s}$ & Devices $\leftrightarrow$ Host (sync) transfers \\
  \hline
\end{tabular}
\caption{Measurable performance values.}
\label{tab:metric}
\end{table}

\TODO{%
  Approximating the total time of skeletons based on SkelCL profiling
  times:
\[t \approx \sum_{i=1}^n{\bm{1c}_{i}} + \bm{1p} + \bm{1s} +
  \frac{\sum_{i=1}^n{\bm{1u}_{i} + \bm{1k}_{i} + \bm{1d}_{i}}}{n} \]
}

\TODO{I'm only tuning GPU-side runtimes, which could be
  misleading~\cite{Gregg2011}}.

\begin{table}
\footnotesize
\centering
\begin{tabular}{| l | l | l | l | l |}
\hline
\textbf{Name} & \textbf{Application} & \textbf{Skeletons used} & \textbf{Iterative?} & \textbf{LOC}\\
\hline
CannyEdgeDetection & Image processing & Stencil & - & 225 / 61\\
DotProduct & Linear algebra & Zip, Reduce & - & 143 / 2\\
FDTD & Scientific simulation & Map, Stencil & Y & 375 / 127\\
GameOfLife & Cellular automata & Stencil & Y & 92 / 12\\
GaussianBlur & Image processing & Stencil & - & 262 / 47\\
HeatSimulation & Scientific simulation & Stencil & Y & 180 / 13\\
MandelbrotSet & Fractal computation & Map & Y & 133 / 78\\
MatrixMultiply & Linear algebra & AllPairs & - & 267 / 8\\
SAXPY & Linear algebra & Zip & - & 149 / 3\\
\hline
\end{tabular}
\caption{Benchmark applications. The LOC column shows lines of code, split between host (C++) and device (OpenCL).}
\label{tab:benchmarks}
\end{table}

\begin{table}
\footnotesize
\centering
\begin{tabular}{| l | l | l |}
\hline
\textbf{Parameter} & \textbf{Values} & \textbf{Skeleton}\\
\hline
Number of columns, rows, segments & \{8, 16, 32, 64, 128\} & AllPairs\\
Global size & \{256, 512, 1024, 2048, \ldots, 2097152\} & Reduce\\
Work group size & [32\ldots$n$] & *\\
Work group size (2D) & \{8, 16, 32, 64, 128, 256\} & Stencil\\
Device type & \{CPU, GPU\} & *\\
Device count & [1,4] & *\\
Halo region size & [1\ldots$n$] & Stencil\\
Implementation of stencil operation & {MapOverlap, Stencil} & Stencil\\
Border loading strategy & TODO & Stencil\\
Thread coarsening factor & TODO & *\\
\hline
\end{tabular}
\caption{Tunable parameters.}
\label{tab:knobs}
\end{table}

\begin{table}
\footnotesize
\centering
\begin{tabular}{| l | l | l | l |}
\hline
\textbf{CPU} & \textbf{Memory} & \textbf{GPU}\\
\hline
Intel i7-4770 & 16GiB & NVIDIA GTX TITAN\\
Intel i7-2600K & 16GiB & NVIDIA GTX 690\\
Intel i7-2600K & 8GiB & 2$\times$ NVIDIA GTX 590\\
Intel i7-3820 & 8GiB & 2$\times$ AMD Tahiti 7970\\
Intel i5-4570 & 8GiB & -\\
\hline
\end{tabular}
\caption{Testing hardware.}
\label{tab:hw}
\end{table}

\TODO{Table of derived dependent variables (e.g. FLOPS, GPU utilisation)}

\newpage

\begin{algorithm}
\caption{Request workgroup size}\label{bar}
\begin{algorithmic}[1]
\Require kernel features $k$, hardware features $h$, dataset features $d$.
\Ensure workgroup size $c$

\State $C \leftarrow \{ c_1, c_2,\ldots, c_n \}$
\Comment Set of all possible workgroup sizes

\If{no control flow in kernel}
    \State \textbf{return} $\underset{c}{\argmin} f(k,h,d,c) = r$
\Else
   \State converged $\leftarrow$ false
   \State $c_b \leftarrow$ baseline values
   \State $r_b \leftarrow$ measure runtime of runtime of program with $c_b$
   \While{not converged}
     \State return $\underset{c}{\argmax} g(k,h,d,c) = s$
     \State evaluate $c$, measuring runtime $r$\;
     \If{measured speedup $\frac{r_b}{r} \approx$ predicted speedup $s$}
       \State converged = true
     \Else
       \State $C = C - \{c\}$
     \EndIf
   \EndWhile
   \State \textbf{return} $c$
\EndIf
\end{algorithmic}
\end{algorithm}
