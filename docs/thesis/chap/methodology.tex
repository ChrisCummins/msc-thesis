\section{Introduction}

This chapter describe the methodology used to achieve statistically
sound comparisons of differing workgroup sizes on the performance of a
stencil code. I describe an experiment to explore the space of
architectures, programs, and datasets, and detail some of the many
factors which can influence the performance of stencils in different
conditions. I present the results of this exploration phase and use
this to present the case for autotuning.


\section{Defining the Optimisation Space}

This section describes the benchmarks, architectures, and datasets
used to gather performance data.


\subsection{Benchmark Stencil Applications}

To explore the effect of workgroup size across different stencils,
reference implementations of standard stencil applications from the
fields of image processing, cellular automata, and partial
differential equation solvers are used.

% \begin{table}
% \footnotesize
% \centering
% \begin{tabular}{| l | l | l | l | l |}
% \hline
% \textbf{Name} & \textbf{Application} & \textbf{Skeletons used} & \textbf{Iterative?} & \textbf{LOC}\\
% \hline
% CannyEdgeDetection & Image processing & Stencil & - & 225 / 61\\
% DotProduct & Linear algebra & Zip, Reduce & - & 143 / 2\\
% FDTD & Scientific simulation & Map, Stencil & Y & 375 / 127\\
% GameOfLife & Cellular automata & Stencil & Y & 92 / 12\\
% GaussianBlur & Image processing & Stencil & - & 262 / 47\\
% HeatSimulation & Scientific simulation & Stencil & Y & 180 / 13\\
% MandelbrotSet & Fractal computation & Map & Y & 133 / 78\\
% MatrixMultiply & Linear algebra & AllPairs & - & 267 / 8\\
% SAXPY & Linear algebra & Zip & - & 149 / 3\\
% \hline
% \end{tabular}
% \caption{Benchmark applications. The LOC column shows lines of code, split between host (C++) and device (OpenCL).}
% \label{tab:benchmarks}
% \end{table}

\TODO{Descriptions (with diagrams) for each application:}

\subsubsection{Finite Difference Time Domain}

\subsubsection{Heat Equation}

\subsubsection{Gaussian Blur}

The Gaussian blur is a common image processing algorithm, used to
reduce noise and detail in an image. A two dimensional Gaussian blur
defines a function to compute a pixel value $f(x,y)$ using a standard
deviation $\sigma$ using:

\begin{equation}
f(x,y) = \frac{1}{2\pi\sigma^2}e^{-\frac{x^2 + y^2}{2\sigma^2}}
\end{equation}

Gaussian blurs are parameterised by a radius $r$ which define
symmetric, square stencil regions about the centre
pixel. \TODO{Describe the range of blur radius' used.} Unlike the
previous two applications, a Gaussian blur is not iterative.

\subsubsection{Game of Life}

Conway's Game of Life (\FIXME{Citation needed}) is a cellular
automaton which models the evolution of a regular grid of cells over
discrete time steps. At each time step, each cell value is updated to
be either \emph{live} or \emph{dead} based on it's current value and
the value of the nearest neighbouring elements
(Algorithm~\ref{alg:gol}).

\begin{algorithm}[b]
\caption{Conway's Game of Life}
\label{alg:gol}
\include{alg/gol}
\end{algorithm}

The stencil shape for Game of Life is always 1 in each direction.

\subsubsection{Canny Edge Detection}

The Canny edge detection algorithm consists of four distinct stages:
\TODO{4 separate stencils, with different behaviours.}


\subsection{Procedural Generation of Synthetic Stencils}

Synthetic benchmarks are a popular method for evaluating relative
performance of competing systems. The algorithmic generation of such
benchmarks has clear benefits for reducing evaluation costs; however,
creating meaningful benchmark programs is a difficult problem if we
are to avoid the problems of redundant computation and produce
provable halting benchmarks (\FIXME{Citations needed, although there
  will need to be a more extensive review of this in the related work
  chapter}).

In practise, stencil codes many common traits: a well tightly
constrained interface, predictable memory access patterns, and well
defined input and output data types. This can be used to create a
confined space of possible stencil codes by enforcing upper and lower
bounds on properties of the codes which can not normally be guaranteed
for general-purpose programs, e.g. the number of floating point
operations. By doing so, it is possible to create a system to
programatically generate synthetic stencil workloads which can .

One motivation for the use of synthetic benchmarks is that for the
purposes of autotuning, generating a large set of synthetic benchmarks
can address the ``small $n$, large $P$'' problem, which describes the
difficulty of statistical inference in spaces for which the set of
possible hypotheses $P$ is greatly larger than the number of
observations $n$\CitationNeeded. Creating parameterised, synthetic
benchmarks, it is possible to explore a much larger set of the space
of possible stencil codes than if relying solely on reference
applications, for example, to evaluate the performance impact of
highly irregular stencil shapes.

\FIXME{The synthetic benchmarks I've used aren't actually
  \emph{procedurally} generated. I'll either need to implement a
  system, or failing that, describe my method for creating the
  synthetic benchmarks and move the discussion of procedurally
  generated stencils into the future work section.}

The synthetic benchmarks used in this thesis are parameterised by
stencil shape (one parameter for each of the four directions), data
type (either integer or single or double precision floating point),
and \emph{complexity} a simple metric for indicating the desired
number of memory accesses and instructions per stencil.


\section{Experimental Setup}

This section describes the environment under which performance data
was collected. Tables~\ref{tab:hw},~\ref{tab:kernels},
and~\ref{tab:datasets} list the range of execution devices, kernels,
and datasets used. For each unique combination of architecture,
kernel, and dataset (hereby referred to as a \emph{scenario}),
training data was collected by randomly sampling the space of legal
workgroup sizes, until multiple samples have been collected for each
combination of scenario and workgroup size.

\TODO{Description of each test system. E.g. mobo, memory, \& GPUs.}

\begin{table}
\input{tab/devices}
\caption{%
  Execution devices. \FIXME{Missing data from CPUs!}%
}
\label{tab:hw}
\end{table}

\begin{table}
\input{tab/kernels}
\caption{%
  Benchmark applications, border sizes, and static instruction counts.
  The ``simple'' and ``complex'' kernels are synthetic training
  programs. \FIXME{I also have a FDTD benchmark which I have yet to
    collect results for.}%
}
\label{tab:kernels}
\end{table}

\begin{table}
\input{tab/datasets}
\caption{%
  Datasets used.%
}
\label{tab:datasets}
\end{table}


\subsection{Runtime Sampling and Statistical Soundness}

The number of ``moving parts'' in the modern software stack makes for
noisy results when measuring program execution time. As such,
evaluating the relative performance of different versions of programs
requires a judicious approach to isolate the appropriate performance
metrics and to take a statistically rigorous approach to collecting
data.


\subsubsection{Isolating the Impact of Workgroup Size}

The execution of a SkelCL stencil application can be divided into 6
distinct phases, shown in Table~\ref{tab:stencil-runtime-components}.

\TODO{Include a flow diagram}

\begin{table}
\include{tab/stencil-runtime-components}
\caption{Execution phases of a SkelCL stencil skeleton. ``Fixed''
  costs are those which occur up to once per stencil
  invocation. ``Iterative'' costs are those which scale with the
  number of iterations of a stencil.}
\label{tab:stencil-runtime-components}
\end{table}

\paragraph{Kernel compilation times} Upon invocation, template
substitution is performed of the user code into the stencil skeleton
implementation, then compiled into an OpenCL program. Once compiled,
the program is cached for the lifetime of the host program.

\paragraph{Skeleton prepare times} Before a kernel is executed, a
preparation phase is required to allocate buffers for the input and
output data on each execution device.

\paragraph{Host $\rightarrow$ Device and Device $\rightarrow$ Host
  transfers} Data must be copied to and from the execution devices
before and after execution of the stencils, respectively. Note that
this is performed lazily, so iterative stencils do not require
repeated transfers between host and device memory.

\paragraph{Kernel execution times} This is the time elapsed executing
the stencil kernel, and is representative of ``work done''.

\paragraph{Devices $\leftrightarrow$ Host (sync) transfers} For
iterative stencils on multiple execution devices, an overlapping halo
region is shared at the border between the devices' grids. This must
be synchronised between iterations, requiring an intermediate transfer
to host memory, since device to device memory is not currently
supported by OpenCL.

For each of the six distinct phases of execution, accurate runtime
information can be gathered either through timers embedded in the host
code, or using the OpenCL \texttt{clGetEventProfilingInfo()} API for
operations on the execution devices. For single-device stencils, the
total time $t$ of a SkelCL stencil application is simply the sum of
all times recorded for each distinct phase:

\begin{equation}
t = \bm{1c} + \bm{1p} + \bm{1u} + \bm{1k} + \bm{1d}
\end{equation}

For multi-device stencils with $n$ execution devices, this can be
approximate as the sum of the sequential host-side phases, and the sum
of the device-side phases divided by the number of devices:

\begin{equation}
t \approx \sum_{i=1}^n{(\bm{1c}_{i})} + \bm{1p} + \bm{1s} +
  \frac{\sum_{i=1}^n{\bm{1u}_{i} + \bm{1k}_{i} + \bm{1d}_{i}}}{n}
\end{equation}

\TODO{Compare against empirical runtime information, showing how
  accurate this estimation is.}

The purpose of tuning workgroup size is to maximise the throughput of
stencil kernels. For this reason, isolating the kernel execution times
$\bm{k}$ produces the most accurate performance comparisons, as it
removes the impact of constant overheads introduced by memory
transfers between host and device memory, for which the selection of
workgroup size has no influence. Note that as demonstrated
in~\cite{Gregg2011}, care must be taken to ensure that isolating
device compute time does not cause misleading comparisons to be made
between across devices. For example, if using an autotuner to
determine whether execution of a given stencil is faster on a CPU or
GPU, the device transfer times $\bm{u}$, $\bm{d}$, and $\bm{s}$ would
need to be considered. For our purposes, we do not need to consider
the location of the data in the system's memory as it is has no
bearing on the throughput of a stencil kernel.


\subsubsection{Selection of sampling plan}

% A. Georges, D. Buytaert, and L. Eeckhout, “Statistically Rigorous
% Java Performance Evaluation,” in Proceedings of the 22Nd Annual ACM
% SIGPLAN Conference on Object-oriented Programming Systems and
% Applications, 2007, vol. 42, no. 10, p. 57.
In~\cite{Georges2007}, \citeauthor{Georges2007} present techniques

\FIXME{Back this up with empirical data. Design and run an experiment
  to show how the static sampling plan is justified.}

% Leather, H., O’Boyle, M., & Worton, B. (2009). Raced Profiles:
% Efficient Selection of Competing Compiler Optimizations. In LCTES
% ’09: Proceedings of the ACM SIGPLAN/SIGBED 2009 Conference on
% Languages, Compilers, and Tools for Embedded Systems
% (pp. 1–10). Dublin.
\TODO{%
  Consider using adapting sampling plans to reduce the number of
  samples required to distinguish good from bad workgroup
  sizes~\cite{Leather2009}.%
}

\section{Results}

\subsection{Statistical soundness}

\TODO{Plot variance proportional to runtime}

\subsection{Upper Bounds of Relative Performance}

% P. J. Fleming and J. J. Wallace, “How not to lie with statistics:
% the correct way to summarize benchmark results,” Commun. ACM,
% vol. 29, no. 3, pp. 218–221, 1986.
\TODO{%
  How to properly report benchmark results~\cite{Fleming1986}.%
}

For a given scenario $s$ and workgroup size $w$, the arithmetic mean
of measured runtimes is $t(s,w)$. From the set of all scenarios $S$
and workgroup sizes $W$, the subset of workgroup scenarios
$W_{legal}(S) \subset W$ which are legal for a given scenario can be
found using:

\begin{equation}
W_{legal}(s) = \left\{w | w \in W, w < W_{max}(s) \right\}
\end{equation}

\subsection{Assessing Oracle Performance}

The arithmetic mean of the measured runtimes for a given scenario $s$
and workgroup size $w$ is represented by $t(s,w)$. The oracle
workgroup size $\Omega(s) \in W_{legal}(s)$ is the $w$ value which
minimises the value of $t(s,w)$:

\begin{equation}
\Omega(s) = \argmin_{w \in W} t(s,w)
\end{equation}

This allows relative comparisons of performance of $w$ values against
the oracle:

\begin{equation}
p(s,w) = \frac{t(s,\Omega(s))}{t(s,w)}
\end{equation}

Where the performance is within the range $0 \le p(s,w) \le 1$. For a
given workgroup size, the average performance $\bar{p}(w)$ can be
found using the geometric mean of performance relative to the oracle
across all scenarios:

\begin{equation}
\bar{p}(w) =
\left(
  \prod_{s \in S} t(s,\Omega(s)) \cdot t(s,w)^{-1}
\right)^{1/|S|}
\end{equation}

\TODO{Plot max relative performance}


\subsection{Comparison with Baseline and Human Expert}

A total of \input{gen/num_scenarios} scenarios were tested. For each
scenario, an average of \input{gen/num_avg_params} unique workgroup
sizes were tested (max \input{gen/num_max_params}), for a total of
\input{gen/num_runtime_stats} combinations of scenario and workgroup
size. Runtimes were collected by randomly sampling $W_{safe}$, for an
average of \input{gen/avg_sample_count} runtimes per scenario (total
\input{gen/num_samples}). Figure~\ref{fig:min-max-runtimes} shows the
distribution of minimum and maximum runtimes.

\begin{figure}
\centering
\includegraphics{gen/img/min_max_runtimes}
\caption{%
  Distribution of minimum and maximum observed runtimes for each
  combination of scenario and parameter value, normalised to their
  respective mean runtimes.%
}
\label{fig:min-max-runtimes}
\end{figure}

The relative performance of different workgroup sizes for a scenario
can be found by normalising the mean runtimes against that of the
oracle workgroup size. This can provide an upper limit on the speedup
which can be attained using autotuning as the reciprocal of the
normalised runtime for the workgroup size which gave the lowest
performance. Applying this to all scenarios, we find the upper limit
of potential speedup to be between
$\input{gen/min_possible_speedup}\times$ --
$\input{gen/max_possible_speedup}\times$ (average
$\input{gen/avg_possible_speedup}\times$). This demonstrates that
selection of the optimal

Figure~\ref{fig:max-wgsizes} shows the distribution of maximum
workgroup sizes across all scenarios. Figure~\ref{fig:oracle-wgsizes}
shows the distribution of oracle workgroup sizes. Clearly, the
workgroup size $64 \times 4$ is the optimal value across the most
scenarios, but even that proves optimal only 10\% of the time. As
Figure~\ref{fig:oracle-accuracy} shows,
\input{gen/num_wgsizes_50_accuracy} unique workgroup sizes are
required in order to achieve oracle performance just 50\% of the time.

\TODO{Plot speedups of over baseline.}

\begin{figure}
\begin{subfigure}[t]{0.32\textwidth}
\centering
\includegraphics{gen/img/performance_kernels.png}
\vspace{-1.5em} % Shrink vertical padding
\caption{Kernels}
\label{fig:performance-kernels}
\end{subfigure}
~%
\begin{subfigure}[t]{0.32\textwidth}
\centering
\includegraphics{gen/img/performance_devices.png}
\vspace{-1.5em} % Shrink vertical padding
\caption{Devices}
\label{fig:performance-devices}
\end{subfigure}
~%
\begin{subfigure}[t]{0.32\textwidth}
\centering
\includegraphics{gen/img/performance_datasets.png}
\vspace{-1.5em} % Shrink vertical padding
\caption{Datasets}
\label{fig:performance-datasets}
\end{subfigure}
\label{fig:performance}
\caption{%
  Relative performance of workgroup sizes for different
  scenarios, divided by kernels, devices, and datasets.%
}
\end{figure}

\begin{figure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics{gen/img/max_wgsizes.png}
\vspace{-1.5em} % Shrink vertical padding
\caption{Maximum workgroup sizes}
\label{fig:max-wgsizes}
\end{subfigure}
~%
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics{gen/img/oracle_param_space.png}
\vspace{-1.5em} % Shrink vertical padding
\caption{Oracle workgroup sizes}
\label{fig:oracle-wgsizes}
\end{subfigure}
\caption{%
  On the left, the distribution of maximum legal workgroup sizes for
  all scenarios. On the right, the distribution of oracle workgroup
  sizes.%
}
\label{fig:heatmaps}
\end{figure}

\begin{figure}
\centering
\includegraphics{gen/img/num_params_oracle.png}
\caption{%
  Accuracy compared to the oracle as a function of the number of
  workgroup sizes used. The best accuracy that is achievable using a
  single statically chosen value is
  \protect\input{gen/max_oracle_param_frequency}\%.%
}
\label{fig:oracle-accuracy}
\end{figure}

\begin{figure}
\centering
\includegraphics{gen/img/performance_max_wgsize.png}
\caption{%
  Performance of a workgroup size relative to the oracle vs the
  maximum legal workgroup size. There is no clear trend between the
  performance of a workgroup size and it's size relative to the
  maximum allowed.%
}
\end{figure}

\begin{figure}
\centering
\includegraphics{gen/img/params_summary.png}
\caption{%
  The red line shows the ``legality'' of the parameter value, i.e.\
  the ratio of scenarios for which that workgroup size is legal.  The
  blue and green lines show the geometric mean of the performance of
  workgroup sizes relative to the oracle for: all scenarios, and only
  the scenarios for which the workgroup size is legal.%
}
\end{figure}


\section{Summary}

\TODO{Hmm. It seems that there is a lot of room for improvement, which
demonstrates the problem with having to generalise for all cases - you
lose out on up to *10x* performance improvements. Let's put this all
together:}

\begin{enumerate}
\item The best workgroup size for a particular workload depends on the
  hardware, software, and dataset.
\item Not all workgroup sizes are legal, and we can only test if a
  value \emph{is} legal at runtime.
\item Differing workloads have wildly different optimal workgroup
  sizes, and selecting the right one can give you a 10x boost in
  performance.
\end{enumerate}

\todo{This presents a compelling case for the development of an
  autotuner which can select the optimal workgroup size at
  runtime. That is what I have set out to achieve.}
