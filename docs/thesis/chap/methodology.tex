\section{Introduction}

This chapter describes an exhaustive enumeration of the workgroup size
optimisation space for 429 combinations of architecture, program, and
dataset. It contains the methodology used to collect empirical
performance data on which to base performance comparisons of different
workgroup sizes, and the steps necessary to obtain repeatable results.


\section{Experimental Setup}

\begin{table}
\input{tab/hosts}
\caption[Specification of experimental platforms]{%
  Specification of experimental platforms.%
}
\label{tab:hosts}
\end{table}

Table~\ref{tab:hosts} describes the experimental platforms used.  All
runtimes were recorded with millisecond precision using either the
system clock or OpenCL's Profiling API. Measurement noise was
minimised by reducing system load through disabling all unwanted
services and graphical environments, and exclusive single-user access
was ensured for each platform. Frequency governors for each CPU were
disabled, and the benchmark processes were set to the highest priority
available to the task scheduler. Datasets and programs were stored in
an in-memory file system.

\subsection{Devices}

\begin{table}
\input{tab/devices}
\caption[Specification of experimental OpenCL devices]{%
  Specification of experimental OpenCL devices.%
}
\label{tab:hw}
\end{table}

Table~\ref{tab:hw} describes the OpenCL devices used for testing, as
available on the experimental platforms.

\subsection{Benchmark Applications}

In addition to the synthetic stencil benchmarks described in
Section~\ref{sec:training}, six stencil kernels taken from four
reference implementations of standard stencil applications from the
fields of image processing, cellular automata, and partial
differential equation solvers are used:
%
\begin{itemize}
\item \textbf{Game of Life} Conway's Game of Life~\cite{Conway1970} is
  a cellular automaton which models the evolution of a regular grid of
  cells over discrete time steps. At each time step, each cell value
  is updated to be either \emph{live} or \emph{dead} based on it's
  current state and the state of the one immediately neighbouring cell
  to the north, south, east, and west.
\item \textbf{Heat Equation} The heat equation is a partial
  differential equation which describes the distribution of heat in a
  given region over time. Each iteration of the stencil represents a
  discrete time step, and the value of each cell (i.e.\ the
  temperature) is smoothed based on the temperatures of surrounding
  cells and the thermal conductivity of the material being simulated.
\item \textbf{Gaussian Blur} The Gaussian blur is a common image
  processing algorithm, used to reduce noise and detail in an image. A
  two dimensional Gaussian blur defines a function to compute a pixel
  value based on the value of neighbouring pixels. Gaussian blurs are
  parameterised by a radius which define symmetric, square stencil
  regions about the centre pixel. Unlike the previous two
  applications, the Gaussian blur is not an iterative stencil.
\item \textbf{Canny Edge Detection} The Canny edge detection algorithm
  is a multi-stage approach to detecting edges in
  images~\cite{Canny1986}. It consists of four distinct stages: a
  noise reduction operation, an edge detection operation, a
  non-maximum suppression, and a threshold operation. Each step is
  implemented as a separate SkelCL stencil and combined into a SkelCL
  StencilSequence.
\end{itemize}
%
Table~\ref{tab:kernels} shows details of the stencils kernels for
these reference applications, and the synthetic training benchmarks
used.

\begin{table}
\input{tab/kernels}
\caption[Description of stencil kernels]{%
  Stencil kernels, border sizes (north, south, east, and west),
  and static instruction counts.
}
\label{tab:kernels}
\end{table}


\subsection{Datasets}

For each benchmark, multiple dataset sizes were used, as shown in
table~\ref{tab:datasets}.

\begin{table}
\input{tab/datasets}
\caption[Description of experimental datasets]{%
  Description of experimental datasets.%
}
\label{tab:datasets}
\end{table}


\subsection{Sampling Strategy}\label{sec:sampling}

The number of ``moving parts'' in the modern software stack provides
multiple sources of noise when measuring program execution times. As
such, evaluating the relative performance of different versions of
programs requires a judicious approach to isolate the appropriate
performance metrics and to take a statistically rigorous approach to
collecting data.

\subsubsection{Isolating the Impact of Workgroup Size}

\begin{table}
\include{tab/stencil-runtime-components}
\caption[SkelCL stencil execution phases]{%
  Execution phases of a SkelCL stencil skeleton. ``Fixed''
  costs are those which occur up to once per stencil
  invocation. ``Iterative'' costs are those which scale with the
  number of iterations of a stencil.%
}
\label{tab:stencil-runtime-components}
\end{table}

The execution of a SkelCL stencil application can be divided into 6
distinct phases, shown in Table~\ref{tab:stencil-runtime-components}.
%
\begin{itemize}
\item \textbf{Kernel compilation times} Upon invocation, template
  substitution is performed of the user code into the stencil skeleton
  implementation, then compiled into an OpenCL program. Once compiled,
  the program binary is cached for the lifetime of the host program.
\item \textbf{Skeleton preparation times} Before a kernel is executed,
  a preparation phase is required to allocate buffers for the input
  and output data on each execution device.
\item \textbf{Host $\rightarrow$ Device and Device $\rightarrow$ Host
    transfer times} Data must be copied to and from the execution
  devices before and after execution of the stencils,
  respectively. Note that this is performed lazily, so iterative
  stencils do not require repeated transfers between host and device
  memory.
\item \textbf{Kernel execution times} This is the time elapsed
  executing the stencil kernel, and is representative of ``work
  done''.
\item \textbf{Devices $\leftrightarrow$ Host (sync) transfer times}
  For iterative stencils on multiple execution devices, an overlapping
  halo region is shared at the border between the devices' grids. This
  must be synchronised between iterations, requiring an intermediate
  transfer to host memory, since device to device memory is not
  currently supported by OpenCL.
\end{itemize}
%
For each of the six distinct phases of execution, accurate runtime
information can be gathered either through timers embedded in the host
code, or using the OpenCL \texttt{clGetEventProfilingInfo()} API for
operations on the execution devices. For single-device stencils, the
total time $t$ of a SkelCL stencil application is simply the sum of
all times recorded for each distinct phase:
%
\begin{equation}
t = \bm{1c^T} + \bm{1p^T} + \bm{1u^T} + \bm{1k^T} + \bm{1d^T}
\end{equation}
%
Note that there are no synchronisation costs $s$. For applications
with $n$ execution devices, the runtime can be approximate as the sum
of the sequential host-side phases, and the sum of the device-side
phases divided by the number of devices:
%
\begin{equation}
t \approx \sum_{i=1}^n{(\bm{1c^T}_{i})} + \bm{1p^T} + \bm{1s^T} +
  \frac{\sum_{i=1}^n{\bm{1u^T}_{i} + \bm{1k^T}_{i} + \bm{1d^T}_{i}}}{n}
\end{equation}
%
The purpose of tuning workgroup size is to maximise the throughput of
stencil kernels. For this reason, isolating the kernel execution times
$\bm{k}$ produces the most accurate performance comparisons, as it
removes the impact of constant overheads introduced by memory
transfers between host and device memory, for which the selection of
workgroup size has no influence. Note that as demonstrated
in~\cite{Gregg2011}, care must be taken to ensure that isolating
device compute time does not cause misleading comparisons to be made
between devices. For example, if using an autotuner to determine
whether execution of a given stencil is faster on a CPU or GPU, the
device transfer times $\bm{u}$, $\bm{d}$, and $\bm{s}$ would need to
be considered. For our purposes, we do not need to consider the
location of the data in the system's memory as it is has no bearing on
the execution time of a stencil kernel.


\subsubsection{Validating Program Behaviour}

Gold standard output was recorded by executing each of the real-world
benchmarks programs using the baseline workgroup size. The output of
real-world benchmarks with other workgroup sizes was compared to this
gold standard output to guarantee correct program execution.


\section{Summary}

This section describes the methodology for collecting relative
performance data of SkelCL stencil benchmarks under different
combinations of architecture, program, dataset, and workgroup size.
The next chapter evaluates these performance results, and analyses the
performance of OmniTune at predicting workgroup sizes.
