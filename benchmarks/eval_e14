#!/usr/bin/env python2
#
# eval_e14 - Evaluate e14 data.
#
from __future__ import print_function

import itertools
import json
import re

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import weka.plot.graph as graph

# benchlib
import plot
import ml

import labm8
from labm8 import io
from labm8 import fs
from labm8 import math as labmath

WGSIZES= [
    [4, 8, 16, 24, 32, 48, 64],
    [4, 8, 16, 24, 32, 48, 64]
]

def get_data(path):
    with open(path) as file:
        return json.load(file)["data"]


def group_by_values(data, *keys):
    groups = {}

    for result in data:
        val = ", ".join([str(result[key]) for key in keys])

        if val in groups:
            groups[val].append(result)
        else:
            groups[val] = [result]

    return groups


def get_speedup(wg_str, result):
    key = "Speedup{0}".format(wg_str)
    return result[key]


def get_scenario_str(scenario):
    title = []
    benchmark = scenario["Benchmark"]
    devcount = scenario["DeviceCount"]
    devname = scenario["dev_name"]
    datasize = (scenario["DataHeight"], scenario["DataWidth"])

    title.append(benchmark)
    title.append("(" + "x".join([str(x) for x in datasize]) + "), ")

    if devcount > 1:
        title.append("%sx " % devcount)
    # Convert devname to human readable form.
    devname = " ".join(devname.split("_"))
    # Remove " cpu @ x.x GHZ" suffix:
    devname = re.sub(" cpu @.*$", "", devname)
    # Remove " r " and " tm " noisewords:
    devname = re.sub(" (r|tm) ", " ", devname)

    title.append(devname.title())

    return "".join([str(x) for x in title])


def process(scenarios, wgsizes, name):
    # Statistics collected across all scenarios.
    stats = {
        "num scenarios": [len(scenarios)],
        "num valid": [],
        "mean": [],
        "mean (valid)": [],
        "predicted": [],
        "diff (mean)": [],
        "diff (valid)": [],
        "diff (oracle)": [],
        "ratio (oracle)": []
    }

    # Add keys for each individual wg size.
    for wg in wgsizes:
        stats[wg[2]] = []

    for key in sorted(scenarios.keys()):
        if len(scenarios[key]) > 1:
            io.fatal("Scenarios should be unique!")

        scenario = scenarios[key][0]
        scenario_str = get_scenario_str(scenario)

        is_synthetic = scenario["Benchmark"][0] == "["

        results = [[x[1]] + scenario[x[2]] for x in wgsizes]
        valid_results = [x for x in results if x[1] != 0]

        mean = labmath.mean([x[1] for x in results])
        mean_valid = labmath.mean([x[1] for x in valid_results])

        e14_wg = scenario["PredictedLocalSize"]
        e14_key = "Speedup" + e14_wg
        e14_speedup = scenario[e14_key]

        oracle_wg = scenario["OracleLocalSize"]
        oracle_key = "Speedup" + oracle_wg
        oracle_speedup = scenario[oracle_key]

        stats["num valid"].append(len(valid_results))
        stats["mean"].append(mean)
        stats["mean (valid)"].append(mean_valid)
        stats["predicted"].append(e14_speedup[0])
        stats["diff (mean)"].append(e14_speedup[0] - mean)
        stats["diff (valid)"].append(e14_speedup[0] - mean_valid)
        stats["diff (oracle)"].append(e14_speedup[0] - oracle_speedup[0])
        try:
            stats["ratio (oracle)"].append(e14_speedup[0] / oracle_speedup[0])
        except ZeroDivisionError:
            stats["ratio (oracle)"].append(0)

        for wg in wgsizes:
            stats[wg[2]].append(scenario[wg[2]][0])

        # Plot results
        filename = re.sub("[, ]+", "_", scenario_str) + ".png"
        path = "./results/e14/speedups/" + filename
        if not is_synthetic and not fs.isfile(path):
            y = [x[1] for x in results]
            err = [x[2] for x in results]
            labels= [x[0] for x in results]

            plot.speedups(y, err=err, labels=labels, title=scenario_str,
                          usetex=False, path=path)

    for stat in sorted(stats):
        print(name, stat, "\t",
              min(stats[stat]), "\t",
              labmath.median(stats[stat]), "\t",
              labmath.mean(stats[stat]), "\t",
              max(stats[stat]))

def violinplot(data, wgsize_keys):
    """
    Create violin plot of all speedups.
    """
    benchmarks = group_by_values(data, "Benchmark")
    violindata = {}
    for benchmark in benchmarks:
        results = benchmarks[benchmark]
        speedups = []
        for result in results:
            speedups += [result[x][0] for x in wgsize_keys if result[x][0] > 0]
        if len(speedups):
            violindata[benchmark] = speedups

    labels = list(reversed(sorted(violindata)))
    y_vals = [violindata[x] for x in labels]

    # Set axis labels.
    syn, exp = 1, 1
    for i in range(len(labels)):
        if labels[i][0] == "[":
            labels[i] = "S" + str(syn)
            syn += 1
        else:
            labels[i] = "E" + str(exp)
            exp += 1

    fig, ax = plt.subplots()
    sns.violinplot(y_vals, names=labels, inner="points", ymin=0)
    plt.ylabel('Speedup')
    plt.xlabel('Programs')
    plt.title(("Distribution of speedups for {0} samples"
               .format(sum([len(x) for x in y_vals]))))
    ymin, ymax = 0, 10
    plt.ylim(ymin=ymin, ymax=ymax)
    ax.yaxis.set_ticks(np.arange(ymin, ymax + 1, 1.0))
    plt.savefig("./results/e14/violin.png")
    plt.close()


def hist_speedups(results):
    """
    Create histogram of oracle and omnitune speedups.
    """
    oracle_speedups, predicted_speedups = [], []
    for result in results:
        oracle_wgsize = result["OracleLocalSize"]
        oracle_speedup = result["Speedup" + oracle_wgsize][0]
        predicted_wgsize = result["PredictedLocalSize"]
        predicted_speedup = result["Speedup" + predicted_wgsize][0]

        if oracle_speedup > 0:
            oracle_speedups.append(oracle_speedup)
            predicted_speedups.append(predicted_speedup)

    fig, axes = plt.subplots(nrows=1, ncols=2)
    bins = np.logspace(0, 1, 20)
    axes[0].hist(oracle_speedups, bins)
    axes[0].set_xlabel("Oracle")
    axes[0].set_ylabel("Frequency")

    axes[1].hist(predicted_speedups, bins,
                 color=sns.desaturate("indianred", .75))
    axes[1].set_xlabel("OmniTune")

    ymin, ymax = 0, 35
    xticks = [1, 1.5, 2, 3, 4, 5, 6, 8, 10]

    for ax in axes:
        ax.set_xscale("log")
        ax.set_ylim([ymin, ymax])
        ax.set_xticks(xticks)
        ax.set_xticklabels(xticks)

    plt.figtext(.5, .935, "Distribution of speedups", ha='center')
    plt.savefig("./results/e14/hist.png")
    plt.close()


def plot_val_frequency(results, wgsizes):
    """
    Plot frequencies that parameter values are optimal or predicted by
    omnitune.
    """
    freqs = {}
    for wgsize in wgsizes:
        wgsize_str = wgsize[1]
        freqs[wgsize_str] = [0,0]

    # Create data.
    for result in results:
        oracle_wgsize = result["OracleLocalSize"]
        freqs[oracle_wgsize][0] += 1
        predicted_wgsize = result["PredictedLocalSize"]
        freqs[predicted_wgsize][1] += 1

    # Prune param values which are never picked.
    for wgsize in wgsizes:
        key = wgsize[1]
        if freqs[key][0] < 1 and freqs[key][1] < 1:
            del freqs[key]

    oracle_freqs = [x[0] for x in freqs.values()]
    predicted_freqs = [x[1] for x in freqs.values()]
    labels = freqs.keys()
    x_vals = np.arange(len(labels))

    # Sort by decreasing value.
    oracle_freqs, predicted_freqs = zip(*reversed(sorted(zip(oracle_freqs,
                                                             predicted_freqs),
                                                         key=lambda x: x[0])))
    width = 7 / 16.
    ax = plt.subplot(111)

    ax.bar(x_vals, oracle_freqs, width=width, label="Oracle")
    ax.bar(x_vals + width, predicted_freqs,
           color=sns.desaturate("indianred", .75), width=width,
           label="OmniTune")
    plt.xticks(x_vals + .5, labels, rotation=90)
    plt.legend()
    plt.ylabel('Frequency')
    plt.xlabel('Parameter values')
    # TODO: This is a shit plot title
    plt.title(("Parameter values of {0} instances"
               .format(sum(oracle_freqs))))
    plt.savefig("./results/e14/freq.png")
    plt.close()


def clusters(results):
    oracle_x, oracle_y = [], []
    predicted_x, predicted_y = [], []
    for result in results:
        oracle_wgsize = [int(x) for x in result["OracleLocalSize"].split("x")]
        oracle_x.append(oracle_wgsize[0])
        oracle_y.append(oracle_wgsize[1])
        predicted_wgsize = [int(x) for x in result["PredictedLocalSize"].split("x")]
        predicted_x.append(predicted_wgsize[0])
        predicted_y.append(predicted_wgsize[1])

    fig, axes = plt.subplots(nrows=1, ncols=2)
    for ax in axes:
        ax.set_xlim([0, 80])
        ax.set_ylim([0, 70])

    axes[0].set_xlabel("Oracle")
    axes[1].set_xlabel("OmniTune")

    sns.kdeplot(np.array(oracle_x), np.array(oracle_y),
                gridsize=128, shade=True, ax=axes[0], cmap="Blues")
    sns.kdeplot(np.array(predicted_x), np.array(predicted_y),
                gridsize=128, shade=True, cmap="Reds")
    plt.figtext(.5, .935, "Clustering of parameter values", ha='center')
    plt.savefig("./results/e14/clusters.png")
    plt.close()


def cluster_invalid(results, wgsizes):
    x_val, y_val = [], []
    for result in results:
        for  wgsize in wgsizes:
            if result[wgsize[2]][0] == 0:
                x_val.append(wgsize[0][0])
                y_val.append(wgsize[0][1])

    sns.kdeplot(np.array(x_val), np.array(y_val), gridsize=64, shade=True)
    plt.figtext(.5, .935, "Invalid parameter values", ha='center')
    plt.savefig("./results/e14/invalid-clusters.png")
    plt.close()


def print_classifier_performance(results):
    oracle_speedups, predicted_speedups = [], []
    failed_count = 0

    for result in results:
        oracle_wgsize = result["OracleLocalSize"]
        oracle_speedup = result["Speedup" + oracle_wgsize][0]

        if oracle_speedup > 0:
            oracle_speedups.append(oracle_speedup)
            predicted_wgsize = result["PredictedLocalSize"]

            predicted_speedup = result["Speedup" + predicted_wgsize][0]
            if predicted_speedup == 0:
                failed_count += 1
                predicted_speedup = result["Speedup32x4"][0]

            predicted_speedups.append(predicted_speedup)

    oracle_mean = labmath.mean(oracle_speedups)
    predicted_mean = labmath.mean(predicted_speedups)
    mean_ratio = predicted_mean / oracle_mean

    oracle_median = labmath.median(oracle_speedups)
    predicted_median = labmath.median(predicted_speedups)
    median_ratio = predicted_median / oracle_median

    fail_rate = failed_count / float(len(oracle_speedups))

    print("    Num instances: ", len(oracle_speedups))
    print("    Mean speedup:   {0:.2f} ({1:.2f}%)".format(predicted_mean,
                                                          mean_ratio * 100))
    print("    Median speedup: {0:.2f} ({1:.2f}%)".format(predicted_median,
                                                          median_ratio * 100))
    print("    Min speedup:    {0:.2f}".format(min(predicted_speedups)))
    print("    Max speedup:    {0:.2f} ({1:.2f}%)".format(max(predicted_speedups),
                                               (max(predicted_speedups) / max(oracle_speedups)) * 100))
    print("    Failure rate    {0:.2f}%".format(fail_rate * 100))

def print_speedups(speedups):
    mean = labmath.mean(speedups)
    median = labmath.median(speedups)

    print("    Num instances: ", len(speedups))
    print("    Mean speedup:   {0:.2f}".format(mean))
    print("    Median speedup: {0:.2f}".format(median))
    print("    Min speedup:    {0:.2f}".format(min(speedups)))
    print("    Max speedup:    {0:.2f}".format(max(speedups)))

def validate(results):
    k = 10
    training_paths, validation_paths = ml.split_arff("./results/e14/training.arff",
                                                     k=k)
    xspeedups = []
    for training,validation in zip(training_paths, validation_paths):
        training_data = ml.load_arff(training)
        test_data = ml.load_arff(validation)
        xspeedups += ml.evaluate_J48(training_data, test_data)

    training_data = ml.load_arff("./results/e14/training.arff")
    test_data = ml.load_arff("./results/e14/test.arff")
    tspeedups = ml.evaluate_J48(training_data, test_data)

    print("\n{0}-Fold cross validation:".format(k))
    print_speedups(xspeedups)

    print("\nTest programs:")
    print_speedups(tspeedups)

    # Plot classifier.
    # tree = ml.get_J48(training_data)
    # graph.plot_dot_graph(tree.graph)


def main():
    """
    Generate plots.
    """
    wgsize_vals = list(itertools.product(*WGSIZES))
    wgsize_strs = ["{0}x{1}".format(*x) for x in wgsize_vals]
    wgsize_keys = ["Speedup{0}".format(x) for x in wgsize_strs]
    wgsizes = zip(wgsize_vals, wgsize_strs, wgsize_keys)

    print("stat \t min \t median \t mean \t max")
    alldata = get_data("results/e14/data.arff.json")

    # Ensure that all datapoints have all speedup keys.
    for result in alldata:
        for key in wgsize_keys:
            if key not in result:
                result[key] = [0, 0, 0]

    training_data = [x for x in alldata if x["Benchmark"][0] == "["]
    test_data = [x for x in alldata if x not in training_data]

    keys = [key for key in alldata[0]]
    invars = [
        "dev_name",
        "DeviceCount",
        "dev_device_type",
        "Benchmark",
        "DataHeight",
        "DataWidth"
    ]

    scenarios = group_by_values(alldata, *invars)
    training_scenarios = group_by_values(training_data, *invars)
    test_scenarios = group_by_values(test_data, *invars)

    process(scenarios, wgsizes, "all")
    process(training_scenarios, wgsizes, "training")
    process(test_scenarios, wgsizes, "test")

    violinplot(alldata, wgsize_keys)
    hist_speedups(alldata)
    plot_val_frequency(alldata, wgsizes)
    clusters(alldata)

    print("\nAll Data:")
    print_classifier_performance(alldata)

    print("\nTest Data:")
    print_classifier_performance(test_data)

    validate(alldata)
    cluster_invalid(alldata, wgsizes)


if __name__ == "__main__":
    main()
