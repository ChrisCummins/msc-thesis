#!/usr/bin/env python2.7
#
# e13 - Tuning number of iterations between swaps for iterative
# Stencil applications on multi-GPUs.
#
from __future__ import print_function
from copy import copy
from itertools import product
from operator import mul
from os import remove
from re import sub
from sys import stdout

import os

# Benchlib imports.

from benchlib import *
from skelcl import *
from util import *
from weka import *

import config

from e14_tree import classify

class SimpleBigKnob(Knob):
    cxxSrc = path(SKELCL, 'examples/SimpleBig/main.cpp')
    clSrc = path(SKELCL, 'examples/SimpleBig/kernels.cl')

class BorderSize(SimpleBigKnob):
    def __init__(self, val):
        SimpleBigKnob.__init__(self, "BorderSize", val)

    def set(self, **kwargs):
        os.system("sed -r -i 's/(define NORTH) [0-9]+/\\1 {val}/' {path}"
                  .format(val=self.val[0], path=self.cxxSrc))
        os.system("sed -r -i 's/(define WEST) [0-9]+/\\1 {val}/' {path}"
                  .format(val=self.val[1], path=self.cxxSrc))
        os.system("sed -r -i 's/(define SOUTH) [0-9]+/\\1 {val}/' {path}"
                  .format(val=self.val[2], path=self.cxxSrc))
        os.system("sed -r -i 's/(define EAST) [0-9]+/\\1 {val}/' {path}"
                  .format(val=self.val[3], path=self.cxxSrc))

jobdesc =  {
    "hosts": [ "florence", "cec", "dhcp-90-060", "whz5", "tim", "monza" ],
    "benchmarks": {
        # Real benchmark
        "CannyEdgeDetection": { "args": {} },
        "FDTD": { "args": {} },
        "GaussianBlur": { "args": {} },
        "GameOfLife": {
            "args": {
                "Size": ["--size {s}".format(s=s) for s in [1024]]
            }
        },
        "HeatEquation": {
            "args": {
                "Size": ["--size {s}".format(s=s) for s in [1024]]
            }
        },
        # Synthetic benchmark
        "SimpleBig": {
            "args": {
                "Size": ["-w {w} -h {h}".format(w=x[0], h=x[1]) for x in [
                    [1024, 1024],
                    [2048, 2048]
                ]],
                "Complexity": ["", "--complex"]
            }
        }
    },
    "args": {},
    "knobs": {
        BorderSize: [
            [ 1,  1,  1,  1],
            [ 5,  5,  5,  5],
            [10, 10, 10, 10],
            [20, 20, 20, 20],
            [30, 30, 30, 30],
            [ 1, 10, 30, 30],
            [20, 10, 20, 10]
        ],
        StencilLocalSizeC: [4, 16, 32, 64],
        StencilLocalSizeR: [4, 16, 32, 64]
    }
}

# Check that values are legal.
def knobvaluesarelegal(host, devargs, knobs):
    # Get the available devices.
    cpus, gpus = host.getDevices()

    # Calculate the workgroup size.
    wg = [lookup1(knobs, StencilLocalSizeC).val,
          lookup1(knobs, StencilLocalSizeR).val]
    workgroupsize = wg[0] * wg[1]

    # Determine the target device.
    devices = gpus if search("GPU", devargs[0].val) else cpus

    # Find the smallest max work group size from those devices.
    hardwareMaxWorkGroupSize = min([x["max_work_group_size"] for x in devices])

    # Detmine whethe the proposed work group size is smaller than the
    # hardware enforced maximum.
    return hardwareMaxWorkGroupSize >= workgroupsize

# For "Real" programs (i.e. not the SimpleBig benchmark), the
# BorderSize knob is a fixed value.
def set_border_size(benchmark, knobs):
    def get_border_size(benchmark, border):
        if benchmark == "SimpleBig":
            return border
        elif benchmark == "CannyEdgeDetection":
            return [5, 5, 5, 5]
        elif benchmark == "FDTD":
            return [1, 1, 1, 1]
        elif benchmark == "GaussianBlur":
            return [5, 5, 5, 5]
        elif benchmark == "GameOfLife":
            return [1, 1, 1, 1]
        elif benchmark == "HeatEquation":
            return [1, 1, 1, 1]
        else:
            print("Unrecognised benchmark '{0}'!".format(benchmark))
            exit(1)

    border = lookup1(knobs, BorderSize)
    border.val = get_border_size(benchmark.name, border.val)


def instantiate(host, benchmark, args, knobs):
    sampler = SkeletonEventTimingsSampler()
    harnesses = []

    for devargs in host.devargs():
        if knobvaluesarelegal(host, devargs, knobs):
            set_border_size(benchmark, knobs)

            c = SkelCLTestCase(benchmark, host=host,
                               invars=knobs + args + devargs)
            harnesses.append(TestHarness(c, sampler))

    return harnesses

HARNESSES = enumerateHarnesses(jobdesc, instantiate)

q = jobqueue(HARNESSES)

runJobQueue(q)

# Headless servers stop here.
if not masterhost(): exit(0)

####### DATA PROCESSING

from re import sub

from stats import *

import plot as plt

def getworkgroupsize(invars):
    return (lookup1(invars, StencilLocalSizeC),
            lookup1(invars, StencilLocalSizeR))

def setworkgroupsize(invars, *newWgSize):
    newinvars = copy(invars)

    # Lookup existing values.
    workgroupsize = getworkgroupsize(newinvars)

    # Remove existing value(s).
    [newinvars.remove(x) for x in workgroupsize]
    [newinvars.append(x) for x in newWgSize]

    return newinvars

def gettime(result):
    inittimes, buildtimes, preptimes, ultimes, skeltimes, swaptimes, dltimes = gettimes(result.outvars)
    return describe(skeltimes)

def humanReadableDeviceString(invars):
    s = ""

    deviceType = lookup1(invars, DeviceTypeArg).val

    if search("GPU", deviceType):
        # Get the --device-count argument.
        deviceCountArg = lookup1(invars, DeviceCountArg).val
        # Get the numerical count.
        deviceCount = search('([0-9]+)', deviceCountArg).group(1)
        return "{count}xGPU".format(count=deviceCount)
    else:
        return "CPU"

# Plot speedups of kernel for a fixed Device and Kernel type.
def plotSpeedupsForDeviceAndWorkGroup(harnesses):
    speedups = [lookup1(x.result().couts, Speedup).val for x in harnesses]
    borders = [lookup1(x.testcase.invars, BorderSize).val for x in harnesses]
    complexities = [lookup1(x.testcase.invars, "Complexity").val for x in harnesses]

    # Constants across all harnesses:

    width, height = getinputsize(harnesses[0].testcase.invars)
    hostname = lookup1(harnesses[0].testcase.invars, Hostname).val
    devicename = humanReadableDeviceString(harnesses[0].testcase.invars)

    iterations = sum([x[2] for x in speedups]) / len(speedups)

    # Work group size.
    workgroup = [
        lookup1(harnesses[0].testcase.invars, StencilLocalSizeC).val,
        lookup1(harnesses[0].testcase.invars, StencilLocalSizeR).val
    ]

    # Unique plot ID.
    plotId = ("-".join([str(x) for x in [
        hostname,
        devicename,
        workgroup[0],
        workgroup[1],
        width,
        height
    ]]))
    plotPath = path(config.RESULTS, config.ID, "kernel", plotId + ".png")

    # Plot title.
    title = ("Work group size: [{c} x {r}]. {hostname} "
             "{device} {width}x{height}"
             .format(c=workgroup[0], r=workgroup[1],
                     hostname=hostname, device=devicename,
                     width=width, height=height))

    # Plot caption.
    caption = "Average {n:.1f} iterations.".format(n=iterations)

    # Datapoints:

    Y   = [x[0] for x in speedups]
    Err = [x[1] for x in speedups]

    # Create xtick labels.
    Labels = []
    for i in range(len(harnesses)):
        border = borders[i]
        is_complex = complexities[i]
        label = "\\textbf{" if is_complex else ""
        label += "[{0}]".format(" ".join([str(x) for x in border]))
        if is_complex: label += "}"
        Labels.append(label)

    # Keyword arguments to speedup.
    kwargs = {
        "title":     title,
        "xlabel":    "Border size and kernel complexity",
        "labels":    Labels,
        "err":       Err,
        "caption":   caption,
        "path":      plotPath
    }

    plt.speedups(Y, **kwargs)

def getinputsize(invars):
    benchmark = lookup1(invars, BenchmarkName).val

    if benchmark == "SimpleBig":
        size_arg = lookup1(invars, "Size").val
        width  = search("-w (\d+)", size_arg).group(1)
        height = search("-h (\d+)", size_arg).group(1)
        return int(width), int(height)
    elif benchmark == "CannyEdgeDetection":
        return 512, 512
    elif benchmark == "FDTD":
        return 1024, 1024
    elif benchmark == "GaussianBlur":
        return 512, 512
    elif benchmark == "GameOfLife":
        return 1024, 1024
    elif benchmark == "HeatEquation":
        return 1024, 1024
    else:
        print("Unrecognised benchmark '{0}'!".format(benchmark))
        exit(1)

def getcomplexity(invars):
    benchmark = lookup1(invars, BenchmarkName).val

    if benchmark == "SimpleBig":
        return 1 if lookup1(invars, "Complexity").val else 0
    elif benchmark == "CannyEdgeDetection":
        return 0
    elif benchmark == "FDTD":
        return 0
    elif benchmark == "GaussianBlur":
        return 0
    elif benchmark == "GameOfLife":
        return 0
    elif benchmark == "HeatEquation":
        return 0
    else:
        print("Unrecognised benchmark '{0}'!".format(benchmark))
        exit(1)

# Plot speedups of work group size and kernel type for a fixed device.
def plotSpeedupsForDevice(groups):
    # Workgroup sizes
    wgVars = [getworkgroupsize(x[0].testcase.invars) for x in groups]
    wgSizes = [(x[0].val, x[1].val) for x in wgVars]
    width, height = getinputsize(groups[0][0].testcase.invars)

    # Constants across all harnesses:

    hostname = lookup1(groups[0][0].testcase.invars, Hostname).val
    devicename = humanReadableDeviceString(groups[0][0].testcase.invars)

    # Constants across all groups:

    borders = [lookup1(x.testcase.invars, BorderSize).val for x in groups[0]]
    complexities = [lookup1(x.testcase.invars, "Complexity").val for x in groups[0]]

    # Flatten grid data into a list:
    speedups = []
    for harnesses in groups:
        speedups += [lookup1(x.result().couts, Speedup).val for x in harnesses]

    iterations = sum([x[2] for x in speedups]) / len(speedups)

    # Unique plot ID.
    plotId = ("-".join([str(x) for x in [
        hostname,
        devicename,
        width,
        height
    ]]))
    plotPath = path(config.RESULTS, config.ID, "wg-kernel", plotId + ".png")

    # Plot title.
    title = ("{hostname} {device} {width}x{height}"
             .format(hostname=hostname, device=devicename,
                     width=width, height=height))

    # Plot caption.
    caption = "Average {n:.1f} iterations.".format(n=iterations)

    # Datapoints:

    Y   = [x[0] for x in speedups]

    # Create xtick labels.
    Xlabels = []
    for i in range(len(borders)):
        border = borders[i]
        is_complex = complexities[i]
        label = "\\textbf{" if is_complex else ""
        label += "[{0}]".format(" ".join([str(x) for x in border]))
        if is_complex: label += "}"
        Xlabels.append(label)

    Ylabels = ["[{c} x {r}]".format(c=x[0], r=x[1]) for x in wgSizes]

    # Keyword arguments to speedup.
    kwargs = {
        "surface":   False, # Surface or bar plot.
        "title":     title,
        "xlabel":    "",#"Border size and kernel complexity",
        "ylabel":    "",#"Work group size",
        "zlabel":    "Speedup",
        "xlabels":   Xlabels,
        "ylabels":   Ylabels,
        "caption":   caption,
        "path":      plotPath,
    }

    plt.speedups3d(Y, len(groups[0]), **kwargs)

def get_kernel_string(invars, tex=False):
    benchmark = lookup1(invars, BenchmarkName).val
    border = lookup1(invars, BorderSize).val
    complexity = getcomplexity(invars)

    if benchmark == "SimpleBig":
        string = "[{0}]".format(",".join([str(x) for x in border]))
    else:
        string = benchmark

    if complexity:
        if tex:
            string = "\\textbf{" + string + "}"
        else:
            string += "c"

    return string

# Plot speedups of work group sizes for each a fixed device and kernel.
def plotSpeedupsForDeviceAndKernel(harnesses):
    # Workgroup sizes
    wgVars = [getworkgroupsize(x.testcase.invars) for x in harnesses]
    wgSizes = [(x[0].val, x[1].val) for x in wgVars]

    # Constants across all harnesses:

    width, height = getinputsize(harnesses[0].testcase.invars)
    hostname = lookup1(harnesses[0].testcase.invars, Hostname).val
    devicename = humanReadableDeviceString(harnesses[0].testcase.invars)
    border = lookup1(harnesses[0].testcase.invars, BorderSize).val
    complexity = lookup1(harnesses[0].testcase.invars, "Complexity").val

    kernelString = get_kernel_string(harnesses[0].testcase.invars)

    # Get X,Y cordinate ranges values
    Xvals, Yvals = [], []
    for x in wgSizes:
        if x[0] not in Xvals: Xvals.append(x[0])
        if x[1] not in Yvals: Yvals.append(x[1])
    Xvals = list(reversed(sorted(Xvals)))
    Yvals = list(sorted(Yvals))

    # Create empty data list:
    speedups = [(0, 0, 0) for i in range(len(Xvals) * len(Yvals))]
    # Populate data:
    for i in range(len(harnesses)):
        # Get harness and work group size
        harness = harnesses[i]
        wg = wgSizes[i]

        # Calculate x,y position and 1D array index
        x, y = Xvals.index(wg[0]), Yvals.index(wg[1])
        index = y * len(Xvals) + x

        # Add speedup
        speedups[index] = lookup1(harness.result().couts, Speedup).val

    iterations = sum([x[2] for x in speedups]) / len(speedups)

    # Unique plot ID.
    plotId = "-".join([str(x) for x in [
        kernelString,
        width,
        height
    ]])
    plotPath = path(config.RESULTS, config.ID, "wg",
                    hostname + "-" + devicename, plotId + ".png")

    # Plot title.
    title = ("{hostname} {device} {kernel} {width}x{height}"
             .format(hostname=hostname, device=devicename, kernel=kernelString,
                     width=width, height=height))

    # Plot caption.
    caption = "Average {n:.1f} iterations.".format(n=iterations)

    # Datapoints:

    Y   = [x[0] for x in speedups]

    # Create xtick labels.
    Xlabels = Xvals
    Ylabels = Yvals

    # Keyword arguments to speedup.
    kwargs = {
        "surface":   False, # Surface or bar plot.
        "title":     title,
        "xlabel":    "Work group size (C)",
        "ylabel":    "Work group size (R)",
        "zlabel":    "Speedup",
        "xlabels":   Xlabels,
        "ylabels":   Ylabels,
        "caption":   caption,
        "path":      plotPath,
    }

    plt.speedups3d(Y, len(Xvals), **kwargs)

# Return the workgroup size and speedup value of the best speedup.
def getBestSpeedupFromHarnesses(harnesses):
    bestharness, bestspeedup = None, 0

    # Iterate over all harnesses to find the best speedup.
    for harness in harnesses:
        speedup = lookup1(harness.result().couts, Speedup).val[0]

        if speedup > bestspeedup:
            bestharness = harness
            bestspeedup = speedup

    # Test to see if we have a result.
    if bestharness:
        bestWgVars = getworkgroupsize(bestharness.testcase.invars)
        bestWg = (bestWgVars[0].val, bestWgVars[1].val)
    else:
        bestWg = None

    return bestWg, bestspeedup


def tprint(*args, **kwargs):
    # Characters to escape.
    escape_chars = ["[", "]"]

    escaped = [str(x) for x in args]
    for i in range(len(escaped)):
        for escape_char in escape_chars:
            if escape_char in escaped[i]:
                escaped[i] = escaped[i].replace(escape_char, "{" + escape_char + "}")

    if "end" not in kwargs: kwargs["end"] = "\\\\\n"

    print(" & ".join(escaped), **kwargs)

# Get speedup of invars over basline default stencil work group
# size.
def getspeedup(invars):
    # Get the result.
    result = resultscache.load(invars)

    # Get the baseline result.
    baselineinvars = setworkgroupsize(invars,
                                      StencilLocalSizeC(StencilLocalSizeC.DEFAULT),
                                      StencilLocalSizeR(StencilLocalSizeR.DEFAULT))
    baseline = resultscache.load(baselineinvars)

    if (baseline.bad or result.bad or
        not len(baseline.outvars) or not len(result.outvars)):
        # If we are missing data, speedup = 0.
        speedup = 0
        err = 0
    else:
        # Else, speedup = Tb / Tc
        time = gettime(result)
        baselinetime = gettime(baseline)
        speedup = baselinetime[0] / time[0]
        err = abs((baselinetime[0] / sum(time)) - speedup)
    # Average number of iterations.
    iterations = sum([len(result.outvars), len(baseline.outvars)]) / 2
    # Add Speedup cout variable.
    return Speedup((speedup, err, iterations))

# Run classifier on attribute "x" with invars.
def getPrediction(x, invars):
    # Get predicted work group size.
    wg = classify(x)
    # Create invars for predicted wg size.
    invars = setworkgroupsize(invars,
                              StencilLocalSizeC(wg[0]),
                              StencilLocalSizeR(wg[1]))
    # Get speedup of prediction.
    speedup = getspeedup(invars).val[0]

    return wg, speedup

# Host features to use for classification.
HOST_ATTR = [
    ("freq", "NUMERIC"),
    ("mem", "NUMERIC"),
    ("nproc", "NUMERIC"),
    ("procname", "NOMINAL")
]

# Device features to use for classification.
DEV_ATTR = [
    ("address bits", "NUMERIC"),
    ("alignment (bits) of base address",  "NUMERIC"),
    ("available", "NOMINAL"),
    ("cache line size",  "NUMERIC"),
    ("cache size", "NUMERIC"),
    ("cache type", "NOMINAL"),
    ("compiler available", "NOMINAL"),
    ("constant buffer size", "NUMERIC"),
    ("denorms", "NOMINAL"),
    ("device endianess", "NOMINAL"),
    ("device opencl c version", "NOMINAL"),
    ("device type", "NOMINAL"),
    ("driver version", "NOMINAL"),
    ("error correction support", "NUMERIC"),
    ("execute native function", "NOMINAL"),
    ("execute opencl kernels", "NOMINAL"),
    ("execution capabilities", "NOMINAL"),
    ("extensions", "NOMINAL"),
    ("global memory size", "NUMERIC"),
    ("ieee754-2008 fused multiply-add", "NOMINAL"),
    ("image support", "NOMINAL"),
    ("kernel preferred work group size multiple", "NUMERIC"),
    ("local memory size", "NUMERIC"),
    ("local memory type", "NOMINAL"),
    ("max clock frequency", "NUMERIC"),
    ("max compute units", "NUMERIC"),
    ("max image 2d height", "NUMERIC"),
    ("max image 2d width", "NUMERIC"),
    ("max image 3d depth", "NUMERIC"),
    ("max image 3d height", "NUMERIC"),
    ("max image 3d width", "NUMERIC"),
    ("max memory allocation", "NUMERIC"),
    ("max number of constant args", "NUMERIC"),
    ("max number of images read arguments", "NUMERIC"),
    ("max number of images write arguments", "NUMERIC"),
    ("max samplers within kernel", "NUMERIC"),
    ("max size of kernel argument", "NUMERIC"),
    ("max work group size", "NUMERIC"),
    ("max work items dimensions", "NUMERIC"),
    ("max work items[0]", "NUMERIC"),
    ("max work items[1]", "NUMERIC"),
    ("max work items[2]", "NUMERIC"),
    ("minimum alignment (bytes) for any datatype", "NUMERIC"),
    ("name", "NOMINAL"),
    ("native vector width char", "NUMERIC"),
    ("native vector width double", "NUMERIC"),
    ("native vector width float", "NUMERIC"),
    ("native vector width int", "NUMERIC"),
    ("native vector width long", "NUMERIC"),
    ("native vector width short", "NUMERIC"),
    ("out-of-order", "NOMINAL"),
    ("platform id", "NOMINAL"),
    ("preferred vector width char", "NUMERIC"),
    ("preferred vector width double", "NUMERIC"),
    ("preferred vector width float", "NUMERIC"),
    ("preferred vector width int", "NUMERIC"),
    ("preferred vector width long", "NUMERIC"),
    ("preferred vector width short", "NUMERIC"),
    ("profile", "NOMINAL"),
    ("profiling", "NOMINAL"),
    ("profiling timer resolution", "NUMERIC"),
    ("queue properties", "NOMINAL"),
    ("quiet nans", "NOMINAL"),
    ("round to +ve and infinity", "NOMINAL"),
    ("round to nearest even", "NOMINAL"),
    ("round to zero", "NOMINAL"),
    ("unified memory for host and device", "NUMERIC"),
    ("vendor", "NOMINAL"),
    ("vendor id", "NOMINAL"),
    ("version", "NOMINAL")
]

# Conert a hardware description into a list of host and device
# attributes.
def hw_info_to_attributes(info, host_attr, dev_attr):
    attributes = []

    for attr in host_attr:
        attributes.append(ArffAttribute("host_" + attr[0],
                                        attr[1], info["host"][attr[0]]))
    for attr in dev_attr:
        val = info["device"][attr[0]] if attr[0] in info["device"] else ""
        attributes.append(ArffAttribute("dev_" + attr[0], attr[1], val))

    return attributes

def createArff(harnesses, filename, schema=None, verbose=True):
    # Dataset.
    data = []
    predictions = []

    # Group harnesses by explanatory variable values.
    groups = groupByInvars(harnesses,
                           Hostname, DeviceTypeArg, DeviceCountArg,
                           BorderSize, "Complexity", "Size").values()

    # Iterate over each group.
    for group in groups:
        # Get the best work group size and speedup.
        wg, speedup = getBestSpeedupFromHarnesses(group)

        invars = group[0].testcase.invars
        border = lookup1(invars, BorderSize).val
        deviceArgs = lookup1(invars, DeviceTypeArg).val

        # Get the hardware attributes.
        hw_info = get_hw_info(invars)
        hw_attributes = hw_info_to_attributes(hw_info, HOST_ATTR, DEV_ATTR)

        # Get the additional attributes.
        if search("GPU", deviceArgs):
            deviceCountArg = lookup1(invars, DeviceCountArg).val
            deviceCount = int(search("(\d+)", deviceCountArg).group(1))
        else:
            deviceCount = 1
        borderNorth = border[0]
        borderEast  = border[1]
        borderSouth = border[2]
        borderWest  = border[3]
        dataWidth, dataHeight = getinputsize(invars)
        complexity = getcomplexity(invars)
        if wg == None:
            # We're deliberately excluding cases where no results were
            # found.
            continue
        else:
            localSize = "{w}x{h}".format(w=wg[0], h=wg[1])

        # Set the attributes.
        attributes = hw_attributes + [
            ArffAttribute("DeviceCount", "NUMERIC", deviceCount),
            ArffAttribute("BorderNorth", "NUMERIC", borderNorth),
            ArffAttribute("BorderEast",  "NUMERIC", borderEast),
            ArffAttribute("BorderSouth", "NUMERIC", borderSouth),
            ArffAttribute("BorderWest",  "NUMERIC", borderWest),
            ArffAttribute("DataWidth",   "NUMERIC", dataWidth),
            ArffAttribute("DataHeight",  "NUMERIC", dataHeight),
            ArffAttribute("Complexity",  "NOMINAL", complexity),
            ArffAttribute("LocalSize",   "NOMINAL", localSize)
        ]

        # Add a row to dataset.
        data.append(attributes)

        # Test the classifier.
        x = dict([(attr.name, str(attr.value)) for attr in attributes])
        predicted, predictedSpeedup = getPrediction(x, invars)

        # Store relative performance of prediction against oracle.
        diff = speedup - predictedSpeedup
        ratio = (diff / float(speedup)) * 100
        predictions.append(("x".join([str(x) for x in predicted]),
                            predictedSpeedup, localSize, speedup,
                            diff, ratio))

    # Determine the path to the output file.
    arffPath = path(config.RESULTS, config.ID, filename)

    # Determine the schema.
    if schema == None:
        schema = make_arff_schema(data)

    # Write the output file.
    mkarff(data, relation="e14", file=mkopen(arffPath, "w"), schema=schema)

    # Print predictions.
    if verbose:
        for x in predictions:
            string = "\t".join([str(y) for y in [
                x[0], "\t",
                x[2], "\t",
                "{0:.4f}".format(x[1]), "\t",
                "{0:.4f}".format(x[3]), "\t"
                "{0:.4f}".format(x[4]),
                "{0:.1f}%".format(x[5])
            ]])
            print(string)

    averageOracle = sum([x[3] for x in predictions]) / len(predictions)
    averagePrediction = sum([x[1] for x in predictions]) / len(predictions)

    # Number of orcale values for which the best speedup is "1",
    # i.e. where the baseline values were the best.
    oracle1count = len(list(filter(lambda x: x[3] == 1, predictions)))

    print("ORACLE     ", "{0:.3f}".format(averageOracle),
          "MIN", "{0:.3f}".format(min([x[3] for x in predictions])),
          "MAX", "{0:.3f}".format(max([x[3] for x in predictions])))
    print("PREDICTION ", "{0:.3f}".format(averagePrediction),
          "MIN", "{0:.3f}".format(min([x[1] for x in predictions])),
          "MAX", "{0:.3f}".format(max([x[1] for x in predictions])))
    print("Prediction performance is", "{0:.2f}%"
          .format(100 - sum([x[5] for x in predictions]) / len(predictions)),
          "of oracle.")
    print("Default values were optimal for {0:.2f}% of cases."
          .format(oracle1count / float(len(predictions)) * 100))

    return schema

def tableBestSpeedupsForKernel(harnesses, path):
    kGrouped = groupByInvars(harnesses, BorderSize, "Complexity", "Size").values()

    wgs = []
    speedups = []

    # Create gride of information.
    for kGroup in kGrouped:
        # Group by device.
        deviceGrouped = groupByInvars(kGroup, Hostname, DeviceTypeArg,
                                      DeviceCountArg).values()

        # Get best work group size and speedup for each device.
        w, s = zip(*[getBestSpeedupFromHarnesses(x)
                     for x in deviceGrouped])

        # Add to data grids.
        wgs.append(w)
        speedups.append(s)

    numDistinctDevVals = []
    numDistinctKernVals = []

    numRows = len(wgs)
    if not numRows:
        print("warning: skipping '{0}' because of no data.".format(path))
        remove(path)
        return
    numCols = len(wgs[0])

    # Count the number of distint best values (row-wise)
    for row in range(numRows):
        allVals = filter(lambda x: x != None, wgs[row])
        uniqVals = set(allVals)
        if len(allVals):
            ratio = int((len(uniqVals) / float(len(allVals))) * 100)
        else:
            ratio = 0
        numDistinctKernVals.append("{n} ({r}\\%)".format(n=len(uniqVals),
                                                         r=ratio))

    # Count the number of distinct best values (col-wise)
    for col in range(numCols):
        allVals = filter(lambda x: x != None,
                         [wgs[i][col] for i in range(numRows)])
        uniqVals = set(allVals)
        if len(allVals):
            ratio = int((len(uniqVals) / float(len(allVals))) * 100)
        else:
            ratio = 0
        numDistinctDevVals.append("{n} ({r}\\%)".format(n=len(uniqVals),
                                                        r=ratio))

    # Open output file
    file = mkopen(path, "w")

    print("\\begin{{tabular}}{{|{sep}|}}"
          .format(sep=(" | ".join(["p{1.8cm}"] +
                                  ["p{1cm}" for x in kGrouped[4:]] +
                                  ["p{1.1cm}"]))), file=file)

    # Print device strings as header.
    deviceGrouped = groupByInvars(kGrouped[0], Hostname, DeviceTypeArg,
                                  DeviceCountArg).values()
    deviceStrings = [("\\textbf{{{host} {dev}}}"
                      .format(host=lookup1(x[0].testcase.invars, Hostname).val,
                              dev=humanReadableDeviceString(x[0].testcase.invars)))
                     for x in deviceGrouped]
    tprint("\\hline", end="\n", file=file)
    tprint(*["\\textbf{Kernel}"] + deviceStrings + ["\\textit{Unique}"], file=file)
    tprint("\\hline", end="\n", file=file)

    for i in range(len(kGrouped)):
        # Kernel string.
        kernel = get_kernel_string(kGrouped[i][0].testcase.invars, tex=True)

        tprint(*[kernel] + list(wgs[i]) + [numDistinctKernVals[i]], file=file)

    # Print uniq vlas.
    tprint("\\hline", end="\n", file=file)
    tprint(*["\\textit{Unique}"] + numDistinctDevVals + [""], file=file)

    # Print footer.
    tprint("\\hline", end="\n", file=file)
    print("\\end{tabular}", file=file)

    # Print completion message.
    Colours.print(Colours.BLUE, "Wrote {path} ...".format(path=file.name))

# Preprocess data: Add derived speedups over default work group sizes.
for harness in HARNESSES:
    harness.result().couts.add(getspeedup(harness.result().invars))

training_harnesses = list(filter(lambda x: lookup1(x.testcase.invars, BenchmarkName).val == "SimpleBig", HARNESSES))
testing_harnesses  = list(filter(lambda x: lookup1(x.testcase.invars, BenchmarkName).val != "SimpleBig", HARNESSES))

print("Number of training instances... {0}".format(len(training_harnesses)))
print("Number of testing instances... {0}".format(len(testing_harnesses)))

# Create a .arff dataset file.
schema = createArff(HARNESSES, "data.arff",     verbose=False)
createArff(training_harnesses, "training.arff", schema=schema)
createArff(testing_harnesses,  "test.arff",     schema=schema)

harnesses_1024 = filterHarnessesByInvarVal(training_harnesses, "Size", "-w 1024 -h 1024")
harnesses_2048 = filterHarnessesByInvarVal(training_harnesses, "Size", "-w 2048 -h 2048")

tableBestSpeedupsForKernel(harnesses_1024, path(config.RESULTS, config.ID,
                                                "bestSpeedupsForKernel-1024.tex"))
tableBestSpeedupsForKernel(harnesses_2048, path(config.RESULTS, config.ID,
                                                "bestSpeedupsForKernel-2048.tex"))

# Group harnesses by Host/Device properties.
deviceGrouped = groupByInvars(training_harnesses, Hostname, DeviceTypeArg,
                              DeviceCountArg, "Size").values()

# Iterate over groups.
for deviceGroup in deviceGrouped:
    # Group harnesses by work group size.
    wgGrouped = groupByInvars(deviceGroup,
                              StencilLocalSizeC,
                              StencilLocalSizeR).values()
    # Plot speedups of work group sizes and kernels for each device.
    plotSpeedupsForDevice(wgGrouped)

    # Plot speedups of kernels for each device and work group size.
    [plotSpeedupsForDeviceAndWorkGroup(x) for x in wgGrouped]

    # Group harnesses by kernel type.
    kGrouped = groupByInvars(deviceGroup,
                             BorderSize,
                             "Complexity").values()

    # Plot speedups of work group sizes for each device and kernel.
    [plotSpeedupsForDeviceAndKernel(x) for x in kGrouped]

# Graph event times.
[plt.openCLEventTimes(x.testcase.invars) for x in training_harnesses]

exit(0)
